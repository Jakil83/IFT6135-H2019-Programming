{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6135-Assignment_1.2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "BQKRlHPW64aP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CNN vs MLP on MNIST"
      ]
    },
    {
      "metadata": {
        "id": "qK9DEdy97GLG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Initialisising Colab drive"
      ]
    },
    {
      "metadata": {
        "id": "S1jpXpNS5Bt-",
        "colab_type": "code",
        "outputId": "460edb60-5133-4226-b547-97241b227d6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install PyDrive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting PyDrive\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e0/0e64788e5dd58ce2d6934549676243dc69d982f198524be9b99e9c2a4fd5/PyDrive-1.3.1.tar.gz (987kB)\n",
            "\u001b[K    100% |████████████████████████████████| 993kB 20.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.6.7)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.11.3)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.11.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.4)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.5)\n",
            "Building wheels for collected packages: PyDrive\n",
            "  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/fa/d2/9a/d3b6b506c2da98289e5d417215ce34b696db856643bad779f4\n",
            "Successfully built PyDrive\n",
            "Installing collected packages: PyDrive\n",
            "Successfully installed PyDrive-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q5rjqlf4JOeM",
        "colab_type": "code",
        "outputId": "7ae188c6-8f9d-4f89-d36f-8510277047d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "!pip install torch\n",
        "\n",
        "import torch\n",
        "print(f\"Your Pytorch version is {torch.__version__}.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.1.post2)\n",
            "Your Pytorch version is 1.0.1.post2.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZHmr_oA8JTwc",
        "colab_type": "code",
        "outputId": "4b91995e-1a8b-40a5-aa10-9790e50f2fc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls \"/content/drive/My Drive/Colab Notebooks/6135/Assignment_1.2\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "6135-Assignment_1.2.ipynb  best_CNN_1  best_CNN_2  best_CNN_3  best_MLP_2L\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Nlxws8C97gfN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Librairies"
      ]
    },
    {
      "metadata": {
        "id": "uxPstMcgJXU-",
        "colab_type": "code",
        "outputId": "7aed66c4-9681-4770-9546-7aa69851397f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import random\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "\n",
        "print(\"Libraries ready\")\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "cuda = device.type == 'cuda'\n",
        "print(device, cuda)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Libraries ready\n",
            "cuda:0 True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D_8xxgxj7lCj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ]
    },
    {
      "metadata": {
        "id": "ksHFF1hIMHNf",
        "colab_type": "code",
        "outputId": "03aeecbd-2fb4-40d1-d3dd-d281ccdcb72f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "cell_type": "code",
      "source": [
        "train_data = datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor()\n",
        "                   ]))\n",
        "\n",
        "test_data = datasets.MNIST('../data', train=False,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor()\n",
        "                   ]))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PlAAYLTHNhXw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "\n",
        "valid_size = 15000\n",
        "\n",
        "indices = list(range(len(train_data)))\n",
        "random.shuffle(indices)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size=batch_size,\n",
        "    sampler=SubsetRandomSampler(indices[valid_size:]),\n",
        "    num_workers=4,\n",
        "    pin_memory=cuda\n",
        ")\n",
        "\n",
        "valid_loader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size=batch_size,\n",
        "    sampler=SubsetRandomSampler(indices[:valid_size]),\n",
        "    num_workers=4,\n",
        "    pin_memory=cuda,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_data,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    pin_memory=cuda,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vCf-NhUs1Xjv",
        "colab_type": "code",
        "outputId": "04f53823-af88-49a4-9f26-e00666fecc3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 60000\n",
              "    Split: train\n",
              "    Root Location: ../data\n",
              "    Transforms (if any): Compose(\n",
              "                             ToTensor()\n",
              "                         )\n",
              "    Target Transforms (if any): None"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "O-eRhARCEzoI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def showRandSample(dataset, n_row=5, n_col=5):\n",
        "    nb_img = n_row * n_col\n",
        "    \n",
        "    sample_loader = DataLoader(\n",
        "      dataset,\n",
        "      batch_size=nb_img, \n",
        "      shuffle=True,\n",
        "      num_workers=1,\n",
        "      pin_memory=cuda)\n",
        "\n",
        "    images, labels = next(iter(sample_loader))\n",
        "    images = images.squeeze()\n",
        "    \n",
        "    fig = plt.figure(figsize=(n_row*2, n_col*2))\n",
        "\n",
        "    for i in range(1, nb_img +1):\n",
        "        img = images[i-1]    \n",
        "        ax = fig.add_subplot(n_row, n_col, i)\n",
        "        ax.set_title(str(labels[i-1].item()))\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img,cmap='gray_r')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gZxeGeWRF0ax",
        "colab_type": "code",
        "outputId": "0366f31d-7cdc-4fc6-85ac-047ea832d7f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "cell_type": "code",
      "source": [
        "showRandSample(train_data) "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkwAAAJOCAYAAABFrFjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyfLkXVcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x720 with 25 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "WrfxvGUdvFM8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### MLP \n",
        "First as our compairason model we chose a strong MLP comming out of part 1, which is a simple model with 2 hidden layers of 512 and 768 neurons with Relu as the activation function. "
      ]
    },
    {
      "metadata": {
        "id": "dY0ZZCaAvFb_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MLP_2L(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        h1 = 512\n",
        "        h2 = 768\n",
        "        \n",
        "        # Fully connected layer\n",
        "        self.fc0 = nn.Linear(28*28, h1)\n",
        "        self.fc1 = nn.Linear(h1, h2)\n",
        "        self.fc2 = nn.Linear(h2, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = F.relu(self.fc0(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2hAX3j1AMoT_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### CNN"
      ]
    },
    {
      "metadata": {
        "id": "A4wZwIHSMcTz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To expose the effects of convolution on an image classification task, we build a simple CNN with convolution layers replacing a part of the MLP, in this case the first hidden layer.\n",
        "\n",
        "Simply put : we replace the first hidden layer of the MLP with a convolution block.  \n",
        "So, where we had :\n",
        "\n",
        "[input]->[hidden 1]->[hidden 2]->[ouput]\n",
        "\n",
        "we now have :\n",
        "\n",
        "[input]->[convolution]->[hidden 2]->[ouput]\n",
        "\n",
        "\n",
        "We tested 3 models CNN1, CNN2, CNN3 with a convolution block of 1 round, 2 rounds and 3 rounds of convolution.\n",
        "\n",
        "Since our MLP's hidden layers have 512 and 768 neurons respectively and a total of 803k paramaters, we had few choices in designing those layers.   For CNN1 it is simply not possible to respect both conditions, we chose to respect the hidden layer; so the total number of parameters is half of the MLP.  CNN2 & CNN3 have not only the ~same amount of parameters but also the same hidden layer.\n",
        "\n",
        "We effectively only **replace** the first hidden layer.  It also leads to a nicely balanced number of parameters compared to the MLP. (ie: the convotion part $\\approx$ h1 of the MLP).   This way, the design patterns stay consistant troughout all of the models.\n",
        "\n",
        "\n",
        "Here are the designs of the convolution block for the 3 models which all result in a layer of 512 neurons :\n",
        "\n",
        "##### CNN1\n",
        "\n",
        "{[  $32$ , $3\\times3$ ]->[pooling $7$]}\n",
        "\n",
        "\n",
        "##### CNN2\n",
        "\n",
        "{[  $87$ , $3\\times3$ ]->[pooling $3$]} -> {[  $512$ , $3\\times3$ ]->[pooling $10$]}\n",
        "\n",
        "\n",
        "##### CNN3\n",
        "\n",
        "{[  $64$ , $3\\times3$ ]->[pooling $2$]} -> {[  $77$ , $3\\times3$ ]->[pooling $4$]} -> {[  $512$ , $3\\times3$ ]->[pooling $4$]}\n",
        "\n",
        "\n",
        "\n",
        "$87$ and $77$ might seem like strange numbers, they are chosen simply so that total number of parameters is as close to 803k as possible.\n",
        "The last round of convolution for CNN2&3 must be of 512 filters with pooling down to 1 so that it connects to the hidden layer just like the MLP.  The filters are all $3\\times3$ with stride of of $1$ so as to remain consistant, the adaptation factor for the size commes only from the padding in the first layer.  (by coincidence it is $1$, $2$, $3$ for CNN 1, 2 ,3 )  \n"
      ]
    },
    {
      "metadata": {
        "id": "azpQfcc9MmVw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CNN_1(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Conv block\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        \n",
        "        # Fully connected layer\n",
        "        self.fc1 = nn.Linear(32*4*4, 768)\n",
        "        self.fc2 = nn.Linear(768, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 7)\n",
        "        \n",
        "        x = x.view(x.size(0), -1) # flatten\n",
        "        \n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x\n",
        "      \n",
        "class CNN_2(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Conv block\n",
        "        self.conv1 = nn.Conv2d(1, 87, 3, padding=2)\n",
        "        self.conv2 = nn.Conv2d(87, 512, 3, padding=1)\n",
        "        \n",
        "        # Fully connected layer\n",
        "        self.fc1 = nn.Linear(512*1*1, 768)\n",
        "        self.fc2 = nn.Linear(768, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 3)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 10)\n",
        "        \n",
        "        x = x.view(x.size(0), -1) # flatten\n",
        "        \n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x     \n",
        "      \n",
        "class CNN_3(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Conv block\n",
        "        self.conv1 = nn.Conv2d(1, 64, 3, padding=3)\n",
        "        self.conv2 = nn.Conv2d(64, 77, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(77, 512, 3, padding=1)\n",
        "        \n",
        "        # Fully connected layer\n",
        "        self.fc1 = nn.Linear(512*1*1, 768)\n",
        "        self.fc2 = nn.Linear(768, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 4)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.max_pool2d(x, 4)\n",
        "        \n",
        "        x = x.view(x.size(0), -1) # flatten\n",
        "        \n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aR-NN1BELCMB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Model verification and number of parameters :"
      ]
    },
    {
      "metadata": {
        "id": "8PdkQ4lMKz9r",
        "colab_type": "code",
        "outputId": "32868639-51d6-4ab0-ce7c-f2ce5ecf83fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1286
        }
      },
      "cell_type": "code",
      "source": [
        "#Number of parameters\n",
        "from torchsummary import summary\n",
        "\n",
        "model = CNN_1().to(device)\n",
        "\n",
        "print(\"CNN_1 : \",sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "summary(model, (1, 28, 28))\n",
        "\n",
        "\n",
        "model = CNN_2().to(device)\n",
        "\n",
        "print(\"\\nCNN_2 : \",sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "summary(model, (1, 28, 28))\n",
        "\n",
        "model = CNN_3().to(device)\n",
        "\n",
        "print(\"\\nCNN_3 : \",sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "summary(model, (1, 28, 28))\n",
        "\n",
        "model = MLP_2L().to(device)\n",
        "\n",
        "print(\"\\nMLP_2L : \",sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "summary(model, (1, 28, 28))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN_1 :  401994\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]             320\n",
            "            Linear-2                  [-1, 768]         393,984\n",
            "            Linear-3                   [-1, 10]           7,690\n",
            "================================================================\n",
            "Total params: 401,994\n",
            "Trainable params: 401,994\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.20\n",
            "Params size (MB): 1.53\n",
            "Estimated Total Size (MB): 1.73\n",
            "----------------------------------------------------------------\n",
            "\n",
            "CNN_2 :  803952\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 87, 30, 30]             870\n",
            "            Conv2d-2          [-1, 512, 10, 10]         401,408\n",
            "            Linear-3                  [-1, 768]         393,984\n",
            "            Linear-4                   [-1, 10]           7,690\n",
            "================================================================\n",
            "Total params: 803,952\n",
            "Trainable params: 803,952\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.99\n",
            "Params size (MB): 3.07\n",
            "Estimated Total Size (MB): 4.06\n",
            "----------------------------------------------------------------\n",
            "\n",
            "CNN_3 :  802071\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]             640\n",
            "            Conv2d-2           [-1, 77, 16, 16]          44,429\n",
            "            Conv2d-3            [-1, 512, 4, 4]         355,328\n",
            "            Linear-4                  [-1, 768]         393,984\n",
            "            Linear-5                   [-1, 10]           7,690\n",
            "================================================================\n",
            "Total params: 802,071\n",
            "Trainable params: 802,071\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.72\n",
            "Params size (MB): 3.06\n",
            "Estimated Total Size (MB): 3.78\n",
            "----------------------------------------------------------------\n",
            "\n",
            "MLP_2L :  803594\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                  [-1, 512]         401,920\n",
            "            Linear-2                  [-1, 768]         393,984\n",
            "            Linear-3                   [-1, 10]           7,690\n",
            "================================================================\n",
            "Total params: 803,594\n",
            "Trainable params: 803,594\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.01\n",
            "Params size (MB): 3.07\n",
            "Estimated Total Size (MB): 3.08\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5IpMJsu4-MOj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "test_loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "#Results directory\n",
        "savedir = 'results'\n",
        "if not os.path.exists(savedir):\n",
        "    os.makedirs(savedir)\n",
        "\n",
        "\n",
        "## Saved states are ignored for this expirement\n",
        "\n",
        "#checkpoint = torch.load('/content/drive/My Drive/Colab Notebooks//6135/Assignment_1.2')\n",
        "#model.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6l6w7Zqp8nvP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(model,train_loader, optimizer, epoch, scheduler):\n",
        "    model.train()\n",
        "    \n",
        "    for batch_idx, (inputs, target) in enumerate(train_loader):\n",
        "        inputs = inputs.to(device, dtype=torch.float)\n",
        "        target = target.to(device, dtype=torch.long)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(inputs)\n",
        "        loss = loss_fn(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch_idx % 100 == 0:  \n",
        "            print(model.__class__.__name__,' Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch +1, batch_idx * len(inputs), len(train_loader) *len(inputs) ,\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    \n",
        "    scheduler.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KqB9Hfl99sgT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    \n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    test_size = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, target in valid_loader:\n",
        "            inputs = inputs.to(device,dtype=torch.float) \n",
        "            target = target.to(device, dtype=torch.long)\n",
        "            \n",
        "            output = model(inputs)\n",
        "            test_size += len(inputs)\n",
        "            test_loss += test_loss_fn(output, target).item() \n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= test_size\n",
        "    accuracy = correct / test_size\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, test_size,\n",
        "        100. * accuracy))\n",
        "    \n",
        "    return test_loss, accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tCxBDgWvdCzz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Running the experiment\n",
        "\n",
        "The experiment being non-determinist, depending on weight initialisation and the draw of the training set, \n",
        "we run the experiment multiple times and take the average results for each correspoding epochs, reseting all weights and sets for each run.\n",
        "\n",
        "Here : 10 runs of 10 epochs."
      ]
    },
    {
      "metadata": {
        "id": "yrKFuF8xfvEn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "runs = 10\n",
        "epochs = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "We1gNp5kIPES",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "def weight_reset(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "        m.reset_parameters()\n",
        "\n",
        "\n",
        "def MultipleRun(runs,epochs,model):\n",
        "  \n",
        "  model = model.to(device)\n",
        "  name =  model.__class__.__name__\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "  exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=1.0)\n",
        "\n",
        "\n",
        "  results = {'name':name, 'loss': [0]*epochs, 'accuracy':[0]*epochs}\n",
        "  savefile = os.path.join(savedir, results['name']+'.pkl' )\n",
        "\n",
        "  best_net = '/content/drive/My Drive/Colab Notebooks//6135/Assignment_1.2/best_' + name\n",
        "  \n",
        "  for run in range(runs):\n",
        "      print(\"\\n\",\"  --  Run : \", run+1)\n",
        "      since = time.time()\n",
        "      best_accuracy = 0\n",
        "      \n",
        "      #Reseting all weights for new run\n",
        "      model.apply(weight_reset)\n",
        "\n",
        "      for epoch in range(epochs):\n",
        "\n",
        "          train(model, train_loader, optimizer, epoch,exp_lr_scheduler)\n",
        "          loss, acc = test(model, valid_loader)\n",
        "\n",
        "          if acc > best_accuracy:\n",
        "            best_accuracy = acc\n",
        "                        \n",
        "            #Saving best model parameters\n",
        "            torch.save({\n",
        "                'epoch_based0': epoch,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'accuracy': acc,\n",
        "                'loss' : loss,\n",
        "                'optimizer' : optimizer.state_dict(),\n",
        "            }, best_net)\n",
        "\n",
        "            print('new accuracy parameters saved {}'.format(best_accuracy))\n",
        "\n",
        "          #Updating average results\n",
        "          results['loss'][epoch] += loss/runs\n",
        "          results['accuracy'][epoch] += acc/runs\n",
        "\n",
        "\n",
        "      time_elapsed = time.time() - since\n",
        "      print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "              time_elapsed // 60, time_elapsed % 60))\n",
        "    \n",
        "  with open(savefile, 'wb') as fout:\n",
        "    pickle.dump(results, fout)\n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C3-Ht73dKDJQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Running CNN1"
      ]
    },
    {
      "metadata": {
        "id": "gSdXrGRB_LW0",
        "colab_type": "code",
        "outputId": "ce348328-91a8-4bd1-9b2f-706b3a271872",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 13937
        }
      },
      "cell_type": "code",
      "source": [
        "MultipleRun(runs,epochs,CNN_1())"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   --  Run :  1\n",
            "CNN_1  Epoch: 1 [0/45056 (0%)]\tLoss: 2.309916\n",
            "CNN_1  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.894862\n",
            "CNN_1  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.524122\n",
            "CNN_1  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.325387\n",
            "\n",
            "Test set: Average loss: 0.2905, Accuracy: 13695/15000 (91.30%)\n",
            "\n",
            "new accuracy parameters saved 0.913\n",
            "CNN_1  Epoch: 2 [0/45056 (0%)]\tLoss: 0.296037\n",
            "CNN_1  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.356762\n",
            "CNN_1  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.217589\n",
            "CNN_1  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.136585\n",
            "\n",
            "Test set: Average loss: 0.1777, Accuracy: 14192/15000 (94.61%)\n",
            "\n",
            "new accuracy parameters saved 0.9461333333333334\n",
            "CNN_1  Epoch: 3 [0/45056 (0%)]\tLoss: 0.190420\n",
            "CNN_1  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.095725\n",
            "CNN_1  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.164518\n",
            "CNN_1  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.240140\n",
            "\n",
            "Test set: Average loss: 0.1457, Accuracy: 14259/15000 (95.06%)\n",
            "\n",
            "new accuracy parameters saved 0.9506\n",
            "CNN_1  Epoch: 4 [0/45056 (0%)]\tLoss: 0.071724\n",
            "CNN_1  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.119588\n",
            "CNN_1  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.046168\n",
            "CNN_1  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.107785\n",
            "\n",
            "Test set: Average loss: 0.1192, Accuracy: 14422/15000 (96.15%)\n",
            "\n",
            "new accuracy parameters saved 0.9614666666666667\n",
            "CNN_1  Epoch: 5 [0/45056 (0%)]\tLoss: 0.072999\n",
            "CNN_1  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.211630\n",
            "CNN_1  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.084627\n",
            "CNN_1  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.092682\n",
            "\n",
            "Test set: Average loss: 0.1046, Accuracy: 14497/15000 (96.65%)\n",
            "\n",
            "new accuracy parameters saved 0.9664666666666667\n",
            "CNN_1  Epoch: 6 [0/45056 (0%)]\tLoss: 0.086588\n",
            "CNN_1  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.097929\n",
            "CNN_1  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.090910\n",
            "CNN_1  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.064214\n",
            "\n",
            "Test set: Average loss: 0.0939, Accuracy: 14529/15000 (96.86%)\n",
            "\n",
            "new accuracy parameters saved 0.9686\n",
            "CNN_1  Epoch: 7 [0/45056 (0%)]\tLoss: 0.099359\n",
            "CNN_1  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.106358\n",
            "CNN_1  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.193278\n",
            "CNN_1  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.049581\n",
            "\n",
            "Test set: Average loss: 0.1010, Accuracy: 14498/15000 (96.65%)\n",
            "\n",
            "CNN_1  Epoch: 8 [0/45056 (0%)]\tLoss: 0.071752\n",
            "CNN_1  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.033163\n",
            "CNN_1  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.072539\n",
            "CNN_1  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.109800\n",
            "\n",
            "Test set: Average loss: 0.0974, Accuracy: 14533/15000 (96.89%)\n",
            "\n",
            "new accuracy parameters saved 0.9688666666666667\n",
            "CNN_1  Epoch: 9 [0/45056 (0%)]\tLoss: 0.073712\n",
            "CNN_1  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.038170\n",
            "CNN_1  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.093374\n",
            "CNN_1  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.060346\n",
            "\n",
            "Test set: Average loss: 0.0821, Accuracy: 14594/15000 (97.29%)\n",
            "\n",
            "new accuracy parameters saved 0.9729333333333333\n",
            "CNN_1  Epoch: 10 [0/45056 (0%)]\tLoss: 0.031173\n",
            "CNN_1  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.137454\n",
            "CNN_1  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.060422\n",
            "CNN_1  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.112676\n",
            "\n",
            "Test set: Average loss: 0.0793, Accuracy: 14619/15000 (97.46%)\n",
            "\n",
            "new accuracy parameters saved 0.9746\n",
            "Training complete in 1m 7s\n",
            "\n",
            "   --  Run :  2\n",
            "CNN_1  Epoch: 1 [0/45056 (0%)]\tLoss: 2.317336\n",
            "CNN_1  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.948699\n",
            "CNN_1  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.570911\n",
            "CNN_1  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.339186\n",
            "\n",
            "Test set: Average loss: 0.2907, Accuracy: 13667/15000 (91.11%)\n",
            "\n",
            "new accuracy parameters saved 0.9111333333333334\n",
            "CNN_1  Epoch: 2 [0/45056 (0%)]\tLoss: 0.313963\n",
            "CNN_1  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.220325\n",
            "CNN_1  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.150709\n",
            "CNN_1  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.115616\n",
            "\n",
            "Test set: Average loss: 0.2163, Accuracy: 13984/15000 (93.23%)\n",
            "\n",
            "new accuracy parameters saved 0.9322666666666667\n",
            "CNN_1  Epoch: 3 [0/45056 (0%)]\tLoss: 0.253941\n",
            "CNN_1  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.210749\n",
            "CNN_1  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.117836\n",
            "CNN_1  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.140452\n",
            "\n",
            "Test set: Average loss: 0.2196, Accuracy: 13876/15000 (92.51%)\n",
            "\n",
            "CNN_1  Epoch: 4 [0/45056 (0%)]\tLoss: 0.206976\n",
            "CNN_1  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.087240\n",
            "CNN_1  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.212090\n",
            "CNN_1  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.123042\n",
            "\n",
            "Test set: Average loss: 0.1173, Accuracy: 14457/15000 (96.38%)\n",
            "\n",
            "new accuracy parameters saved 0.9638\n",
            "CNN_1  Epoch: 5 [0/45056 (0%)]\tLoss: 0.131707\n",
            "CNN_1  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.075259\n",
            "CNN_1  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.088232\n",
            "CNN_1  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.072761\n",
            "\n",
            "Test set: Average loss: 0.1041, Accuracy: 14485/15000 (96.57%)\n",
            "\n",
            "new accuracy parameters saved 0.9656666666666667\n",
            "CNN_1  Epoch: 6 [0/45056 (0%)]\tLoss: 0.115415\n",
            "CNN_1  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.096867\n",
            "CNN_1  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.072560\n",
            "CNN_1  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.069935\n",
            "\n",
            "Test set: Average loss: 0.0969, Accuracy: 14529/15000 (96.86%)\n",
            "\n",
            "new accuracy parameters saved 0.9686\n",
            "CNN_1  Epoch: 7 [0/45056 (0%)]\tLoss: 0.087094\n",
            "CNN_1  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.070559\n",
            "CNN_1  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.026572\n",
            "CNN_1  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.067239\n",
            "\n",
            "Test set: Average loss: 0.0923, Accuracy: 14559/15000 (97.06%)\n",
            "\n",
            "new accuracy parameters saved 0.9706\n",
            "CNN_1  Epoch: 8 [0/45056 (0%)]\tLoss: 0.152518\n",
            "CNN_1  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.107597\n",
            "CNN_1  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.067949\n",
            "CNN_1  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.038607\n",
            "\n",
            "Test set: Average loss: 0.0800, Accuracy: 14613/15000 (97.42%)\n",
            "\n",
            "new accuracy parameters saved 0.9742\n",
            "CNN_1  Epoch: 9 [0/45056 (0%)]\tLoss: 0.043299\n",
            "CNN_1  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.027495\n",
            "CNN_1  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.090232\n",
            "CNN_1  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.080364\n",
            "\n",
            "Test set: Average loss: 0.0761, Accuracy: 14636/15000 (97.57%)\n",
            "\n",
            "new accuracy parameters saved 0.9757333333333333\n",
            "CNN_1  Epoch: 10 [0/45056 (0%)]\tLoss: 0.026809\n",
            "CNN_1  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.027884\n",
            "CNN_1  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.019187\n",
            "CNN_1  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.075298\n",
            "\n",
            "Test set: Average loss: 0.1435, Accuracy: 14322/15000 (95.48%)\n",
            "\n",
            "Training complete in 1m 7s\n",
            "\n",
            "   --  Run :  3\n",
            "CNN_1  Epoch: 1 [0/45056 (0%)]\tLoss: 2.293956\n",
            "CNN_1  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.973112\n",
            "CNN_1  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.584940\n",
            "CNN_1  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.302869\n",
            "\n",
            "Test set: Average loss: 0.3092, Accuracy: 13608/15000 (90.72%)\n",
            "\n",
            "new accuracy parameters saved 0.9072\n",
            "CNN_1  Epoch: 2 [0/45056 (0%)]\tLoss: 0.265321\n",
            "CNN_1  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.263294\n",
            "CNN_1  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.154825\n",
            "CNN_1  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.183476\n",
            "\n",
            "Test set: Average loss: 0.1987, Accuracy: 14054/15000 (93.69%)\n",
            "\n",
            "new accuracy parameters saved 0.9369333333333333\n",
            "CNN_1  Epoch: 3 [0/45056 (0%)]\tLoss: 0.195261\n",
            "CNN_1  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.089059\n",
            "CNN_1  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.155438\n",
            "CNN_1  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.111083\n",
            "\n",
            "Test set: Average loss: 0.1332, Accuracy: 14367/15000 (95.78%)\n",
            "\n",
            "new accuracy parameters saved 0.9578\n",
            "CNN_1  Epoch: 4 [0/45056 (0%)]\tLoss: 0.051466\n",
            "CNN_1  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.122722\n",
            "CNN_1  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.111228\n",
            "CNN_1  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.125950\n",
            "\n",
            "Test set: Average loss: 0.1392, Accuracy: 14346/15000 (95.64%)\n",
            "\n",
            "CNN_1  Epoch: 5 [0/45056 (0%)]\tLoss: 0.050569\n",
            "CNN_1  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.051090\n",
            "CNN_1  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.130709\n",
            "CNN_1  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.060404\n",
            "\n",
            "Test set: Average loss: 0.1041, Accuracy: 14481/15000 (96.54%)\n",
            "\n",
            "new accuracy parameters saved 0.9654\n",
            "CNN_1  Epoch: 6 [0/45056 (0%)]\tLoss: 0.139773\n",
            "CNN_1  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.087716\n",
            "CNN_1  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.093287\n",
            "CNN_1  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.071245\n",
            "\n",
            "Test set: Average loss: 0.1149, Accuracy: 14413/15000 (96.09%)\n",
            "\n",
            "CNN_1  Epoch: 7 [0/45056 (0%)]\tLoss: 0.075873\n",
            "CNN_1  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.142673\n",
            "CNN_1  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.058573\n",
            "CNN_1  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.076050\n",
            "\n",
            "Test set: Average loss: 0.0853, Accuracy: 14577/15000 (97.18%)\n",
            "\n",
            "new accuracy parameters saved 0.9718\n",
            "CNN_1  Epoch: 8 [0/45056 (0%)]\tLoss: 0.027227\n",
            "CNN_1  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.078851\n",
            "CNN_1  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.078376\n",
            "CNN_1  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.116490\n",
            "\n",
            "Test set: Average loss: 0.0891, Accuracy: 14564/15000 (97.09%)\n",
            "\n",
            "CNN_1  Epoch: 9 [0/45056 (0%)]\tLoss: 0.030119\n",
            "CNN_1  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.091421\n",
            "CNN_1  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.107548\n",
            "CNN_1  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.049175\n",
            "\n",
            "Test set: Average loss: 0.0777, Accuracy: 14624/15000 (97.49%)\n",
            "\n",
            "new accuracy parameters saved 0.9749333333333333\n",
            "CNN_1  Epoch: 10 [0/45056 (0%)]\tLoss: 0.034104\n",
            "CNN_1  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.025374\n",
            "CNN_1  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.026002\n",
            "CNN_1  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.032823\n",
            "\n",
            "Test set: Average loss: 0.0826, Accuracy: 14594/15000 (97.29%)\n",
            "\n",
            "Training complete in 1m 7s\n",
            "\n",
            "   --  Run :  4\n",
            "CNN_1  Epoch: 1 [0/45056 (0%)]\tLoss: 2.320024\n",
            "CNN_1  Epoch: 1 [12800/45056 (28%)]\tLoss: 1.125904\n",
            "CNN_1  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.704241\n",
            "CNN_1  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.434928\n",
            "\n",
            "Test set: Average loss: 0.2862, Accuracy: 13709/15000 (91.39%)\n",
            "\n",
            "new accuracy parameters saved 0.9139333333333334\n",
            "CNN_1  Epoch: 2 [0/45056 (0%)]\tLoss: 0.155891\n",
            "CNN_1  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.221794\n",
            "CNN_1  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.243501\n",
            "CNN_1  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.168275\n",
            "\n",
            "Test set: Average loss: 0.1853, Accuracy: 14127/15000 (94.18%)\n",
            "\n",
            "new accuracy parameters saved 0.9418\n",
            "CNN_1  Epoch: 3 [0/45056 (0%)]\tLoss: 0.193860\n",
            "CNN_1  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.167859\n",
            "CNN_1  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.200917\n",
            "CNN_1  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.179192\n",
            "\n",
            "Test set: Average loss: 0.1425, Accuracy: 14319/15000 (95.46%)\n",
            "\n",
            "new accuracy parameters saved 0.9546\n",
            "CNN_1  Epoch: 4 [0/45056 (0%)]\tLoss: 0.149091\n",
            "CNN_1  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.224480\n",
            "CNN_1  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.108458\n",
            "CNN_1  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.130382\n",
            "\n",
            "Test set: Average loss: 0.1202, Accuracy: 14420/15000 (96.13%)\n",
            "\n",
            "new accuracy parameters saved 0.9613333333333334\n",
            "CNN_1  Epoch: 5 [0/45056 (0%)]\tLoss: 0.065500\n",
            "CNN_1  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.083966\n",
            "CNN_1  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.127936\n",
            "CNN_1  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.074035\n",
            "\n",
            "Test set: Average loss: 0.1041, Accuracy: 14499/15000 (96.66%)\n",
            "\n",
            "new accuracy parameters saved 0.9666\n",
            "CNN_1  Epoch: 6 [0/45056 (0%)]\tLoss: 0.105034\n",
            "CNN_1  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.087281\n",
            "CNN_1  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.080931\n",
            "CNN_1  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.116170\n",
            "\n",
            "Test set: Average loss: 0.0950, Accuracy: 14545/15000 (96.97%)\n",
            "\n",
            "new accuracy parameters saved 0.9696666666666667\n",
            "CNN_1  Epoch: 7 [0/45056 (0%)]\tLoss: 0.054345\n",
            "CNN_1  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.051836\n",
            "CNN_1  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.039112\n",
            "CNN_1  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.037868\n",
            "\n",
            "Test set: Average loss: 0.0896, Accuracy: 14555/15000 (97.03%)\n",
            "\n",
            "new accuracy parameters saved 0.9703333333333334\n",
            "CNN_1  Epoch: 8 [0/45056 (0%)]\tLoss: 0.064036\n",
            "CNN_1  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.055007\n",
            "CNN_1  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.067840\n",
            "CNN_1  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.039930\n",
            "\n",
            "Test set: Average loss: 0.0811, Accuracy: 14597/15000 (97.31%)\n",
            "\n",
            "new accuracy parameters saved 0.9731333333333333\n",
            "CNN_1  Epoch: 9 [0/45056 (0%)]\tLoss: 0.029444\n",
            "CNN_1  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.062161\n",
            "CNN_1  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.077190\n",
            "CNN_1  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.050612\n",
            "\n",
            "Test set: Average loss: 0.0745, Accuracy: 14642/15000 (97.61%)\n",
            "\n",
            "new accuracy parameters saved 0.9761333333333333\n",
            "CNN_1  Epoch: 10 [0/45056 (0%)]\tLoss: 0.078344\n",
            "CNN_1  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.106993\n",
            "CNN_1  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.123195\n",
            "CNN_1  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.053916\n",
            "\n",
            "Test set: Average loss: 0.0717, Accuracy: 14643/15000 (97.62%)\n",
            "\n",
            "new accuracy parameters saved 0.9762\n",
            "Training complete in 1m 7s\n",
            "\n",
            "   --  Run :  5\n",
            "CNN_1  Epoch: 1 [0/45056 (0%)]\tLoss: 2.304358\n",
            "CNN_1  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.947180\n",
            "CNN_1  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.588659\n",
            "CNN_1  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.431377\n",
            "\n",
            "Test set: Average loss: 0.2719, Accuracy: 13849/15000 (92.33%)\n",
            "\n",
            "new accuracy parameters saved 0.9232666666666667\n",
            "CNN_1  Epoch: 2 [0/45056 (0%)]\tLoss: 0.176216\n",
            "CNN_1  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.252506\n",
            "CNN_1  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.147825\n",
            "CNN_1  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.206439\n",
            "\n",
            "Test set: Average loss: 0.1832, Accuracy: 14151/15000 (94.34%)\n",
            "\n",
            "new accuracy parameters saved 0.9434\n",
            "CNN_1  Epoch: 3 [0/45056 (0%)]\tLoss: 0.199038\n",
            "CNN_1  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.241031\n",
            "CNN_1  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.150866\n",
            "CNN_1  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.100835\n",
            "\n",
            "Test set: Average loss: 0.1528, Accuracy: 14282/15000 (95.21%)\n",
            "\n",
            "new accuracy parameters saved 0.9521333333333334\n",
            "CNN_1  Epoch: 4 [0/45056 (0%)]\tLoss: 0.127788\n",
            "CNN_1  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.103280\n",
            "CNN_1  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.161008\n",
            "CNN_1  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.101126\n",
            "\n",
            "Test set: Average loss: 0.1177, Accuracy: 14408/15000 (96.05%)\n",
            "\n",
            "new accuracy parameters saved 0.9605333333333334\n",
            "CNN_1  Epoch: 5 [0/45056 (0%)]\tLoss: 0.102874\n",
            "CNN_1  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.129263\n",
            "CNN_1  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.124863\n",
            "CNN_1  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.104826\n",
            "\n",
            "Test set: Average loss: 0.1315, Accuracy: 14340/15000 (95.60%)\n",
            "\n",
            "CNN_1  Epoch: 6 [0/45056 (0%)]\tLoss: 0.172933\n",
            "CNN_1  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.085604\n",
            "CNN_1  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.072539\n",
            "CNN_1  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.144720\n",
            "\n",
            "Test set: Average loss: 0.1012, Accuracy: 14512/15000 (96.75%)\n",
            "\n",
            "new accuracy parameters saved 0.9674666666666667\n",
            "CNN_1  Epoch: 7 [0/45056 (0%)]\tLoss: 0.060806\n",
            "CNN_1  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.120762\n",
            "CNN_1  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.047822\n",
            "CNN_1  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.044966\n",
            "\n",
            "Test set: Average loss: 0.0974, Accuracy: 14519/15000 (96.79%)\n",
            "\n",
            "new accuracy parameters saved 0.9679333333333333\n",
            "CNN_1  Epoch: 8 [0/45056 (0%)]\tLoss: 0.065719\n",
            "CNN_1  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.054299\n",
            "CNN_1  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.094889\n",
            "CNN_1  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.099530\n",
            "\n",
            "Test set: Average loss: 0.1058, Accuracy: 14490/15000 (96.60%)\n",
            "\n",
            "CNN_1  Epoch: 9 [0/45056 (0%)]\tLoss: 0.099457\n",
            "CNN_1  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.042113\n",
            "CNN_1  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.094608\n",
            "CNN_1  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.055335\n",
            "\n",
            "Test set: Average loss: 0.1051, Accuracy: 14494/15000 (96.63%)\n",
            "\n",
            "CNN_1  Epoch: 10 [0/45056 (0%)]\tLoss: 0.072166\n",
            "CNN_1  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.097326\n",
            "CNN_1  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.156481\n",
            "CNN_1  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.042885\n",
            "\n",
            "Test set: Average loss: 0.0841, Accuracy: 14586/15000 (97.24%)\n",
            "\n",
            "new accuracy parameters saved 0.9724\n",
            "Training complete in 1m 7s\n",
            "\n",
            "   --  Run :  6\n",
            "CNN_1  Epoch: 1 [0/45056 (0%)]\tLoss: 2.296993\n",
            "CNN_1  Epoch: 1 [12800/45056 (28%)]\tLoss: 1.050443\n",
            "CNN_1  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.582622\n",
            "CNN_1  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.410521\n",
            "\n",
            "Test set: Average loss: 0.2986, Accuracy: 13653/15000 (91.02%)\n",
            "\n",
            "new accuracy parameters saved 0.9102\n",
            "CNN_1  Epoch: 2 [0/45056 (0%)]\tLoss: 0.278277\n",
            "CNN_1  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.217444\n",
            "CNN_1  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.179659\n",
            "CNN_1  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.214130\n",
            "\n",
            "Test set: Average loss: 0.1760, Accuracy: 14182/15000 (94.55%)\n",
            "\n",
            "new accuracy parameters saved 0.9454666666666667\n",
            "CNN_1  Epoch: 3 [0/45056 (0%)]\tLoss: 0.113877\n",
            "CNN_1  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.102168\n",
            "CNN_1  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.164601\n",
            "CNN_1  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.195766\n",
            "\n",
            "Test set: Average loss: 0.1704, Accuracy: 14192/15000 (94.61%)\n",
            "\n",
            "new accuracy parameters saved 0.9461333333333334\n",
            "CNN_1  Epoch: 4 [0/45056 (0%)]\tLoss: 0.108443\n",
            "CNN_1  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.135547\n",
            "CNN_1  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.150060\n",
            "CNN_1  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.089210\n",
            "\n",
            "Test set: Average loss: 0.1153, Accuracy: 14469/15000 (96.46%)\n",
            "\n",
            "new accuracy parameters saved 0.9646\n",
            "CNN_1  Epoch: 5 [0/45056 (0%)]\tLoss: 0.135418\n",
            "CNN_1  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.151547\n",
            "CNN_1  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.064885\n",
            "CNN_1  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.188208\n",
            "\n",
            "Test set: Average loss: 0.1082, Accuracy: 14475/15000 (96.50%)\n",
            "\n",
            "new accuracy parameters saved 0.965\n",
            "CNN_1  Epoch: 6 [0/45056 (0%)]\tLoss: 0.096522\n",
            "CNN_1  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.133001\n",
            "CNN_1  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.140457\n",
            "CNN_1  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.095957\n",
            "\n",
            "Test set: Average loss: 0.1029, Accuracy: 14510/15000 (96.73%)\n",
            "\n",
            "new accuracy parameters saved 0.9673333333333334\n",
            "CNN_1  Epoch: 7 [0/45056 (0%)]\tLoss: 0.066501\n",
            "CNN_1  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.065749\n",
            "CNN_1  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.088953\n",
            "CNN_1  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.031322\n",
            "\n",
            "Test set: Average loss: 0.0846, Accuracy: 14602/15000 (97.35%)\n",
            "\n",
            "new accuracy parameters saved 0.9734666666666667\n",
            "CNN_1  Epoch: 8 [0/45056 (0%)]\tLoss: 0.079159\n",
            "CNN_1  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.029453\n",
            "CNN_1  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.038270\n",
            "CNN_1  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.154381\n",
            "\n",
            "Test set: Average loss: 0.0853, Accuracy: 14580/15000 (97.20%)\n",
            "\n",
            "CNN_1  Epoch: 9 [0/45056 (0%)]\tLoss: 0.048593\n",
            "CNN_1  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.044304\n",
            "CNN_1  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.058334\n",
            "CNN_1  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.071197\n",
            "\n",
            "Test set: Average loss: 0.1136, Accuracy: 14454/15000 (96.36%)\n",
            "\n",
            "CNN_1  Epoch: 10 [0/45056 (0%)]\tLoss: 0.138085\n",
            "CNN_1  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.049052\n",
            "CNN_1  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.048350\n",
            "CNN_1  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.122984\n",
            "\n",
            "Test set: Average loss: 0.0754, Accuracy: 14647/15000 (97.65%)\n",
            "\n",
            "new accuracy parameters saved 0.9764666666666667\n",
            "Training complete in 1m 7s\n",
            "\n",
            "   --  Run :  7\n",
            "CNN_1  Epoch: 1 [0/45056 (0%)]\tLoss: 2.299340\n",
            "CNN_1  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.874169\n",
            "CNN_1  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.548035\n",
            "CNN_1  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.388606\n",
            "\n",
            "Test set: Average loss: 0.2768, Accuracy: 13764/15000 (91.76%)\n",
            "\n",
            "new accuracy parameters saved 0.9176\n",
            "CNN_1  Epoch: 2 [0/45056 (0%)]\tLoss: 0.234875\n",
            "CNN_1  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.171173\n",
            "CNN_1  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.135050\n",
            "CNN_1  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.183236\n",
            "\n",
            "Test set: Average loss: 0.1793, Accuracy: 14144/15000 (94.29%)\n",
            "\n",
            "new accuracy parameters saved 0.9429333333333333\n",
            "CNN_1  Epoch: 3 [0/45056 (0%)]\tLoss: 0.168156\n",
            "CNN_1  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.191012\n",
            "CNN_1  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.092208\n",
            "CNN_1  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.091586\n",
            "\n",
            "Test set: Average loss: 0.1435, Accuracy: 14339/15000 (95.59%)\n",
            "\n",
            "new accuracy parameters saved 0.9559333333333333\n",
            "CNN_1  Epoch: 4 [0/45056 (0%)]\tLoss: 0.073641\n",
            "CNN_1  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.179714\n",
            "CNN_1  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.111589\n",
            "CNN_1  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.083508\n",
            "\n",
            "Test set: Average loss: 0.1144, Accuracy: 14449/15000 (96.33%)\n",
            "\n",
            "new accuracy parameters saved 0.9632666666666667\n",
            "CNN_1  Epoch: 5 [0/45056 (0%)]\tLoss: 0.079133\n",
            "CNN_1  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.164876\n",
            "CNN_1  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.137460\n",
            "CNN_1  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.153091\n",
            "\n",
            "Test set: Average loss: 0.1421, Accuracy: 14288/15000 (95.25%)\n",
            "\n",
            "CNN_1  Epoch: 6 [0/45056 (0%)]\tLoss: 0.103799\n",
            "CNN_1  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.102148\n",
            "CNN_1  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.138805\n",
            "CNN_1  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.095307\n",
            "\n",
            "Test set: Average loss: 0.1119, Accuracy: 14473/15000 (96.49%)\n",
            "\n",
            "new accuracy parameters saved 0.9648666666666667\n",
            "CNN_1  Epoch: 7 [0/45056 (0%)]\tLoss: 0.089259\n",
            "CNN_1  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.114259\n",
            "CNN_1  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.072338\n",
            "CNN_1  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.059699\n",
            "\n",
            "Test set: Average loss: 0.1038, Accuracy: 14492/15000 (96.61%)\n",
            "\n",
            "new accuracy parameters saved 0.9661333333333333\n",
            "CNN_1  Epoch: 8 [0/45056 (0%)]\tLoss: 0.078603\n",
            "CNN_1  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.051208\n",
            "CNN_1  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.056302\n",
            "CNN_1  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.058472\n",
            "\n",
            "Test set: Average loss: 0.0788, Accuracy: 14624/15000 (97.49%)\n",
            "\n",
            "new accuracy parameters saved 0.9749333333333333\n",
            "CNN_1  Epoch: 9 [0/45056 (0%)]\tLoss: 0.077085\n",
            "CNN_1  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.030621\n",
            "CNN_1  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.064746\n",
            "CNN_1  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.045855\n",
            "\n",
            "Test set: Average loss: 0.0722, Accuracy: 14655/15000 (97.70%)\n",
            "\n",
            "new accuracy parameters saved 0.977\n",
            "CNN_1  Epoch: 10 [0/45056 (0%)]\tLoss: 0.112379\n",
            "CNN_1  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.079759\n",
            "CNN_1  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.176677\n",
            "CNN_1  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.054422\n",
            "\n",
            "Test set: Average loss: 0.0876, Accuracy: 14570/15000 (97.13%)\n",
            "\n",
            "Training complete in 1m 7s\n",
            "\n",
            "   --  Run :  8\n",
            "CNN_1  Epoch: 1 [0/45056 (0%)]\tLoss: 2.318520\n",
            "CNN_1  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.979567\n",
            "CNN_1  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.501264\n",
            "CNN_1  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.405517\n",
            "\n",
            "Test set: Average loss: 0.3414, Accuracy: 13513/15000 (90.09%)\n",
            "\n",
            "new accuracy parameters saved 0.9008666666666667\n",
            "CNN_1  Epoch: 2 [0/45056 (0%)]\tLoss: 0.292157\n",
            "CNN_1  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.157887\n",
            "CNN_1  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.202530\n",
            "CNN_1  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.179739\n",
            "\n",
            "Test set: Average loss: 0.1826, Accuracy: 14161/15000 (94.41%)\n",
            "\n",
            "new accuracy parameters saved 0.9440666666666667\n",
            "CNN_1  Epoch: 3 [0/45056 (0%)]\tLoss: 0.214923\n",
            "CNN_1  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.222282\n",
            "CNN_1  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.191723\n",
            "CNN_1  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.102955\n",
            "\n",
            "Test set: Average loss: 0.1389, Accuracy: 14339/15000 (95.59%)\n",
            "\n",
            "new accuracy parameters saved 0.9559333333333333\n",
            "CNN_1  Epoch: 4 [0/45056 (0%)]\tLoss: 0.103197\n",
            "CNN_1  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.178787\n",
            "CNN_1  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.132452\n",
            "CNN_1  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.085363\n",
            "\n",
            "Test set: Average loss: 0.1260, Accuracy: 14398/15000 (95.99%)\n",
            "\n",
            "new accuracy parameters saved 0.9598666666666666\n",
            "CNN_1  Epoch: 5 [0/45056 (0%)]\tLoss: 0.081561\n",
            "CNN_1  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.211616\n",
            "CNN_1  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.096551\n",
            "CNN_1  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.120384\n",
            "\n",
            "Test set: Average loss: 0.1086, Accuracy: 14457/15000 (96.38%)\n",
            "\n",
            "new accuracy parameters saved 0.9638\n",
            "CNN_1  Epoch: 6 [0/45056 (0%)]\tLoss: 0.085424\n",
            "CNN_1  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.031319\n",
            "CNN_1  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.055458\n",
            "CNN_1  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.067366\n",
            "\n",
            "Test set: Average loss: 0.1009, Accuracy: 14515/15000 (96.77%)\n",
            "\n",
            "new accuracy parameters saved 0.9676666666666667\n",
            "CNN_1  Epoch: 7 [0/45056 (0%)]\tLoss: 0.089099\n",
            "CNN_1  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.035316\n",
            "CNN_1  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.149432\n",
            "CNN_1  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.121191\n",
            "\n",
            "Test set: Average loss: 0.0841, Accuracy: 14589/15000 (97.26%)\n",
            "\n",
            "new accuracy parameters saved 0.9726\n",
            "CNN_1  Epoch: 8 [0/45056 (0%)]\tLoss: 0.081339\n",
            "CNN_1  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.074314\n",
            "CNN_1  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.087270\n",
            "CNN_1  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.042991\n",
            "\n",
            "Test set: Average loss: 0.0786, Accuracy: 14633/15000 (97.55%)\n",
            "\n",
            "new accuracy parameters saved 0.9755333333333334\n",
            "CNN_1  Epoch: 9 [0/45056 (0%)]\tLoss: 0.045092\n",
            "CNN_1  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.150339\n",
            "CNN_1  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.046867\n",
            "CNN_1  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.045222\n",
            "\n",
            "Test set: Average loss: 0.0752, Accuracy: 14652/15000 (97.68%)\n",
            "\n",
            "new accuracy parameters saved 0.9768\n",
            "CNN_1  Epoch: 10 [0/45056 (0%)]\tLoss: 0.034148\n",
            "CNN_1  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.054804\n",
            "CNN_1  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.044703\n",
            "CNN_1  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.088401\n",
            "\n",
            "Test set: Average loss: 0.1018, Accuracy: 14505/15000 (96.70%)\n",
            "\n",
            "Training complete in 1m 6s\n",
            "\n",
            "   --  Run :  9\n",
            "CNN_1  Epoch: 1 [0/45056 (0%)]\tLoss: 2.308646\n",
            "CNN_1  Epoch: 1 [12800/45056 (28%)]\tLoss: 1.371506\n",
            "CNN_1  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.579413\n",
            "CNN_1  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.387086\n",
            "\n",
            "Test set: Average loss: 0.3542, Accuracy: 13361/15000 (89.07%)\n",
            "\n",
            "new accuracy parameters saved 0.8907333333333334\n",
            "CNN_1  Epoch: 2 [0/45056 (0%)]\tLoss: 0.331835\n",
            "CNN_1  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.164300\n",
            "CNN_1  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.308046\n",
            "CNN_1  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.196479\n",
            "\n",
            "Test set: Average loss: 0.1780, Accuracy: 14113/15000 (94.09%)\n",
            "\n",
            "new accuracy parameters saved 0.9408666666666666\n",
            "CNN_1  Epoch: 3 [0/45056 (0%)]\tLoss: 0.196468\n",
            "CNN_1  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.098734\n",
            "CNN_1  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.091643\n",
            "CNN_1  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.210216\n",
            "\n",
            "Test set: Average loss: 0.1497, Accuracy: 14289/15000 (95.26%)\n",
            "\n",
            "new accuracy parameters saved 0.9526\n",
            "CNN_1  Epoch: 4 [0/45056 (0%)]\tLoss: 0.200948\n",
            "CNN_1  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.128286\n",
            "CNN_1  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.131626\n",
            "CNN_1  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.077812\n",
            "\n",
            "Test set: Average loss: 0.1171, Accuracy: 14423/15000 (96.15%)\n",
            "\n",
            "new accuracy parameters saved 0.9615333333333334\n",
            "CNN_1  Epoch: 5 [0/45056 (0%)]\tLoss: 0.065948\n",
            "CNN_1  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.094393\n",
            "CNN_1  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.091504\n",
            "CNN_1  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.083778\n",
            "\n",
            "Test set: Average loss: 0.1051, Accuracy: 14499/15000 (96.66%)\n",
            "\n",
            "new accuracy parameters saved 0.9666\n",
            "CNN_1  Epoch: 6 [0/45056 (0%)]\tLoss: 0.053290\n",
            "CNN_1  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.158028\n",
            "CNN_1  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.133005\n",
            "CNN_1  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.072809\n",
            "\n",
            "Test set: Average loss: 0.1057, Accuracy: 14513/15000 (96.75%)\n",
            "\n",
            "new accuracy parameters saved 0.9675333333333334\n",
            "CNN_1  Epoch: 7 [0/45056 (0%)]\tLoss: 0.116542\n",
            "CNN_1  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.080266\n",
            "CNN_1  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.126209\n",
            "CNN_1  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.057810\n",
            "\n",
            "Test set: Average loss: 0.0917, Accuracy: 14554/15000 (97.03%)\n",
            "\n",
            "new accuracy parameters saved 0.9702666666666667\n",
            "CNN_1  Epoch: 8 [0/45056 (0%)]\tLoss: 0.064963\n",
            "CNN_1  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.109923\n",
            "CNN_1  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.098243\n",
            "CNN_1  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.051001\n",
            "\n",
            "Test set: Average loss: 0.1036, Accuracy: 14488/15000 (96.59%)\n",
            "\n",
            "CNN_1  Epoch: 9 [0/45056 (0%)]\tLoss: 0.050610\n",
            "CNN_1  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.116562\n",
            "CNN_1  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.053880\n",
            "CNN_1  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.087409\n",
            "\n",
            "Test set: Average loss: 0.0941, Accuracy: 14551/15000 (97.01%)\n",
            "\n",
            "CNN_1  Epoch: 10 [0/45056 (0%)]\tLoss: 0.054595\n",
            "CNN_1  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.088501\n",
            "CNN_1  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.153449\n",
            "CNN_1  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.076285\n",
            "\n",
            "Test set: Average loss: 0.0883, Accuracy: 14585/15000 (97.23%)\n",
            "\n",
            "new accuracy parameters saved 0.9723333333333334\n",
            "Training complete in 1m 7s\n",
            "\n",
            "   --  Run :  10\n",
            "CNN_1  Epoch: 1 [0/45056 (0%)]\tLoss: 2.309900\n",
            "CNN_1  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.926289\n",
            "CNN_1  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.600328\n",
            "CNN_1  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.342147\n",
            "\n",
            "Test set: Average loss: 0.2919, Accuracy: 13689/15000 (91.26%)\n",
            "\n",
            "new accuracy parameters saved 0.9126\n",
            "CNN_1  Epoch: 2 [0/45056 (0%)]\tLoss: 0.217433\n",
            "CNN_1  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.234566\n",
            "CNN_1  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.117818\n",
            "CNN_1  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.179563\n",
            "\n",
            "Test set: Average loss: 0.1789, Accuracy: 14171/15000 (94.47%)\n",
            "\n",
            "new accuracy parameters saved 0.9447333333333333\n",
            "CNN_1  Epoch: 3 [0/45056 (0%)]\tLoss: 0.128685\n",
            "CNN_1  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.159236\n",
            "CNN_1  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.091916\n",
            "CNN_1  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.264608\n",
            "\n",
            "Test set: Average loss: 0.1411, Accuracy: 14329/15000 (95.53%)\n",
            "\n",
            "new accuracy parameters saved 0.9552666666666667\n",
            "CNN_1  Epoch: 4 [0/45056 (0%)]\tLoss: 0.171028\n",
            "CNN_1  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.163637\n",
            "CNN_1  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.163790\n",
            "CNN_1  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.077049\n",
            "\n",
            "Test set: Average loss: 0.1280, Accuracy: 14389/15000 (95.93%)\n",
            "\n",
            "new accuracy parameters saved 0.9592666666666667\n",
            "CNN_1  Epoch: 5 [0/45056 (0%)]\tLoss: 0.127491\n",
            "CNN_1  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.133521\n",
            "CNN_1  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.077018\n",
            "CNN_1  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.073088\n",
            "\n",
            "Test set: Average loss: 0.1105, Accuracy: 14476/15000 (96.51%)\n",
            "\n",
            "new accuracy parameters saved 0.9650666666666666\n",
            "CNN_1  Epoch: 6 [0/45056 (0%)]\tLoss: 0.095373\n",
            "CNN_1  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.112270\n",
            "CNN_1  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.062370\n",
            "CNN_1  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.087387\n",
            "\n",
            "Test set: Average loss: 0.1032, Accuracy: 14500/15000 (96.67%)\n",
            "\n",
            "new accuracy parameters saved 0.9666666666666667\n",
            "CNN_1  Epoch: 7 [0/45056 (0%)]\tLoss: 0.073609\n",
            "CNN_1  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.159113\n",
            "CNN_1  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.142834\n",
            "CNN_1  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.046977\n",
            "\n",
            "Test set: Average loss: 0.1043, Accuracy: 14510/15000 (96.73%)\n",
            "\n",
            "new accuracy parameters saved 0.9673333333333334\n",
            "CNN_1  Epoch: 8 [0/45056 (0%)]\tLoss: 0.034747\n",
            "CNN_1  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.065819\n",
            "CNN_1  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.038039\n",
            "CNN_1  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.061181\n",
            "\n",
            "Test set: Average loss: 0.0817, Accuracy: 14602/15000 (97.35%)\n",
            "\n",
            "new accuracy parameters saved 0.9734666666666667\n",
            "CNN_1  Epoch: 9 [0/45056 (0%)]\tLoss: 0.089728\n",
            "CNN_1  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.067609\n",
            "CNN_1  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.091666\n",
            "CNN_1  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.093743\n",
            "\n",
            "Test set: Average loss: 0.0783, Accuracy: 14628/15000 (97.52%)\n",
            "\n",
            "new accuracy parameters saved 0.9752\n",
            "CNN_1  Epoch: 10 [0/45056 (0%)]\tLoss: 0.063476\n",
            "CNN_1  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.073334\n",
            "CNN_1  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.124375\n",
            "CNN_1  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.066936\n",
            "\n",
            "Test set: Average loss: 0.0837, Accuracy: 14602/15000 (97.35%)\n",
            "\n",
            "Training complete in 1m 7s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pY-mUrGLKJnD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Running CNN2"
      ]
    },
    {
      "metadata": {
        "id": "qSQqcY_KKMFO",
        "colab_type": "code",
        "outputId": "3e35240a-aae2-4e8b-d1eb-fe9fea994b08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 13646
        }
      },
      "cell_type": "code",
      "source": [
        "MultipleRun(runs,epochs,CNN_2())"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   --  Run :  1\n",
            "CNN_2  Epoch: 1 [0/45056 (0%)]\tLoss: 2.302581\n",
            "CNN_2  Epoch: 1 [12800/45056 (28%)]\tLoss: 1.890951\n",
            "CNN_2  Epoch: 1 [25600/45056 (57%)]\tLoss: 1.227582\n",
            "CNN_2  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.432840\n",
            "\n",
            "Test set: Average loss: 0.5959, Accuracy: 11829/15000 (78.86%)\n",
            "\n",
            "new accuracy parameters saved 0.7886\n",
            "CNN_2  Epoch: 2 [0/45056 (0%)]\tLoss: 0.626503\n",
            "CNN_2  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.170889\n",
            "CNN_2  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.102885\n",
            "CNN_2  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.087838\n",
            "\n",
            "Test set: Average loss: 0.1559, Accuracy: 14288/15000 (95.25%)\n",
            "\n",
            "new accuracy parameters saved 0.9525333333333333\n",
            "CNN_2  Epoch: 3 [0/45056 (0%)]\tLoss: 0.138652\n",
            "CNN_2  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.130883\n",
            "CNN_2  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.164763\n",
            "CNN_2  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.138317\n",
            "\n",
            "Test set: Average loss: 0.0907, Accuracy: 14596/15000 (97.31%)\n",
            "\n",
            "new accuracy parameters saved 0.9730666666666666\n",
            "CNN_2  Epoch: 4 [0/45056 (0%)]\tLoss: 0.019037\n",
            "CNN_2  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.065327\n",
            "CNN_2  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.090688\n",
            "CNN_2  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.061938\n",
            "\n",
            "Test set: Average loss: 0.1065, Accuracy: 14483/15000 (96.55%)\n",
            "\n",
            "CNN_2  Epoch: 5 [0/45056 (0%)]\tLoss: 0.086317\n",
            "CNN_2  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.076214\n",
            "CNN_2  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.055940\n",
            "CNN_2  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.058060\n",
            "\n",
            "Test set: Average loss: 0.1006, Accuracy: 14542/15000 (96.95%)\n",
            "\n",
            "CNN_2  Epoch: 6 [0/45056 (0%)]\tLoss: 0.205942\n",
            "CNN_2  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.063589\n",
            "CNN_2  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.066009\n",
            "CNN_2  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.032444\n",
            "\n",
            "Test set: Average loss: 0.1046, Accuracy: 14505/15000 (96.70%)\n",
            "\n",
            "CNN_2  Epoch: 7 [0/45056 (0%)]\tLoss: 0.169834\n",
            "CNN_2  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.048009\n",
            "CNN_2  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.103506\n",
            "CNN_2  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.061927\n",
            "\n",
            "Test set: Average loss: 0.0745, Accuracy: 14642/15000 (97.61%)\n",
            "\n",
            "new accuracy parameters saved 0.9761333333333333\n",
            "CNN_2  Epoch: 8 [0/45056 (0%)]\tLoss: 0.142513\n",
            "CNN_2  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.059325\n",
            "CNN_2  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.022093\n",
            "CNN_2  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.034747\n",
            "\n",
            "Test set: Average loss: 0.0602, Accuracy: 14716/15000 (98.11%)\n",
            "\n",
            "new accuracy parameters saved 0.9810666666666666\n",
            "CNN_2  Epoch: 9 [0/45056 (0%)]\tLoss: 0.023945\n",
            "CNN_2  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.061365\n",
            "CNN_2  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.034656\n",
            "CNN_2  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.036243\n",
            "\n",
            "Test set: Average loss: 0.0962, Accuracy: 14521/15000 (96.81%)\n",
            "\n",
            "CNN_2  Epoch: 10 [0/45056 (0%)]\tLoss: 0.051430\n",
            "CNN_2  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.004506\n",
            "CNN_2  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.014663\n",
            "CNN_2  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.028020\n",
            "\n",
            "Test set: Average loss: 0.0467, Accuracy: 14775/15000 (98.50%)\n",
            "\n",
            "new accuracy parameters saved 0.985\n",
            "Training complete in 2m 32s\n",
            "\n",
            "   --  Run :  2\n",
            "CNN_2  Epoch: 1 [0/45056 (0%)]\tLoss: 2.306766\n",
            "CNN_2  Epoch: 1 [12800/45056 (28%)]\tLoss: 1.910212\n",
            "CNN_2  Epoch: 1 [25600/45056 (57%)]\tLoss: 1.573146\n",
            "CNN_2  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.436773\n",
            "\n",
            "Test set: Average loss: 0.4550, Accuracy: 12654/15000 (84.36%)\n",
            "\n",
            "new accuracy parameters saved 0.8436\n",
            "CNN_2  Epoch: 2 [0/45056 (0%)]\tLoss: 0.323835\n",
            "CNN_2  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.248452\n",
            "CNN_2  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.293482\n",
            "CNN_2  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.320025\n",
            "\n",
            "Test set: Average loss: 0.1324, Accuracy: 14383/15000 (95.89%)\n",
            "\n",
            "new accuracy parameters saved 0.9588666666666666\n",
            "CNN_2  Epoch: 3 [0/45056 (0%)]\tLoss: 0.108500\n",
            "CNN_2  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.144552\n",
            "CNN_2  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.071645\n",
            "CNN_2  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.062162\n",
            "\n",
            "Test set: Average loss: 0.1086, Accuracy: 14490/15000 (96.60%)\n",
            "\n",
            "new accuracy parameters saved 0.966\n",
            "CNN_2  Epoch: 4 [0/45056 (0%)]\tLoss: 0.078346\n",
            "CNN_2  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.094187\n",
            "CNN_2  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.095168\n",
            "CNN_2  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.082455\n",
            "\n",
            "Test set: Average loss: 0.0817, Accuracy: 14607/15000 (97.38%)\n",
            "\n",
            "new accuracy parameters saved 0.9738\n",
            "CNN_2  Epoch: 5 [0/45056 (0%)]\tLoss: 0.052150\n",
            "CNN_2  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.054273\n",
            "CNN_2  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.049719\n",
            "CNN_2  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.042187\n",
            "\n",
            "Test set: Average loss: 0.1477, Accuracy: 14527/15000 (96.85%)\n",
            "\n",
            "CNN_2  Epoch: 6 [0/45056 (0%)]\tLoss: 0.127612\n",
            "CNN_2  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.150256\n",
            "CNN_2  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.022767\n",
            "CNN_2  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.070961\n",
            "\n",
            "Test set: Average loss: 0.0716, Accuracy: 14657/15000 (97.71%)\n",
            "\n",
            "new accuracy parameters saved 0.9771333333333333\n",
            "CNN_2  Epoch: 7 [0/45056 (0%)]\tLoss: 0.049573\n",
            "CNN_2  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.055333\n",
            "CNN_2  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.064484\n",
            "CNN_2  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.030502\n",
            "\n",
            "Test set: Average loss: 0.0969, Accuracy: 14551/15000 (97.01%)\n",
            "\n",
            "CNN_2  Epoch: 8 [0/45056 (0%)]\tLoss: 0.041977\n",
            "CNN_2  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.006627\n",
            "CNN_2  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.039783\n",
            "CNN_2  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.047997\n",
            "\n",
            "Test set: Average loss: 0.0817, Accuracy: 14620/15000 (97.47%)\n",
            "\n",
            "CNN_2  Epoch: 9 [0/45056 (0%)]\tLoss: 0.085986\n",
            "CNN_2  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.046695\n",
            "CNN_2  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.009596\n",
            "CNN_2  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.006577\n",
            "\n",
            "Test set: Average loss: 0.0537, Accuracy: 14734/15000 (98.23%)\n",
            "\n",
            "new accuracy parameters saved 0.9822666666666666\n",
            "CNN_2  Epoch: 10 [0/45056 (0%)]\tLoss: 0.051328\n",
            "CNN_2  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.026201\n",
            "CNN_2  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.014214\n",
            "CNN_2  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.002441\n",
            "\n",
            "Test set: Average loss: 0.0798, Accuracy: 14618/15000 (97.45%)\n",
            "\n",
            "Training complete in 2m 32s\n",
            "\n",
            "   --  Run :  3\n",
            "CNN_2  Epoch: 1 [0/45056 (0%)]\tLoss: 2.306792\n",
            "CNN_2  Epoch: 1 [12800/45056 (28%)]\tLoss: 2.037208\n",
            "CNN_2  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.974585\n",
            "CNN_2  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.374624\n",
            "\n",
            "Test set: Average loss: 0.3385, Accuracy: 13691/15000 (91.27%)\n",
            "\n",
            "new accuracy parameters saved 0.9127333333333333\n",
            "CNN_2  Epoch: 2 [0/45056 (0%)]\tLoss: 0.409152\n",
            "CNN_2  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.196766\n",
            "CNN_2  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.230548\n",
            "CNN_2  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.189213\n",
            "\n",
            "Test set: Average loss: 0.1452, Accuracy: 14347/15000 (95.65%)\n",
            "\n",
            "new accuracy parameters saved 0.9564666666666667\n",
            "CNN_2  Epoch: 3 [0/45056 (0%)]\tLoss: 0.146172\n",
            "CNN_2  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.071445\n",
            "CNN_2  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.109410\n",
            "CNN_2  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.078482\n",
            "\n",
            "Test set: Average loss: 0.3111, Accuracy: 13471/15000 (89.81%)\n",
            "\n",
            "CNN_2  Epoch: 4 [0/45056 (0%)]\tLoss: 0.230200\n",
            "CNN_2  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.034576\n",
            "CNN_2  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.075671\n",
            "CNN_2  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.087050\n",
            "\n",
            "Test set: Average loss: 0.1009, Accuracy: 14504/15000 (96.69%)\n",
            "\n",
            "new accuracy parameters saved 0.9669333333333333\n",
            "CNN_2  Epoch: 5 [0/45056 (0%)]\tLoss: 0.084760\n",
            "CNN_2  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.089195\n",
            "CNN_2  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.064331\n",
            "CNN_2  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.048799\n",
            "\n",
            "Test set: Average loss: 0.1406, Accuracy: 14320/15000 (95.47%)\n",
            "\n",
            "CNN_2  Epoch: 6 [0/45056 (0%)]\tLoss: 0.112586\n",
            "CNN_2  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.035799\n",
            "CNN_2  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.059922\n",
            "CNN_2  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.053467\n",
            "\n",
            "Test set: Average loss: 0.0755, Accuracy: 14655/15000 (97.70%)\n",
            "\n",
            "new accuracy parameters saved 0.977\n",
            "CNN_2  Epoch: 7 [0/45056 (0%)]\tLoss: 0.079959\n",
            "CNN_2  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.068271\n",
            "CNN_2  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.044024\n",
            "CNN_2  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.036108\n",
            "\n",
            "Test set: Average loss: 0.0746, Accuracy: 14642/15000 (97.61%)\n",
            "\n",
            "CNN_2  Epoch: 8 [0/45056 (0%)]\tLoss: 0.061106\n",
            "CNN_2  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.024984\n",
            "CNN_2  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.066223\n",
            "CNN_2  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.022970\n",
            "\n",
            "Test set: Average loss: 0.0693, Accuracy: 14701/15000 (98.01%)\n",
            "\n",
            "new accuracy parameters saved 0.9800666666666666\n",
            "CNN_2  Epoch: 9 [0/45056 (0%)]\tLoss: 0.035251\n",
            "CNN_2  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.026092\n",
            "CNN_2  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.047464\n",
            "CNN_2  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.046329\n",
            "\n",
            "Test set: Average loss: 0.0814, Accuracy: 14604/15000 (97.36%)\n",
            "\n",
            "CNN_2  Epoch: 10 [0/45056 (0%)]\tLoss: 0.049660\n",
            "CNN_2  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.026277\n",
            "CNN_2  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.056997\n",
            "CNN_2  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.021757\n",
            "\n",
            "Test set: Average loss: 0.0554, Accuracy: 14746/15000 (98.31%)\n",
            "\n",
            "new accuracy parameters saved 0.9830666666666666\n",
            "Training complete in 2m 32s\n",
            "\n",
            "   --  Run :  4\n",
            "CNN_2  Epoch: 1 [0/45056 (0%)]\tLoss: 2.297544\n",
            "CNN_2  Epoch: 1 [12800/45056 (28%)]\tLoss: 1.875968\n",
            "CNN_2  Epoch: 1 [25600/45056 (57%)]\tLoss: 1.005838\n",
            "CNN_2  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.764324\n",
            "\n",
            "Test set: Average loss: 0.4358, Accuracy: 12986/15000 (86.57%)\n",
            "\n",
            "new accuracy parameters saved 0.8657333333333334\n",
            "CNN_2  Epoch: 2 [0/45056 (0%)]\tLoss: 0.449422\n",
            "CNN_2  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.338530\n",
            "CNN_2  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.166724\n",
            "CNN_2  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.164778\n",
            "\n",
            "Test set: Average loss: 0.1731, Accuracy: 14165/15000 (94.43%)\n",
            "\n",
            "new accuracy parameters saved 0.9443333333333334\n",
            "CNN_2  Epoch: 3 [0/45056 (0%)]\tLoss: 0.179088\n",
            "CNN_2  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.163488\n",
            "CNN_2  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.121209\n",
            "CNN_2  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.074714\n",
            "\n",
            "Test set: Average loss: 0.1064, Accuracy: 14509/15000 (96.73%)\n",
            "\n",
            "new accuracy parameters saved 0.9672666666666667\n",
            "CNN_2  Epoch: 4 [0/45056 (0%)]\tLoss: 0.075747\n",
            "CNN_2  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.109358\n",
            "CNN_2  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.079323\n",
            "CNN_2  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.036362\n",
            "\n",
            "Test set: Average loss: 0.0765, Accuracy: 14650/15000 (97.67%)\n",
            "\n",
            "new accuracy parameters saved 0.9766666666666667\n",
            "CNN_2  Epoch: 5 [0/45056 (0%)]\tLoss: 0.042241\n",
            "CNN_2  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.061229\n",
            "CNN_2  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.101176\n",
            "CNN_2  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.036720\n",
            "\n",
            "Test set: Average loss: 0.0941, Accuracy: 14545/15000 (96.97%)\n",
            "\n",
            "CNN_2  Epoch: 6 [0/45056 (0%)]\tLoss: 0.066029\n",
            "CNN_2  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.075759\n",
            "CNN_2  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.134730\n",
            "CNN_2  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.036964\n",
            "\n",
            "Test set: Average loss: 0.0662, Accuracy: 14688/15000 (97.92%)\n",
            "\n",
            "new accuracy parameters saved 0.9792\n",
            "CNN_2  Epoch: 7 [0/45056 (0%)]\tLoss: 0.106289\n",
            "CNN_2  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.075197\n",
            "CNN_2  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.202884\n",
            "CNN_2  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.085291\n",
            "\n",
            "Test set: Average loss: 0.0653, Accuracy: 14695/15000 (97.97%)\n",
            "\n",
            "new accuracy parameters saved 0.9796666666666667\n",
            "CNN_2  Epoch: 8 [0/45056 (0%)]\tLoss: 0.044793\n",
            "CNN_2  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.036547\n",
            "CNN_2  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.020898\n",
            "CNN_2  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.094406\n",
            "\n",
            "Test set: Average loss: 0.0673, Accuracy: 14695/15000 (97.97%)\n",
            "\n",
            "CNN_2  Epoch: 9 [0/45056 (0%)]\tLoss: 0.025973\n",
            "CNN_2  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.039245\n",
            "CNN_2  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.018083\n",
            "CNN_2  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.018124\n",
            "\n",
            "Test set: Average loss: 0.0506, Accuracy: 14762/15000 (98.41%)\n",
            "\n",
            "new accuracy parameters saved 0.9841333333333333\n",
            "CNN_2  Epoch: 10 [0/45056 (0%)]\tLoss: 0.023602\n",
            "CNN_2  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.032165\n",
            "CNN_2  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.032486\n",
            "CNN_2  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.034401\n",
            "\n",
            "Test set: Average loss: 0.0496, Accuracy: 14761/15000 (98.41%)\n",
            "\n",
            "Training complete in 2m 32s\n",
            "\n",
            "   --  Run :  5\n",
            "CNN_2  Epoch: 1 [0/45056 (0%)]\tLoss: 2.306946\n",
            "CNN_2  Epoch: 1 [12800/45056 (28%)]\tLoss: 1.880264\n",
            "CNN_2  Epoch: 1 [25600/45056 (57%)]\tLoss: 1.481397\n",
            "CNN_2  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.482564\n",
            "\n",
            "Test set: Average loss: 0.4286, Accuracy: 13007/15000 (86.71%)\n",
            "\n",
            "new accuracy parameters saved 0.8671333333333333\n",
            "CNN_2  Epoch: 2 [0/45056 (0%)]\tLoss: 0.508565\n",
            "CNN_2  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.193145\n",
            "CNN_2  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.241746\n",
            "CNN_2  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.063981\n",
            "\n",
            "Test set: Average loss: 0.1553, Accuracy: 14294/15000 (95.29%)\n",
            "\n",
            "new accuracy parameters saved 0.9529333333333333\n",
            "CNN_2  Epoch: 3 [0/45056 (0%)]\tLoss: 0.123273\n",
            "CNN_2  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.138824\n",
            "CNN_2  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.145980\n",
            "CNN_2  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.317966\n",
            "\n",
            "Test set: Average loss: 0.1385, Accuracy: 14365/15000 (95.77%)\n",
            "\n",
            "new accuracy parameters saved 0.9576666666666667\n",
            "CNN_2  Epoch: 4 [0/45056 (0%)]\tLoss: 0.089780\n",
            "CNN_2  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.058299\n",
            "CNN_2  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.074958\n",
            "CNN_2  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.050958\n",
            "\n",
            "Test set: Average loss: 0.1202, Accuracy: 14418/15000 (96.12%)\n",
            "\n",
            "new accuracy parameters saved 0.9612\n",
            "CNN_2  Epoch: 5 [0/45056 (0%)]\tLoss: 0.078243\n",
            "CNN_2  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.091491\n",
            "CNN_2  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.034351\n",
            "CNN_2  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.030768\n",
            "\n",
            "Test set: Average loss: 0.0914, Accuracy: 14563/15000 (97.09%)\n",
            "\n",
            "new accuracy parameters saved 0.9708666666666667\n",
            "CNN_2  Epoch: 6 [0/45056 (0%)]\tLoss: 0.061776\n",
            "CNN_2  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.092931\n",
            "CNN_2  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.025898\n",
            "CNN_2  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.056474\n",
            "\n",
            "Test set: Average loss: 0.0703, Accuracy: 14663/15000 (97.75%)\n",
            "\n",
            "new accuracy parameters saved 0.9775333333333334\n",
            "CNN_2  Epoch: 7 [0/45056 (0%)]\tLoss: 0.044727\n",
            "CNN_2  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.050253\n",
            "CNN_2  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.066404\n",
            "CNN_2  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.020043\n",
            "\n",
            "Test set: Average loss: 0.0735, Accuracy: 14662/15000 (97.75%)\n",
            "\n",
            "CNN_2  Epoch: 8 [0/45056 (0%)]\tLoss: 0.133281\n",
            "CNN_2  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.014623\n",
            "CNN_2  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.022561\n",
            "CNN_2  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.055387\n",
            "\n",
            "Test set: Average loss: 0.0960, Accuracy: 14552/15000 (97.01%)\n",
            "\n",
            "CNN_2  Epoch: 9 [0/45056 (0%)]\tLoss: 0.074438\n",
            "CNN_2  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.020037\n",
            "CNN_2  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.025161\n",
            "CNN_2  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.009820\n",
            "\n",
            "Test set: Average loss: 0.0538, Accuracy: 14756/15000 (98.37%)\n",
            "\n",
            "new accuracy parameters saved 0.9837333333333333\n",
            "CNN_2  Epoch: 10 [0/45056 (0%)]\tLoss: 0.045324\n",
            "CNN_2  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.021278\n",
            "CNN_2  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.007168\n",
            "CNN_2  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.048417\n",
            "\n",
            "Test set: Average loss: 0.0469, Accuracy: 14792/15000 (98.61%)\n",
            "\n",
            "new accuracy parameters saved 0.9861333333333333\n",
            "Training complete in 2m 32s\n",
            "\n",
            "   --  Run :  6\n",
            "CNN_2  Epoch: 1 [0/45056 (0%)]\tLoss: 2.298541\n",
            "CNN_2  Epoch: 1 [12800/45056 (28%)]\tLoss: 1.768635\n",
            "CNN_2  Epoch: 1 [25600/45056 (57%)]\tLoss: 1.173432\n",
            "CNN_2  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.524313\n",
            "\n",
            "Test set: Average loss: 0.3646, Accuracy: 13353/15000 (89.02%)\n",
            "\n",
            "new accuracy parameters saved 0.8902\n",
            "CNN_2  Epoch: 2 [0/45056 (0%)]\tLoss: 0.395336\n",
            "CNN_2  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.148689\n",
            "CNN_2  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.207860\n",
            "CNN_2  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.143098\n",
            "\n",
            "Test set: Average loss: 0.1742, Accuracy: 14228/15000 (94.85%)\n",
            "\n",
            "new accuracy parameters saved 0.9485333333333333\n",
            "CNN_2  Epoch: 3 [0/45056 (0%)]\tLoss: 0.121491\n",
            "CNN_2  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.059021\n",
            "CNN_2  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.056123\n",
            "CNN_2  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.078111\n",
            "\n",
            "Test set: Average loss: 0.2096, Accuracy: 14286/15000 (95.24%)\n",
            "\n",
            "new accuracy parameters saved 0.9524\n",
            "CNN_2  Epoch: 4 [0/45056 (0%)]\tLoss: 0.235425\n",
            "CNN_2  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.102652\n",
            "CNN_2  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.133040\n",
            "CNN_2  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.038646\n",
            "\n",
            "Test set: Average loss: 0.0809, Accuracy: 14643/15000 (97.62%)\n",
            "\n",
            "new accuracy parameters saved 0.9762\n",
            "CNN_2  Epoch: 5 [0/45056 (0%)]\tLoss: 0.144002\n",
            "CNN_2  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.119081\n",
            "CNN_2  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.131807\n",
            "CNN_2  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.042906\n",
            "\n",
            "Test set: Average loss: 0.1045, Accuracy: 14510/15000 (96.73%)\n",
            "\n",
            "CNN_2  Epoch: 6 [0/45056 (0%)]\tLoss: 0.077825\n",
            "CNN_2  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.065466\n",
            "CNN_2  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.087120\n",
            "CNN_2  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.047400\n",
            "\n",
            "Test set: Average loss: 0.0873, Accuracy: 14575/15000 (97.17%)\n",
            "\n",
            "CNN_2  Epoch: 7 [0/45056 (0%)]\tLoss: 0.109319\n",
            "CNN_2  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.018573\n",
            "CNN_2  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.065175\n",
            "CNN_2  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.015606\n",
            "\n",
            "Test set: Average loss: 0.0669, Accuracy: 14692/15000 (97.95%)\n",
            "\n",
            "new accuracy parameters saved 0.9794666666666667\n",
            "CNN_2  Epoch: 8 [0/45056 (0%)]\tLoss: 0.024636\n",
            "CNN_2  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.040099\n",
            "CNN_2  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.012416\n",
            "CNN_2  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.013328\n",
            "\n",
            "Test set: Average loss: 0.0826, Accuracy: 14640/15000 (97.60%)\n",
            "\n",
            "CNN_2  Epoch: 9 [0/45056 (0%)]\tLoss: 0.041318\n",
            "CNN_2  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.049786\n",
            "CNN_2  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.038400\n",
            "CNN_2  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.040519\n",
            "\n",
            "Test set: Average loss: 0.0535, Accuracy: 14762/15000 (98.41%)\n",
            "\n",
            "new accuracy parameters saved 0.9841333333333333\n",
            "CNN_2  Epoch: 10 [0/45056 (0%)]\tLoss: 0.027564\n",
            "CNN_2  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.046780\n",
            "CNN_2  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.014050\n",
            "CNN_2  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.021038\n",
            "\n",
            "Test set: Average loss: 0.0519, Accuracy: 14760/15000 (98.40%)\n",
            "\n",
            "Training complete in 2m 32s\n",
            "\n",
            "   --  Run :  7\n",
            "CNN_2  Epoch: 1 [0/45056 (0%)]\tLoss: 2.297221\n",
            "CNN_2  Epoch: 1 [12800/45056 (28%)]\tLoss: 1.846164\n",
            "CNN_2  Epoch: 1 [25600/45056 (57%)]\tLoss: 1.158790\n",
            "CNN_2  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.557980\n",
            "\n",
            "Test set: Average loss: 0.4511, Accuracy: 12914/15000 (86.09%)\n",
            "\n",
            "new accuracy parameters saved 0.8609333333333333\n",
            "CNN_2  Epoch: 2 [0/45056 (0%)]\tLoss: 0.440130\n",
            "CNN_2  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.344240\n",
            "CNN_2  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.192780\n",
            "CNN_2  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.245939\n",
            "\n",
            "Test set: Average loss: 0.1724, Accuracy: 14196/15000 (94.64%)\n",
            "\n",
            "new accuracy parameters saved 0.9464\n",
            "CNN_2  Epoch: 3 [0/45056 (0%)]\tLoss: 0.125848\n",
            "CNN_2  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.051222\n",
            "CNN_2  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.164312\n",
            "CNN_2  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.084398\n",
            "\n",
            "Test set: Average loss: 0.0895, Accuracy: 14584/15000 (97.23%)\n",
            "\n",
            "new accuracy parameters saved 0.9722666666666666\n",
            "CNN_2  Epoch: 4 [0/45056 (0%)]\tLoss: 0.069154\n",
            "CNN_2  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.062027\n",
            "CNN_2  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.088553\n",
            "CNN_2  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.058273\n",
            "\n",
            "Test set: Average loss: 0.0746, Accuracy: 14642/15000 (97.61%)\n",
            "\n",
            "new accuracy parameters saved 0.9761333333333333\n",
            "CNN_2  Epoch: 5 [0/45056 (0%)]\tLoss: 0.028371\n",
            "CNN_2  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.170170\n",
            "CNN_2  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.052118\n",
            "CNN_2  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.055100\n",
            "\n",
            "Test set: Average loss: 0.1326, Accuracy: 14353/15000 (95.69%)\n",
            "\n",
            "CNN_2  Epoch: 6 [0/45056 (0%)]\tLoss: 0.103841\n",
            "CNN_2  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.036228\n",
            "CNN_2  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.087385\n",
            "CNN_2  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.049389\n",
            "\n",
            "Test set: Average loss: 0.1175, Accuracy: 14434/15000 (96.23%)\n",
            "\n",
            "CNN_2  Epoch: 7 [0/45056 (0%)]\tLoss: 0.086159\n",
            "CNN_2  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.103275\n",
            "CNN_2  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.032191\n",
            "CNN_2  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.012057\n",
            "\n",
            "Test set: Average loss: 0.0595, Accuracy: 14728/15000 (98.19%)\n",
            "\n",
            "new accuracy parameters saved 0.9818666666666667\n",
            "CNN_2  Epoch: 8 [0/45056 (0%)]\tLoss: 0.048222\n",
            "CNN_2  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.043074\n",
            "CNN_2  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.042465\n",
            "CNN_2  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.039866\n",
            "\n",
            "Test set: Average loss: 0.0780, Accuracy: 14653/15000 (97.69%)\n",
            "\n",
            "CNN_2  Epoch: 9 [0/45056 (0%)]\tLoss: 0.039346\n",
            "CNN_2  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.030683\n",
            "CNN_2  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.021750\n",
            "CNN_2  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.032655\n",
            "\n",
            "Test set: Average loss: 0.0513, Accuracy: 14762/15000 (98.41%)\n",
            "\n",
            "new accuracy parameters saved 0.9841333333333333\n",
            "CNN_2  Epoch: 10 [0/45056 (0%)]\tLoss: 0.030737\n",
            "CNN_2  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.027983\n",
            "CNN_2  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.036160\n",
            "CNN_2  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.024683\n",
            "\n",
            "Test set: Average loss: 0.0689, Accuracy: 14676/15000 (97.84%)\n",
            "\n",
            "Training complete in 2m 32s\n",
            "\n",
            "   --  Run :  8\n",
            "CNN_2  Epoch: 1 [0/45056 (0%)]\tLoss: 2.311178\n",
            "CNN_2  Epoch: 1 [12800/45056 (28%)]\tLoss: 1.940252\n",
            "CNN_2  Epoch: 1 [25600/45056 (57%)]\tLoss: 1.568972\n",
            "CNN_2  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.745911\n",
            "\n",
            "Test set: Average loss: 0.6642, Accuracy: 11370/15000 (75.80%)\n",
            "\n",
            "new accuracy parameters saved 0.758\n",
            "CNN_2  Epoch: 2 [0/45056 (0%)]\tLoss: 0.691463\n",
            "CNN_2  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.238854\n",
            "CNN_2  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.155673\n",
            "CNN_2  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.142824\n",
            "\n",
            "Test set: Average loss: 0.1549, Accuracy: 14267/15000 (95.11%)\n",
            "\n",
            "new accuracy parameters saved 0.9511333333333334\n",
            "CNN_2  Epoch: 3 [0/45056 (0%)]\tLoss: 0.154125\n",
            "CNN_2  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.135274\n",
            "CNN_2  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.087786\n",
            "CNN_2  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.069175\n",
            "\n",
            "Test set: Average loss: 0.1138, Accuracy: 14469/15000 (96.46%)\n",
            "\n",
            "new accuracy parameters saved 0.9646\n",
            "CNN_2  Epoch: 4 [0/45056 (0%)]\tLoss: 0.110197\n",
            "CNN_2  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.182939\n",
            "CNN_2  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.077408\n",
            "CNN_2  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.063421\n",
            "\n",
            "Test set: Average loss: 0.1192, Accuracy: 14454/15000 (96.36%)\n",
            "\n",
            "CNN_2  Epoch: 5 [0/45056 (0%)]\tLoss: 0.122482\n",
            "CNN_2  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.061600\n",
            "CNN_2  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.106954\n",
            "CNN_2  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.080507\n",
            "\n",
            "Test set: Average loss: 0.0704, Accuracy: 14659/15000 (97.73%)\n",
            "\n",
            "new accuracy parameters saved 0.9772666666666666\n",
            "CNN_2  Epoch: 6 [0/45056 (0%)]\tLoss: 0.057914\n",
            "CNN_2  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.028335\n",
            "CNN_2  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.061649\n",
            "CNN_2  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.059706\n",
            "\n",
            "Test set: Average loss: 0.1283, Accuracy: 14364/15000 (95.76%)\n",
            "\n",
            "CNN_2  Epoch: 7 [0/45056 (0%)]\tLoss: 0.155041\n",
            "CNN_2  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.039331\n",
            "CNN_2  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.034752\n",
            "CNN_2  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.012072\n",
            "\n",
            "Test set: Average loss: 0.0628, Accuracy: 14710/15000 (98.07%)\n",
            "\n",
            "new accuracy parameters saved 0.9806666666666667\n",
            "CNN_2  Epoch: 8 [0/45056 (0%)]\tLoss: 0.115223\n",
            "CNN_2  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.011020\n",
            "CNN_2  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.023925\n",
            "CNN_2  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.040202\n",
            "\n",
            "Test set: Average loss: 0.0590, Accuracy: 14732/15000 (98.21%)\n",
            "\n",
            "new accuracy parameters saved 0.9821333333333333\n",
            "CNN_2  Epoch: 9 [0/45056 (0%)]\tLoss: 0.032073\n",
            "CNN_2  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.034085\n",
            "CNN_2  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.024971\n",
            "CNN_2  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.085701\n",
            "\n",
            "Test set: Average loss: 0.0507, Accuracy: 14777/15000 (98.51%)\n",
            "\n",
            "new accuracy parameters saved 0.9851333333333333\n",
            "CNN_2  Epoch: 10 [0/45056 (0%)]\tLoss: 0.016893\n",
            "CNN_2  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.291293\n",
            "CNN_2  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.035455\n",
            "CNN_2  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.038046\n",
            "\n",
            "Test set: Average loss: 0.0536, Accuracy: 14747/15000 (98.31%)\n",
            "\n",
            "Training complete in 2m 32s\n",
            "\n",
            "   --  Run :  9\n",
            "CNN_2  Epoch: 1 [0/45056 (0%)]\tLoss: 2.305279\n",
            "CNN_2  Epoch: 1 [12800/45056 (28%)]\tLoss: 2.188909\n",
            "CNN_2  Epoch: 1 [25600/45056 (57%)]\tLoss: 1.196531\n",
            "CNN_2  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.441195\n",
            "\n",
            "Test set: Average loss: 0.3256, Accuracy: 13684/15000 (91.23%)\n",
            "\n",
            "new accuracy parameters saved 0.9122666666666667\n",
            "CNN_2  Epoch: 2 [0/45056 (0%)]\tLoss: 0.376575\n",
            "CNN_2  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.129274\n",
            "CNN_2  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.208722\n",
            "CNN_2  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.117013\n",
            "\n",
            "Test set: Average loss: 0.1367, Accuracy: 14344/15000 (95.63%)\n",
            "\n",
            "new accuracy parameters saved 0.9562666666666667\n",
            "CNN_2  Epoch: 3 [0/45056 (0%)]\tLoss: 0.149114\n",
            "CNN_2  Epoch: 3 [12800/45056 (28%)]\tLoss: 2.098712\n",
            "CNN_2  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.085434\n",
            "CNN_2  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.092264\n",
            "\n",
            "Test set: Average loss: 0.1034, Accuracy: 14538/15000 (96.92%)\n",
            "\n",
            "new accuracy parameters saved 0.9692\n",
            "CNN_2  Epoch: 4 [0/45056 (0%)]\tLoss: 0.065260\n",
            "CNN_2  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.056455\n",
            "CNN_2  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.096079\n",
            "CNN_2  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.079342\n",
            "\n",
            "Test set: Average loss: 0.0883, Accuracy: 14611/15000 (97.41%)\n",
            "\n",
            "new accuracy parameters saved 0.9740666666666666\n",
            "CNN_2  Epoch: 5 [0/45056 (0%)]\tLoss: 0.160519\n",
            "CNN_2  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.057468\n",
            "CNN_2  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.067036\n",
            "CNN_2  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.032932\n",
            "\n",
            "Test set: Average loss: 0.0713, Accuracy: 14674/15000 (97.83%)\n",
            "\n",
            "new accuracy parameters saved 0.9782666666666666\n",
            "CNN_2  Epoch: 6 [0/45056 (0%)]\tLoss: 0.065577\n",
            "CNN_2  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.059850\n",
            "CNN_2  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.084104\n",
            "CNN_2  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.077338\n",
            "\n",
            "Test set: Average loss: 0.0590, Accuracy: 14730/15000 (98.20%)\n",
            "\n",
            "new accuracy parameters saved 0.982\n",
            "CNN_2  Epoch: 7 [0/45056 (0%)]\tLoss: 0.021800\n",
            "CNN_2  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.040344\n",
            "CNN_2  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.078566\n",
            "CNN_2  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.045069\n",
            "\n",
            "Test set: Average loss: 0.0579, Accuracy: 14721/15000 (98.14%)\n",
            "\n",
            "CNN_2  Epoch: 8 [0/45056 (0%)]\tLoss: 0.043193\n",
            "CNN_2  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.041054\n",
            "CNN_2  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.223641\n",
            "CNN_2  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.020369\n",
            "\n",
            "Test set: Average loss: 0.0587, Accuracy: 14713/15000 (98.09%)\n",
            "\n",
            "CNN_2  Epoch: 9 [0/45056 (0%)]\tLoss: 0.035539\n",
            "CNN_2  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.036991\n",
            "CNN_2  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.041882\n",
            "CNN_2  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.032418\n",
            "\n",
            "Test set: Average loss: 0.0862, Accuracy: 14585/15000 (97.23%)\n",
            "\n",
            "CNN_2  Epoch: 10 [0/45056 (0%)]\tLoss: 0.025824\n",
            "CNN_2  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.028813\n",
            "CNN_2  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.037316\n",
            "CNN_2  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.020847\n",
            "\n",
            "Test set: Average loss: 0.0555, Accuracy: 14747/15000 (98.31%)\n",
            "\n",
            "new accuracy parameters saved 0.9831333333333333\n",
            "Training complete in 2m 32s\n",
            "\n",
            "   --  Run :  10\n",
            "CNN_2  Epoch: 1 [0/45056 (0%)]\tLoss: 2.296574\n",
            "CNN_2  Epoch: 1 [12800/45056 (28%)]\tLoss: 2.196820\n",
            "CNN_2  Epoch: 1 [25600/45056 (57%)]\tLoss: 1.123355\n",
            "CNN_2  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.484469\n",
            "\n",
            "Test set: Average loss: 0.3601, Accuracy: 13472/15000 (89.81%)\n",
            "\n",
            "new accuracy parameters saved 0.8981333333333333\n",
            "CNN_2  Epoch: 2 [0/45056 (0%)]\tLoss: 0.308220\n",
            "CNN_2  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.191081\n",
            "CNN_2  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.203979\n",
            "CNN_2  Epoch: 2 [38400/45056 (85%)]\tLoss: 1.468933\n",
            "\n",
            "Test set: Average loss: 0.1512, Accuracy: 14314/15000 (95.43%)\n",
            "\n",
            "new accuracy parameters saved 0.9542666666666667\n",
            "CNN_2  Epoch: 3 [0/45056 (0%)]\tLoss: 0.166206\n",
            "CNN_2  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.094308\n",
            "CNN_2  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.093250\n",
            "CNN_2  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.188822\n",
            "\n",
            "Test set: Average loss: 0.1057, Accuracy: 14491/15000 (96.61%)\n",
            "\n",
            "new accuracy parameters saved 0.9660666666666666\n",
            "CNN_2  Epoch: 4 [0/45056 (0%)]\tLoss: 0.046029\n",
            "CNN_2  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.048172\n",
            "CNN_2  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.063501\n",
            "CNN_2  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.130487\n",
            "\n",
            "Test set: Average loss: 0.1760, Accuracy: 14207/15000 (94.71%)\n",
            "\n",
            "CNN_2  Epoch: 5 [0/45056 (0%)]\tLoss: 0.261316\n",
            "CNN_2  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.120741\n",
            "CNN_2  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.123942\n",
            "CNN_2  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.071017\n",
            "\n",
            "Test set: Average loss: 0.0934, Accuracy: 14570/15000 (97.13%)\n",
            "\n",
            "new accuracy parameters saved 0.9713333333333334\n",
            "CNN_2  Epoch: 6 [0/45056 (0%)]\tLoss: 0.082622\n",
            "CNN_2  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.081240\n",
            "CNN_2  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.025723\n",
            "CNN_2  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.058729\n",
            "\n",
            "Test set: Average loss: 0.1908, Accuracy: 14060/15000 (93.73%)\n",
            "\n",
            "CNN_2  Epoch: 7 [0/45056 (0%)]\tLoss: 0.164644\n",
            "CNN_2  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.067853\n",
            "CNN_2  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.087890\n",
            "CNN_2  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.075687\n",
            "\n",
            "Test set: Average loss: 0.0541, Accuracy: 14750/15000 (98.33%)\n",
            "\n",
            "new accuracy parameters saved 0.9833333333333333\n",
            "CNN_2  Epoch: 8 [0/45056 (0%)]\tLoss: 0.013681\n",
            "CNN_2  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.020951\n",
            "CNN_2  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.031716\n",
            "CNN_2  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.030371\n",
            "\n",
            "Test set: Average loss: 0.0553, Accuracy: 14738/15000 (98.25%)\n",
            "\n",
            "CNN_2  Epoch: 9 [0/45056 (0%)]\tLoss: 0.042788\n",
            "CNN_2  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.129484\n",
            "CNN_2  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.015760\n",
            "CNN_2  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.064085\n",
            "\n",
            "Test set: Average loss: 0.0491, Accuracy: 14778/15000 (98.52%)\n",
            "\n",
            "new accuracy parameters saved 0.9852\n",
            "CNN_2  Epoch: 10 [0/45056 (0%)]\tLoss: 0.011707\n",
            "CNN_2  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.028339\n",
            "CNN_2  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.006446\n",
            "CNN_2  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.065636\n",
            "\n",
            "Test set: Average loss: 0.0541, Accuracy: 14749/15000 (98.33%)\n",
            "\n",
            "Training complete in 2m 32s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3NvIdf7DKM2F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Running CNN3 "
      ]
    },
    {
      "metadata": {
        "id": "dK7Dj6haKSbT",
        "colab_type": "code",
        "outputId": "3fc18825-f26b-4328-aa6b-620dec9686e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 13628
        }
      },
      "cell_type": "code",
      "source": [
        "MultipleRun(runs,epochs,CNN_3())"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   --  Run :  1\n",
            "CNN_3  Epoch: 1 [0/45056 (0%)]\tLoss: 2.300875\n",
            "CNN_3  Epoch: 1 [12800/45056 (28%)]\tLoss: 2.082704\n",
            "CNN_3  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.705327\n",
            "CNN_3  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.341417\n",
            "\n",
            "Test set: Average loss: 0.3642, Accuracy: 13107/15000 (87.38%)\n",
            "\n",
            "new accuracy parameters saved 0.8738\n",
            "CNN_3  Epoch: 2 [0/45056 (0%)]\tLoss: 0.350184\n",
            "CNN_3  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.143621\n",
            "CNN_3  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.111957\n",
            "CNN_3  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.089449\n",
            "\n",
            "Test set: Average loss: 0.1282, Accuracy: 14386/15000 (95.91%)\n",
            "\n",
            "new accuracy parameters saved 0.9590666666666666\n",
            "CNN_3  Epoch: 3 [0/45056 (0%)]\tLoss: 0.203546\n",
            "CNN_3  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.043133\n",
            "CNN_3  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.089760\n",
            "CNN_3  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.049538\n",
            "\n",
            "Test set: Average loss: 0.1139, Accuracy: 14452/15000 (96.35%)\n",
            "\n",
            "new accuracy parameters saved 0.9634666666666667\n",
            "CNN_3  Epoch: 4 [0/45056 (0%)]\tLoss: 0.075681\n",
            "CNN_3  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.063502\n",
            "CNN_3  Epoch: 4 [25600/45056 (57%)]\tLoss: 1.370110\n",
            "CNN_3  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.091065\n",
            "\n",
            "Test set: Average loss: 0.0934, Accuracy: 14567/15000 (97.11%)\n",
            "\n",
            "new accuracy parameters saved 0.9711333333333333\n",
            "CNN_3  Epoch: 5 [0/45056 (0%)]\tLoss: 0.045956\n",
            "CNN_3  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.084623\n",
            "CNN_3  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.039115\n",
            "CNN_3  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.024600\n",
            "\n",
            "Test set: Average loss: 0.0859, Accuracy: 14576/15000 (97.17%)\n",
            "\n",
            "new accuracy parameters saved 0.9717333333333333\n",
            "CNN_3  Epoch: 6 [0/45056 (0%)]\tLoss: 0.061183\n",
            "CNN_3  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.022861\n",
            "CNN_3  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.029907\n",
            "CNN_3  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.042092\n",
            "\n",
            "Test set: Average loss: 0.0477, Accuracy: 14767/15000 (98.45%)\n",
            "\n",
            "new accuracy parameters saved 0.9844666666666667\n",
            "CNN_3  Epoch: 7 [0/45056 (0%)]\tLoss: 0.022007\n",
            "CNN_3  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.032597\n",
            "CNN_3  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.082185\n",
            "CNN_3  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.034748\n",
            "\n",
            "Test set: Average loss: 0.1094, Accuracy: 14475/15000 (96.50%)\n",
            "\n",
            "CNN_3  Epoch: 8 [0/45056 (0%)]\tLoss: 0.057630\n",
            "CNN_3  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.033783\n",
            "CNN_3  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.050963\n",
            "CNN_3  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.018546\n",
            "\n",
            "Test set: Average loss: 0.0400, Accuracy: 14808/15000 (98.72%)\n",
            "\n",
            "new accuracy parameters saved 0.9872\n",
            "CNN_3  Epoch: 9 [0/45056 (0%)]\tLoss: 0.012039\n",
            "CNN_3  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.108221\n",
            "CNN_3  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.012372\n",
            "CNN_3  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.014295\n",
            "\n",
            "Test set: Average loss: 0.1108, Accuracy: 14508/15000 (96.72%)\n",
            "\n",
            "CNN_3  Epoch: 10 [0/45056 (0%)]\tLoss: 0.067378\n",
            "CNN_3  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.006719\n",
            "CNN_3  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.003626\n",
            "CNN_3  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.030033\n",
            "\n",
            "Test set: Average loss: 0.0436, Accuracy: 14798/15000 (98.65%)\n",
            "\n",
            "Training complete in 2m 13s\n",
            "\n",
            "   --  Run :  2\n",
            "CNN_3  Epoch: 1 [0/45056 (0%)]\tLoss: 2.302286\n",
            "CNN_3  Epoch: 1 [12800/45056 (28%)]\tLoss: 1.961588\n",
            "CNN_3  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.644451\n",
            "CNN_3  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.354013\n",
            "\n",
            "Test set: Average loss: 0.1837, Accuracy: 14155/15000 (94.37%)\n",
            "\n",
            "new accuracy parameters saved 0.9436666666666667\n",
            "CNN_3  Epoch: 2 [0/45056 (0%)]\tLoss: 0.145578\n",
            "CNN_3  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.114200\n",
            "CNN_3  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.158221\n",
            "CNN_3  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.148617\n",
            "\n",
            "Test set: Average loss: 0.1483, Accuracy: 14271/15000 (95.14%)\n",
            "\n",
            "new accuracy parameters saved 0.9514\n",
            "CNN_3  Epoch: 3 [0/45056 (0%)]\tLoss: 0.088746\n",
            "CNN_3  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.075616\n",
            "CNN_3  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.077730\n",
            "CNN_3  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.070237\n",
            "\n",
            "Test set: Average loss: 0.1061, Accuracy: 14469/15000 (96.46%)\n",
            "\n",
            "new accuracy parameters saved 0.9646\n",
            "CNN_3  Epoch: 4 [0/45056 (0%)]\tLoss: 0.043150\n",
            "CNN_3  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.566251\n",
            "CNN_3  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.054544\n",
            "CNN_3  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.059101\n",
            "\n",
            "Test set: Average loss: 0.0604, Accuracy: 14714/15000 (98.09%)\n",
            "\n",
            "new accuracy parameters saved 0.9809333333333333\n",
            "CNN_3  Epoch: 5 [0/45056 (0%)]\tLoss: 0.006149\n",
            "CNN_3  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.018915\n",
            "CNN_3  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.044329\n",
            "CNN_3  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.079222\n",
            "\n",
            "Test set: Average loss: 0.0542, Accuracy: 14732/15000 (98.21%)\n",
            "\n",
            "new accuracy parameters saved 0.9821333333333333\n",
            "CNN_3  Epoch: 6 [0/45056 (0%)]\tLoss: 0.024727\n",
            "CNN_3  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.045962\n",
            "CNN_3  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.024128\n",
            "CNN_3  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.046730\n",
            "\n",
            "Test set: Average loss: 0.0582, Accuracy: 14705/15000 (98.03%)\n",
            "\n",
            "CNN_3  Epoch: 7 [0/45056 (0%)]\tLoss: 0.039992\n",
            "CNN_3  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.012503\n",
            "CNN_3  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.096051\n",
            "CNN_3  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.071207\n",
            "\n",
            "Test set: Average loss: 0.0669, Accuracy: 14693/15000 (97.95%)\n",
            "\n",
            "CNN_3  Epoch: 8 [0/45056 (0%)]\tLoss: 0.030611\n",
            "CNN_3  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.005199\n",
            "CNN_3  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.009291\n",
            "CNN_3  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.031458\n",
            "\n",
            "Test set: Average loss: 0.0763, Accuracy: 14649/15000 (97.66%)\n",
            "\n",
            "CNN_3  Epoch: 9 [0/45056 (0%)]\tLoss: 0.045657\n",
            "CNN_3  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.005720\n",
            "CNN_3  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.025803\n",
            "CNN_3  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.033644\n",
            "\n",
            "Test set: Average loss: 0.1162, Accuracy: 14406/15000 (96.04%)\n",
            "\n",
            "CNN_3  Epoch: 10 [0/45056 (0%)]\tLoss: 0.100139\n",
            "CNN_3  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.025053\n",
            "CNN_3  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.003464\n",
            "CNN_3  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.014635\n",
            "\n",
            "Test set: Average loss: 0.0690, Accuracy: 14681/15000 (97.87%)\n",
            "\n",
            "Training complete in 2m 13s\n",
            "\n",
            "   --  Run :  3\n",
            "CNN_3  Epoch: 1 [0/45056 (0%)]\tLoss: 2.300284\n",
            "CNN_3  Epoch: 1 [12800/45056 (28%)]\tLoss: 1.860895\n",
            "CNN_3  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.589448\n",
            "CNN_3  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.328179\n",
            "\n",
            "Test set: Average loss: 0.1849, Accuracy: 14152/15000 (94.35%)\n",
            "\n",
            "new accuracy parameters saved 0.9434666666666667\n",
            "CNN_3  Epoch: 2 [0/45056 (0%)]\tLoss: 0.177029\n",
            "CNN_3  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.174974\n",
            "CNN_3  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.052584\n",
            "CNN_3  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.096457\n",
            "\n",
            "Test set: Average loss: 0.1260, Accuracy: 14418/15000 (96.12%)\n",
            "\n",
            "new accuracy parameters saved 0.9612\n",
            "CNN_3  Epoch: 3 [0/45056 (0%)]\tLoss: 0.094109\n",
            "CNN_3  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.017563\n",
            "CNN_3  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.079606\n",
            "CNN_3  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.022854\n",
            "\n",
            "Test set: Average loss: 0.0846, Accuracy: 14626/15000 (97.51%)\n",
            "\n",
            "new accuracy parameters saved 0.9750666666666666\n",
            "CNN_3  Epoch: 4 [0/45056 (0%)]\tLoss: 0.063502\n",
            "CNN_3  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.103472\n",
            "CNN_3  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.016159\n",
            "CNN_3  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.046284\n",
            "\n",
            "Test set: Average loss: 0.0694, Accuracy: 14666/15000 (97.77%)\n",
            "\n",
            "new accuracy parameters saved 0.9777333333333333\n",
            "CNN_3  Epoch: 5 [0/45056 (0%)]\tLoss: 0.019706\n",
            "CNN_3  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.029904\n",
            "CNN_3  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.104218\n",
            "CNN_3  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.030449\n",
            "\n",
            "Test set: Average loss: 0.0500, Accuracy: 14756/15000 (98.37%)\n",
            "\n",
            "new accuracy parameters saved 0.9837333333333333\n",
            "CNN_3  Epoch: 6 [0/45056 (0%)]\tLoss: 0.024858\n",
            "CNN_3  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.028770\n",
            "CNN_3  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.009146\n",
            "CNN_3  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.016839\n",
            "\n",
            "Test set: Average loss: 0.0533, Accuracy: 14763/15000 (98.42%)\n",
            "\n",
            "new accuracy parameters saved 0.9842\n",
            "CNN_3  Epoch: 7 [0/45056 (0%)]\tLoss: 0.039625\n",
            "CNN_3  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.021371\n",
            "CNN_3  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.083800\n",
            "CNN_3  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.012043\n",
            "\n",
            "Test set: Average loss: 0.0443, Accuracy: 14802/15000 (98.68%)\n",
            "\n",
            "new accuracy parameters saved 0.9868\n",
            "CNN_3  Epoch: 8 [0/45056 (0%)]\tLoss: 0.015931\n",
            "CNN_3  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.013338\n",
            "CNN_3  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.014728\n",
            "CNN_3  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.042631\n",
            "\n",
            "Test set: Average loss: 0.0572, Accuracy: 14716/15000 (98.11%)\n",
            "\n",
            "CNN_3  Epoch: 9 [0/45056 (0%)]\tLoss: 0.053344\n",
            "CNN_3  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.020475\n",
            "CNN_3  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.062402\n",
            "CNN_3  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.064430\n",
            "\n",
            "Test set: Average loss: 0.0444, Accuracy: 14793/15000 (98.62%)\n",
            "\n",
            "CNN_3  Epoch: 10 [0/45056 (0%)]\tLoss: 0.022254\n",
            "CNN_3  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.012876\n",
            "CNN_3  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.031107\n",
            "CNN_3  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.026314\n",
            "\n",
            "Test set: Average loss: 0.0408, Accuracy: 14809/15000 (98.73%)\n",
            "\n",
            "new accuracy parameters saved 0.9872666666666666\n",
            "Training complete in 2m 13s\n",
            "\n",
            "   --  Run :  4\n",
            "CNN_3  Epoch: 1 [0/45056 (0%)]\tLoss: 2.302778\n",
            "CNN_3  Epoch: 1 [12800/45056 (28%)]\tLoss: 2.199050\n",
            "CNN_3  Epoch: 1 [25600/45056 (57%)]\tLoss: 1.284516\n",
            "CNN_3  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.271008\n",
            "\n",
            "Test set: Average loss: 0.3427, Accuracy: 13166/15000 (87.77%)\n",
            "\n",
            "new accuracy parameters saved 0.8777333333333334\n",
            "CNN_3  Epoch: 2 [0/45056 (0%)]\tLoss: 0.315568\n",
            "CNN_3  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.065390\n",
            "CNN_3  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.118828\n",
            "CNN_3  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.075537\n",
            "\n",
            "Test set: Average loss: 0.0930, Accuracy: 14549/15000 (96.99%)\n",
            "\n",
            "new accuracy parameters saved 0.9699333333333333\n",
            "CNN_3  Epoch: 3 [0/45056 (0%)]\tLoss: 0.119723\n",
            "CNN_3  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.095627\n",
            "CNN_3  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.110481\n",
            "CNN_3  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.098151\n",
            "\n",
            "Test set: Average loss: 0.0950, Accuracy: 14546/15000 (96.97%)\n",
            "\n",
            "CNN_3  Epoch: 4 [0/45056 (0%)]\tLoss: 0.048441\n",
            "CNN_3  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.040434\n",
            "CNN_3  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.085117\n",
            "CNN_3  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.029090\n",
            "\n",
            "Test set: Average loss: 0.0877, Accuracy: 14586/15000 (97.24%)\n",
            "\n",
            "new accuracy parameters saved 0.9724\n",
            "CNN_3  Epoch: 5 [0/45056 (0%)]\tLoss: 0.114700\n",
            "CNN_3  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.042868\n",
            "CNN_3  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.047869\n",
            "CNN_3  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.025212\n",
            "\n",
            "Test set: Average loss: 0.0749, Accuracy: 14642/15000 (97.61%)\n",
            "\n",
            "new accuracy parameters saved 0.9761333333333333\n",
            "CNN_3  Epoch: 6 [0/45056 (0%)]\tLoss: 0.049192\n",
            "CNN_3  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.018841\n",
            "CNN_3  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.175275\n",
            "CNN_3  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.106133\n",
            "\n",
            "Test set: Average loss: 0.0809, Accuracy: 14591/15000 (97.27%)\n",
            "\n",
            "CNN_3  Epoch: 7 [0/45056 (0%)]\tLoss: 0.053949\n",
            "CNN_3  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.041548\n",
            "CNN_3  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.015595\n",
            "CNN_3  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.033209\n",
            "\n",
            "Test set: Average loss: 0.0593, Accuracy: 14736/15000 (98.24%)\n",
            "\n",
            "new accuracy parameters saved 0.9824\n",
            "CNN_3  Epoch: 8 [0/45056 (0%)]\tLoss: 0.066165\n",
            "CNN_3  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.018598\n",
            "CNN_3  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.044525\n",
            "CNN_3  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.026408\n",
            "\n",
            "Test set: Average loss: 0.0683, Accuracy: 14683/15000 (97.89%)\n",
            "\n",
            "CNN_3  Epoch: 9 [0/45056 (0%)]\tLoss: 0.056364\n",
            "CNN_3  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.016461\n",
            "CNN_3  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.063710\n",
            "CNN_3  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.017703\n",
            "\n",
            "Test set: Average loss: 0.0478, Accuracy: 14776/15000 (98.51%)\n",
            "\n",
            "new accuracy parameters saved 0.9850666666666666\n",
            "CNN_3  Epoch: 10 [0/45056 (0%)]\tLoss: 0.048425\n",
            "CNN_3  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.047545\n",
            "CNN_3  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.035774\n",
            "CNN_3  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.009598\n",
            "\n",
            "Test set: Average loss: 0.0396, Accuracy: 14819/15000 (98.79%)\n",
            "\n",
            "new accuracy parameters saved 0.9879333333333333\n",
            "Training complete in 2m 13s\n",
            "\n",
            "   --  Run :  5\n",
            "CNN_3  Epoch: 1 [0/45056 (0%)]\tLoss: 2.302517\n",
            "CNN_3  Epoch: 1 [12800/45056 (28%)]\tLoss: 2.056029\n",
            "CNN_3  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.682727\n",
            "CNN_3  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.168135\n",
            "\n",
            "Test set: Average loss: 0.2201, Accuracy: 13960/15000 (93.07%)\n",
            "\n",
            "new accuracy parameters saved 0.9306666666666666\n",
            "CNN_3  Epoch: 2 [0/45056 (0%)]\tLoss: 0.280801\n",
            "CNN_3  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.148763\n",
            "CNN_3  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.079025\n",
            "CNN_3  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.120730\n",
            "\n",
            "Test set: Average loss: 0.1145, Accuracy: 14450/15000 (96.33%)\n",
            "\n",
            "new accuracy parameters saved 0.9633333333333334\n",
            "CNN_3  Epoch: 3 [0/45056 (0%)]\tLoss: 0.055569\n",
            "CNN_3  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.073995\n",
            "CNN_3  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.043751\n",
            "CNN_3  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.065180\n",
            "\n",
            "Test set: Average loss: 0.1192, Accuracy: 14431/15000 (96.21%)\n",
            "\n",
            "CNN_3  Epoch: 4 [0/45056 (0%)]\tLoss: 0.112134\n",
            "CNN_3  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.170469\n",
            "CNN_3  Epoch: 4 [25600/45056 (57%)]\tLoss: 5.776205\n",
            "CNN_3  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.086227\n",
            "\n",
            "Test set: Average loss: 0.0726, Accuracy: 14665/15000 (97.77%)\n",
            "\n",
            "new accuracy parameters saved 0.9776666666666667\n",
            "CNN_3  Epoch: 5 [0/45056 (0%)]\tLoss: 0.097390\n",
            "CNN_3  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.035910\n",
            "CNN_3  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.102185\n",
            "CNN_3  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.051242\n",
            "\n",
            "Test set: Average loss: 0.0565, Accuracy: 14755/15000 (98.37%)\n",
            "\n",
            "new accuracy parameters saved 0.9836666666666667\n",
            "CNN_3  Epoch: 6 [0/45056 (0%)]\tLoss: 0.040787\n",
            "CNN_3  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.007010\n",
            "CNN_3  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.039156\n",
            "CNN_3  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.009218\n",
            "\n",
            "Test set: Average loss: 0.0582, Accuracy: 14712/15000 (98.08%)\n",
            "\n",
            "CNN_3  Epoch: 7 [0/45056 (0%)]\tLoss: 0.061968\n",
            "CNN_3  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.015002\n",
            "CNN_3  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.004967\n",
            "CNN_3  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.032151\n",
            "\n",
            "Test set: Average loss: 0.1013, Accuracy: 14534/15000 (96.89%)\n",
            "\n",
            "CNN_3  Epoch: 8 [0/45056 (0%)]\tLoss: 0.076613\n",
            "CNN_3  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.041303\n",
            "CNN_3  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.046851\n",
            "CNN_3  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.019810\n",
            "\n",
            "Test set: Average loss: 0.0589, Accuracy: 14719/15000 (98.13%)\n",
            "\n",
            "CNN_3  Epoch: 9 [0/45056 (0%)]\tLoss: 0.057195\n",
            "CNN_3  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.031573\n",
            "CNN_3  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.036869\n",
            "CNN_3  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.039031\n",
            "\n",
            "Test set: Average loss: 0.0625, Accuracy: 14707/15000 (98.05%)\n",
            "\n",
            "CNN_3  Epoch: 10 [0/45056 (0%)]\tLoss: 0.094736\n",
            "CNN_3  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.019448\n",
            "CNN_3  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.056738\n",
            "CNN_3  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.024249\n",
            "\n",
            "Test set: Average loss: 0.0615, Accuracy: 14728/15000 (98.19%)\n",
            "\n",
            "Training complete in 2m 13s\n",
            "\n",
            "   --  Run :  6\n",
            "CNN_3  Epoch: 1 [0/45056 (0%)]\tLoss: 2.302320\n",
            "CNN_3  Epoch: 1 [12800/45056 (28%)]\tLoss: 2.076309\n",
            "CNN_3  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.577617\n",
            "CNN_3  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.197306\n",
            "\n",
            "Test set: Average loss: 0.2315, Accuracy: 13941/15000 (92.94%)\n",
            "\n",
            "new accuracy parameters saved 0.9294\n",
            "CNN_3  Epoch: 2 [0/45056 (0%)]\tLoss: 0.283511\n",
            "CNN_3  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.132779\n",
            "CNN_3  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.135200\n",
            "CNN_3  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.145489\n",
            "\n",
            "Test set: Average loss: 0.0974, Accuracy: 14534/15000 (96.89%)\n",
            "\n",
            "new accuracy parameters saved 0.9689333333333333\n",
            "CNN_3  Epoch: 3 [0/45056 (0%)]\tLoss: 0.093733\n",
            "CNN_3  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.040772\n",
            "CNN_3  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.089785\n",
            "CNN_3  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.052709\n",
            "\n",
            "Test set: Average loss: 0.0768, Accuracy: 14626/15000 (97.51%)\n",
            "\n",
            "new accuracy parameters saved 0.9750666666666666\n",
            "CNN_3  Epoch: 4 [0/45056 (0%)]\tLoss: 0.041719\n",
            "CNN_3  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.042759\n",
            "CNN_3  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.122990\n",
            "CNN_3  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.036030\n",
            "\n",
            "Test set: Average loss: 0.0718, Accuracy: 14657/15000 (97.71%)\n",
            "\n",
            "new accuracy parameters saved 0.9771333333333333\n",
            "CNN_3  Epoch: 5 [0/45056 (0%)]\tLoss: 0.015501\n",
            "CNN_3  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.045659\n",
            "CNN_3  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.017544\n",
            "CNN_3  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.046983\n",
            "\n",
            "Test set: Average loss: 0.0688, Accuracy: 14665/15000 (97.77%)\n",
            "\n",
            "new accuracy parameters saved 0.9776666666666667\n",
            "CNN_3  Epoch: 6 [0/45056 (0%)]\tLoss: 0.051128\n",
            "CNN_3  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.019435\n",
            "CNN_3  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.033868\n",
            "CNN_3  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.050325\n",
            "\n",
            "Test set: Average loss: 0.0445, Accuracy: 14782/15000 (98.55%)\n",
            "\n",
            "new accuracy parameters saved 0.9854666666666667\n",
            "CNN_3  Epoch: 7 [0/45056 (0%)]\tLoss: 0.026694\n",
            "CNN_3  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.024383\n",
            "CNN_3  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.019807\n",
            "CNN_3  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.012793\n",
            "\n",
            "Test set: Average loss: 0.0437, Accuracy: 14790/15000 (98.60%)\n",
            "\n",
            "new accuracy parameters saved 0.986\n",
            "CNN_3  Epoch: 8 [0/45056 (0%)]\tLoss: 0.007729\n",
            "CNN_3  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.018020\n",
            "CNN_3  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.012625\n",
            "CNN_3  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.103263\n",
            "\n",
            "Test set: Average loss: 0.1850, Accuracy: 14099/15000 (93.99%)\n",
            "\n",
            "CNN_3  Epoch: 9 [0/45056 (0%)]\tLoss: 0.158730\n",
            "CNN_3  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.035044\n",
            "CNN_3  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.011949\n",
            "CNN_3  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.016174\n",
            "\n",
            "Test set: Average loss: 0.0463, Accuracy: 14779/15000 (98.53%)\n",
            "\n",
            "CNN_3  Epoch: 10 [0/45056 (0%)]\tLoss: 0.024872\n",
            "CNN_3  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.009373\n",
            "CNN_3  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.022643\n",
            "CNN_3  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.031666\n",
            "\n",
            "Test set: Average loss: 0.0538, Accuracy: 14752/15000 (98.35%)\n",
            "\n",
            "Training complete in 2m 13s\n",
            "\n",
            "   --  Run :  7\n",
            "CNN_3  Epoch: 1 [0/45056 (0%)]\tLoss: 2.307729\n",
            "CNN_3  Epoch: 1 [12800/45056 (28%)]\tLoss: 2.012053\n",
            "CNN_3  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.612498\n",
            "CNN_3  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.167410\n",
            "\n",
            "Test set: Average loss: 0.1775, Accuracy: 14173/15000 (94.49%)\n",
            "\n",
            "new accuracy parameters saved 0.9448666666666666\n",
            "CNN_3  Epoch: 2 [0/45056 (0%)]\tLoss: 0.279214\n",
            "CNN_3  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.188299\n",
            "CNN_3  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.066263\n",
            "CNN_3  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.240008\n",
            "\n",
            "Test set: Average loss: 0.1140, Accuracy: 14466/15000 (96.44%)\n",
            "\n",
            "new accuracy parameters saved 0.9644\n",
            "CNN_3  Epoch: 3 [0/45056 (0%)]\tLoss: 0.066620\n",
            "CNN_3  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.063431\n",
            "CNN_3  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.032693\n",
            "CNN_3  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.065349\n",
            "\n",
            "Test set: Average loss: 0.0945, Accuracy: 14563/15000 (97.09%)\n",
            "\n",
            "new accuracy parameters saved 0.9708666666666667\n",
            "CNN_3  Epoch: 4 [0/45056 (0%)]\tLoss: 0.128060\n",
            "CNN_3  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.084176\n",
            "CNN_3  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.050767\n",
            "CNN_3  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.030439\n",
            "\n",
            "Test set: Average loss: 0.0694, Accuracy: 14675/15000 (97.83%)\n",
            "\n",
            "new accuracy parameters saved 0.9783333333333334\n",
            "CNN_3  Epoch: 5 [0/45056 (0%)]\tLoss: 0.029459\n",
            "CNN_3  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.092944\n",
            "CNN_3  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.040599\n",
            "CNN_3  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.048991\n",
            "\n",
            "Test set: Average loss: 0.0574, Accuracy: 14727/15000 (98.18%)\n",
            "\n",
            "new accuracy parameters saved 0.9818\n",
            "CNN_3  Epoch: 6 [0/45056 (0%)]\tLoss: 0.026700\n",
            "CNN_3  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.032977\n",
            "CNN_3  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.044393\n",
            "CNN_3  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.015939\n",
            "\n",
            "Test set: Average loss: 0.0452, Accuracy: 14791/15000 (98.61%)\n",
            "\n",
            "new accuracy parameters saved 0.9860666666666666\n",
            "CNN_3  Epoch: 7 [0/45056 (0%)]\tLoss: 0.026217\n",
            "CNN_3  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.014717\n",
            "CNN_3  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.010307\n",
            "CNN_3  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.011911\n",
            "\n",
            "Test set: Average loss: 0.0468, Accuracy: 14774/15000 (98.49%)\n",
            "\n",
            "CNN_3  Epoch: 8 [0/45056 (0%)]\tLoss: 0.019534\n",
            "CNN_3  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.025746\n",
            "CNN_3  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.014271\n",
            "CNN_3  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.041966\n",
            "\n",
            "Test set: Average loss: 0.0502, Accuracy: 14755/15000 (98.37%)\n",
            "\n",
            "CNN_3  Epoch: 9 [0/45056 (0%)]\tLoss: 0.062001\n",
            "CNN_3  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.080819\n",
            "CNN_3  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.005942\n",
            "CNN_3  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.012965\n",
            "\n",
            "Test set: Average loss: 0.0386, Accuracy: 14822/15000 (98.81%)\n",
            "\n",
            "new accuracy parameters saved 0.9881333333333333\n",
            "CNN_3  Epoch: 10 [0/45056 (0%)]\tLoss: 0.010060\n",
            "CNN_3  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.013763\n",
            "CNN_3  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.004848\n",
            "CNN_3  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.055010\n",
            "\n",
            "Test set: Average loss: 0.0391, Accuracy: 14819/15000 (98.79%)\n",
            "\n",
            "Training complete in 2m 13s\n",
            "\n",
            "   --  Run :  8\n",
            "CNN_3  Epoch: 1 [0/45056 (0%)]\tLoss: 2.304480\n",
            "CNN_3  Epoch: 1 [12800/45056 (28%)]\tLoss: 1.985083\n",
            "CNN_3  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.921794\n",
            "CNN_3  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.324544\n",
            "\n",
            "Test set: Average loss: 0.1993, Accuracy: 14091/15000 (93.94%)\n",
            "\n",
            "new accuracy parameters saved 0.9394\n",
            "CNN_3  Epoch: 2 [0/45056 (0%)]\tLoss: 0.254320\n",
            "CNN_3  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.110973\n",
            "CNN_3  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.362112\n",
            "CNN_3  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.098417\n",
            "\n",
            "Test set: Average loss: 0.0910, Accuracy: 14574/15000 (97.16%)\n",
            "\n",
            "new accuracy parameters saved 0.9716\n",
            "CNN_3  Epoch: 3 [0/45056 (0%)]\tLoss: 0.057621\n",
            "CNN_3  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.025438\n",
            "CNN_3  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.047924\n",
            "CNN_3  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.087007\n",
            "\n",
            "Test set: Average loss: 0.0942, Accuracy: 14541/15000 (96.94%)\n",
            "\n",
            "CNN_3  Epoch: 4 [0/45056 (0%)]\tLoss: 0.140876\n",
            "CNN_3  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.121248\n",
            "CNN_3  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.062143\n",
            "CNN_3  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.114166\n",
            "\n",
            "Test set: Average loss: 0.0572, Accuracy: 14732/15000 (98.21%)\n",
            "\n",
            "new accuracy parameters saved 0.9821333333333333\n",
            "CNN_3  Epoch: 5 [0/45056 (0%)]\tLoss: 0.030822\n",
            "CNN_3  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.059977\n",
            "CNN_3  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.023943\n",
            "CNN_3  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.028080\n",
            "\n",
            "Test set: Average loss: 0.0793, Accuracy: 14633/15000 (97.55%)\n",
            "\n",
            "CNN_3  Epoch: 6 [0/45056 (0%)]\tLoss: 0.043459\n",
            "CNN_3  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.021848\n",
            "CNN_3  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.016365\n",
            "CNN_3  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.029709\n",
            "\n",
            "Test set: Average loss: 0.0604, Accuracy: 14716/15000 (98.11%)\n",
            "\n",
            "CNN_3  Epoch: 7 [0/45056 (0%)]\tLoss: 0.060392\n",
            "CNN_3  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.035671\n",
            "CNN_3  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.008805\n",
            "CNN_3  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.060900\n",
            "\n",
            "Test set: Average loss: 0.0450, Accuracy: 14801/15000 (98.67%)\n",
            "\n",
            "new accuracy parameters saved 0.9867333333333334\n",
            "CNN_3  Epoch: 8 [0/45056 (0%)]\tLoss: 0.058753\n",
            "CNN_3  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.025919\n",
            "CNN_3  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.013578\n",
            "CNN_3  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.003397\n",
            "\n",
            "Test set: Average loss: 0.0395, Accuracy: 14799/15000 (98.66%)\n",
            "\n",
            "CNN_3  Epoch: 9 [0/45056 (0%)]\tLoss: 0.024744\n",
            "CNN_3  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.014839\n",
            "CNN_3  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.010563\n",
            "CNN_3  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.021466\n",
            "\n",
            "Test set: Average loss: 0.0441, Accuracy: 14795/15000 (98.63%)\n",
            "\n",
            "CNN_3  Epoch: 10 [0/45056 (0%)]\tLoss: 0.011232\n",
            "CNN_3  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.022100\n",
            "CNN_3  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.010609\n",
            "CNN_3  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.023231\n",
            "\n",
            "Test set: Average loss: 0.0395, Accuracy: 14821/15000 (98.81%)\n",
            "\n",
            "new accuracy parameters saved 0.9880666666666666\n",
            "Training complete in 2m 13s\n",
            "\n",
            "   --  Run :  9\n",
            "CNN_3  Epoch: 1 [0/45056 (0%)]\tLoss: 2.297810\n",
            "CNN_3  Epoch: 1 [12800/45056 (28%)]\tLoss: 1.988570\n",
            "CNN_3  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.512353\n",
            "CNN_3  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.967090\n",
            "\n",
            "Test set: Average loss: 0.1981, Accuracy: 14128/15000 (94.19%)\n",
            "\n",
            "new accuracy parameters saved 0.9418666666666666\n",
            "CNN_3  Epoch: 2 [0/45056 (0%)]\tLoss: 0.207549\n",
            "CNN_3  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.183223\n",
            "CNN_3  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.112207\n",
            "CNN_3  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.151777\n",
            "\n",
            "Test set: Average loss: 0.1722, Accuracy: 14143/15000 (94.29%)\n",
            "\n",
            "new accuracy parameters saved 0.9428666666666666\n",
            "CNN_3  Epoch: 3 [0/45056 (0%)]\tLoss: 0.131923\n",
            "CNN_3  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.094949\n",
            "CNN_3  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.146594\n",
            "CNN_3  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.062683\n",
            "\n",
            "Test set: Average loss: 0.0736, Accuracy: 14648/15000 (97.65%)\n",
            "\n",
            "new accuracy parameters saved 0.9765333333333334\n",
            "CNN_3  Epoch: 4 [0/45056 (0%)]\tLoss: 0.040293\n",
            "CNN_3  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.095731\n",
            "CNN_3  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.041773\n",
            "CNN_3  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.037393\n",
            "\n",
            "Test set: Average loss: 0.0618, Accuracy: 14720/15000 (98.13%)\n",
            "\n",
            "new accuracy parameters saved 0.9813333333333333\n",
            "CNN_3  Epoch: 5 [0/45056 (0%)]\tLoss: 0.036299\n",
            "CNN_3  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.072905\n",
            "CNN_3  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.094822\n",
            "CNN_3  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.049840\n",
            "\n",
            "Test set: Average loss: 0.0552, Accuracy: 14742/15000 (98.28%)\n",
            "\n",
            "new accuracy parameters saved 0.9828\n",
            "CNN_3  Epoch: 6 [0/45056 (0%)]\tLoss: 0.057077\n",
            "CNN_3  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.022211\n",
            "CNN_3  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.037587\n",
            "CNN_3  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.026980\n",
            "\n",
            "Test set: Average loss: 0.0473, Accuracy: 14785/15000 (98.57%)\n",
            "\n",
            "new accuracy parameters saved 0.9856666666666667\n",
            "CNN_3  Epoch: 7 [0/45056 (0%)]\tLoss: 0.009275\n",
            "CNN_3  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.013124\n",
            "CNN_3  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.079891\n",
            "CNN_3  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.023430\n",
            "\n",
            "Test set: Average loss: 0.0581, Accuracy: 14717/15000 (98.11%)\n",
            "\n",
            "CNN_3  Epoch: 8 [0/45056 (0%)]\tLoss: 0.056718\n",
            "CNN_3  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.077704\n",
            "CNN_3  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.050206\n",
            "CNN_3  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.025562\n",
            "\n",
            "Test set: Average loss: 0.0566, Accuracy: 14738/15000 (98.25%)\n",
            "\n",
            "CNN_3  Epoch: 9 [0/45056 (0%)]\tLoss: 0.074986\n",
            "CNN_3  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.061569\n",
            "CNN_3  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.053452\n",
            "CNN_3  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.049974\n",
            "\n",
            "Test set: Average loss: 0.0395, Accuracy: 14813/15000 (98.75%)\n",
            "\n",
            "new accuracy parameters saved 0.9875333333333334\n",
            "CNN_3  Epoch: 10 [0/45056 (0%)]\tLoss: 0.009529\n",
            "CNN_3  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.039676\n",
            "CNN_3  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.003006\n",
            "CNN_3  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.003823\n",
            "\n",
            "Test set: Average loss: 0.0370, Accuracy: 14844/15000 (98.96%)\n",
            "\n",
            "new accuracy parameters saved 0.9896\n",
            "Training complete in 2m 13s\n",
            "\n",
            "   --  Run :  10\n",
            "CNN_3  Epoch: 1 [0/45056 (0%)]\tLoss: 2.308969\n",
            "CNN_3  Epoch: 1 [12800/45056 (28%)]\tLoss: 1.922462\n",
            "CNN_3  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.417637\n",
            "CNN_3  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.205000\n",
            "\n",
            "Test set: Average loss: 0.2204, Accuracy: 13934/15000 (92.89%)\n",
            "\n",
            "new accuracy parameters saved 0.9289333333333334\n",
            "CNN_3  Epoch: 2 [0/45056 (0%)]\tLoss: 0.161533\n",
            "CNN_3  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.162640\n",
            "CNN_3  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.111247\n",
            "CNN_3  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.125218\n",
            "\n",
            "Test set: Average loss: 0.2476, Accuracy: 13755/15000 (91.70%)\n",
            "\n",
            "CNN_3  Epoch: 3 [0/45056 (0%)]\tLoss: 0.244974\n",
            "CNN_3  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.092606\n",
            "CNN_3  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.165306\n",
            "CNN_3  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.129279\n",
            "\n",
            "Test set: Average loss: 0.0919, Accuracy: 14545/15000 (96.97%)\n",
            "\n",
            "new accuracy parameters saved 0.9696666666666667\n",
            "CNN_3  Epoch: 4 [0/45056 (0%)]\tLoss: 0.150033\n",
            "CNN_3  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.029609\n",
            "CNN_3  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.013375\n",
            "CNN_3  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.056548\n",
            "\n",
            "Test set: Average loss: 0.0558, Accuracy: 14745/15000 (98.30%)\n",
            "\n",
            "new accuracy parameters saved 0.983\n",
            "CNN_3  Epoch: 5 [0/45056 (0%)]\tLoss: 0.009751\n",
            "CNN_3  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.039832\n",
            "CNN_3  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.011053\n",
            "CNN_3  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.054824\n",
            "\n",
            "Test set: Average loss: 0.0594, Accuracy: 14707/15000 (98.05%)\n",
            "\n",
            "CNN_3  Epoch: 6 [0/45056 (0%)]\tLoss: 0.017353\n",
            "CNN_3  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.011733\n",
            "CNN_3  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.029426\n",
            "CNN_3  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.006540\n",
            "\n",
            "Test set: Average loss: 0.0483, Accuracy: 14766/15000 (98.44%)\n",
            "\n",
            "new accuracy parameters saved 0.9844\n",
            "CNN_3  Epoch: 7 [0/45056 (0%)]\tLoss: 0.016993\n",
            "CNN_3  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.022732\n",
            "CNN_3  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.057002\n",
            "CNN_3  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.077978\n",
            "\n",
            "Test set: Average loss: 0.0528, Accuracy: 14757/15000 (98.38%)\n",
            "\n",
            "CNN_3  Epoch: 8 [0/45056 (0%)]\tLoss: 0.032595\n",
            "CNN_3  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.026557\n",
            "CNN_3  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.015810\n",
            "CNN_3  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.014149\n",
            "\n",
            "Test set: Average loss: 0.0409, Accuracy: 14827/15000 (98.85%)\n",
            "\n",
            "new accuracy parameters saved 0.9884666666666667\n",
            "CNN_3  Epoch: 9 [0/45056 (0%)]\tLoss: 0.002819\n",
            "CNN_3  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.011579\n",
            "CNN_3  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.063071\n",
            "CNN_3  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.015538\n",
            "\n",
            "Test set: Average loss: 0.0443, Accuracy: 14807/15000 (98.71%)\n",
            "\n",
            "CNN_3  Epoch: 10 [0/45056 (0%)]\tLoss: 0.051149\n",
            "CNN_3  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.018761\n",
            "CNN_3  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.016813\n",
            "CNN_3  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.013931\n",
            "\n",
            "Test set: Average loss: 0.0359, Accuracy: 14840/15000 (98.93%)\n",
            "\n",
            "new accuracy parameters saved 0.9893333333333333\n",
            "Training complete in 2m 13s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OBRH6Vd8aTAG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### MLP"
      ]
    },
    {
      "metadata": {
        "id": "ZOKaKG_XCzzQ",
        "colab_type": "code",
        "outputId": "08118424-a26a-4f72-df53-2523c64bcada",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 14160
        }
      },
      "cell_type": "code",
      "source": [
        "MultipleRun(runs,epochs,MLP_2L())\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   --  Run :  1\n",
            "MLP_2L  Epoch: 1 [0/45056 (0%)]\tLoss: 2.306213\n",
            "MLP_2L  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.713640\n",
            "MLP_2L  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.375195\n",
            "MLP_2L  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.347246\n",
            "\n",
            "Test set: Average loss: 0.3465, Accuracy: 13518/15000 (90.12%)\n",
            "\n",
            "new accuracy parameters saved 0.9012\n",
            "MLP_2L  Epoch: 2 [0/45056 (0%)]\tLoss: 0.399282\n",
            "MLP_2L  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.312602\n",
            "MLP_2L  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.358865\n",
            "MLP_2L  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.225480\n",
            "\n",
            "Test set: Average loss: 0.2626, Accuracy: 13878/15000 (92.52%)\n",
            "\n",
            "new accuracy parameters saved 0.9252\n",
            "MLP_2L  Epoch: 3 [0/45056 (0%)]\tLoss: 0.251717\n",
            "MLP_2L  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.203761\n",
            "MLP_2L  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.279361\n",
            "MLP_2L  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.183252\n",
            "\n",
            "Test set: Average loss: 0.2152, Accuracy: 14080/15000 (93.87%)\n",
            "\n",
            "new accuracy parameters saved 0.9386666666666666\n",
            "MLP_2L  Epoch: 4 [0/45056 (0%)]\tLoss: 0.170148\n",
            "MLP_2L  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.135501\n",
            "MLP_2L  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.214950\n",
            "MLP_2L  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.087036\n",
            "\n",
            "Test set: Average loss: 0.1757, Accuracy: 14260/15000 (95.07%)\n",
            "\n",
            "new accuracy parameters saved 0.9506666666666667\n",
            "MLP_2L  Epoch: 5 [0/45056 (0%)]\tLoss: 0.065195\n",
            "MLP_2L  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.217978\n",
            "MLP_2L  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.125003\n",
            "MLP_2L  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.135126\n",
            "\n",
            "Test set: Average loss: 0.1564, Accuracy: 14358/15000 (95.72%)\n",
            "\n",
            "new accuracy parameters saved 0.9572\n",
            "MLP_2L  Epoch: 6 [0/45056 (0%)]\tLoss: 0.147358\n",
            "MLP_2L  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.100817\n",
            "MLP_2L  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.155792\n",
            "MLP_2L  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.088012\n",
            "\n",
            "Test set: Average loss: 0.1402, Accuracy: 14404/15000 (96.03%)\n",
            "\n",
            "new accuracy parameters saved 0.9602666666666667\n",
            "MLP_2L  Epoch: 7 [0/45056 (0%)]\tLoss: 0.077687\n",
            "MLP_2L  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.091134\n",
            "MLP_2L  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.169809\n",
            "MLP_2L  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.126332\n",
            "\n",
            "Test set: Average loss: 0.1270, Accuracy: 14460/15000 (96.40%)\n",
            "\n",
            "new accuracy parameters saved 0.964\n",
            "MLP_2L  Epoch: 8 [0/45056 (0%)]\tLoss: 0.140073\n",
            "MLP_2L  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.099469\n",
            "MLP_2L  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.042888\n",
            "MLP_2L  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.084166\n",
            "\n",
            "Test set: Average loss: 0.1160, Accuracy: 14492/15000 (96.61%)\n",
            "\n",
            "new accuracy parameters saved 0.9661333333333333\n",
            "MLP_2L  Epoch: 9 [0/45056 (0%)]\tLoss: 0.074272\n",
            "MLP_2L  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.062469\n",
            "MLP_2L  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.065312\n",
            "MLP_2L  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.019106\n",
            "\n",
            "Test set: Average loss: 0.1162, Accuracy: 14510/15000 (96.73%)\n",
            "\n",
            "new accuracy parameters saved 0.9673333333333334\n",
            "MLP_2L  Epoch: 10 [0/45056 (0%)]\tLoss: 0.057653\n",
            "MLP_2L  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.065288\n",
            "MLP_2L  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.094283\n",
            "MLP_2L  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.033173\n",
            "\n",
            "Test set: Average loss: 0.1042, Accuracy: 14542/15000 (96.95%)\n",
            "\n",
            "new accuracy parameters saved 0.9694666666666667\n",
            "Training complete in 1m 4s\n",
            "\n",
            "   --  Run :  2\n",
            "MLP_2L  Epoch: 1 [0/45056 (0%)]\tLoss: 2.301891\n",
            "MLP_2L  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.667565\n",
            "MLP_2L  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.532980\n",
            "MLP_2L  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.326155\n",
            "\n",
            "Test set: Average loss: 0.3710, Accuracy: 13396/15000 (89.31%)\n",
            "\n",
            "new accuracy parameters saved 0.8930666666666667\n",
            "MLP_2L  Epoch: 2 [0/45056 (0%)]\tLoss: 0.343212\n",
            "MLP_2L  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.259629\n",
            "MLP_2L  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.364058\n",
            "MLP_2L  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.357062\n",
            "\n",
            "Test set: Average loss: 0.2658, Accuracy: 13851/15000 (92.34%)\n",
            "\n",
            "new accuracy parameters saved 0.9234\n",
            "MLP_2L  Epoch: 3 [0/45056 (0%)]\tLoss: 0.221350\n",
            "MLP_2L  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.244847\n",
            "MLP_2L  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.227827\n",
            "MLP_2L  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.160710\n",
            "\n",
            "Test set: Average loss: 0.2170, Accuracy: 14087/15000 (93.91%)\n",
            "\n",
            "new accuracy parameters saved 0.9391333333333334\n",
            "MLP_2L  Epoch: 4 [0/45056 (0%)]\tLoss: 0.192265\n",
            "MLP_2L  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.220538\n",
            "MLP_2L  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.142122\n",
            "MLP_2L  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.222895\n",
            "\n",
            "Test set: Average loss: 0.1795, Accuracy: 14232/15000 (94.88%)\n",
            "\n",
            "new accuracy parameters saved 0.9488\n",
            "MLP_2L  Epoch: 5 [0/45056 (0%)]\tLoss: 0.153319\n",
            "MLP_2L  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.099875\n",
            "MLP_2L  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.148651\n",
            "MLP_2L  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.117106\n",
            "\n",
            "Test set: Average loss: 0.1643, Accuracy: 14298/15000 (95.32%)\n",
            "\n",
            "new accuracy parameters saved 0.9532\n",
            "MLP_2L  Epoch: 6 [0/45056 (0%)]\tLoss: 0.110043\n",
            "MLP_2L  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.134236\n",
            "MLP_2L  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.163855\n",
            "MLP_2L  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.191437\n",
            "\n",
            "Test set: Average loss: 0.1370, Accuracy: 14417/15000 (96.11%)\n",
            "\n",
            "new accuracy parameters saved 0.9611333333333333\n",
            "MLP_2L  Epoch: 7 [0/45056 (0%)]\tLoss: 0.135878\n",
            "MLP_2L  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.121403\n",
            "MLP_2L  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.070095\n",
            "MLP_2L  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.082319\n",
            "\n",
            "Test set: Average loss: 0.1284, Accuracy: 14449/15000 (96.33%)\n",
            "\n",
            "new accuracy parameters saved 0.9632666666666667\n",
            "MLP_2L  Epoch: 8 [0/45056 (0%)]\tLoss: 0.034625\n",
            "MLP_2L  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.055903\n",
            "MLP_2L  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.080563\n",
            "MLP_2L  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.092075\n",
            "\n",
            "Test set: Average loss: 0.1232, Accuracy: 14463/15000 (96.42%)\n",
            "\n",
            "new accuracy parameters saved 0.9642\n",
            "MLP_2L  Epoch: 9 [0/45056 (0%)]\tLoss: 0.040184\n",
            "MLP_2L  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.107002\n",
            "MLP_2L  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.082369\n",
            "MLP_2L  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.076092\n",
            "\n",
            "Test set: Average loss: 0.1181, Accuracy: 14508/15000 (96.72%)\n",
            "\n",
            "new accuracy parameters saved 0.9672\n",
            "MLP_2L  Epoch: 10 [0/45056 (0%)]\tLoss: 0.094315\n",
            "MLP_2L  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.040895\n",
            "MLP_2L  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.068231\n",
            "MLP_2L  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.068282\n",
            "\n",
            "Test set: Average loss: 0.1068, Accuracy: 14540/15000 (96.93%)\n",
            "\n",
            "new accuracy parameters saved 0.9693333333333334\n",
            "Training complete in 1m 4s\n",
            "\n",
            "   --  Run :  3\n",
            "MLP_2L  Epoch: 1 [0/45056 (0%)]\tLoss: 2.299392\n",
            "MLP_2L  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.587185\n",
            "MLP_2L  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.319271\n",
            "MLP_2L  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.393182\n",
            "\n",
            "Test set: Average loss: 0.3527, Accuracy: 13419/15000 (89.46%)\n",
            "\n",
            "new accuracy parameters saved 0.8946\n",
            "MLP_2L  Epoch: 2 [0/45056 (0%)]\tLoss: 0.308453\n",
            "MLP_2L  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.278838\n",
            "MLP_2L  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.282595\n",
            "MLP_2L  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.196660\n",
            "\n",
            "Test set: Average loss: 0.2788, Accuracy: 13785/15000 (91.90%)\n",
            "\n",
            "new accuracy parameters saved 0.919\n",
            "MLP_2L  Epoch: 3 [0/45056 (0%)]\tLoss: 0.289208\n",
            "MLP_2L  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.173948\n",
            "MLP_2L  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.117880\n",
            "MLP_2L  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.227868\n",
            "\n",
            "Test set: Average loss: 0.2129, Accuracy: 14116/15000 (94.11%)\n",
            "\n",
            "new accuracy parameters saved 0.9410666666666667\n",
            "MLP_2L  Epoch: 4 [0/45056 (0%)]\tLoss: 0.147912\n",
            "MLP_2L  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.230685\n",
            "MLP_2L  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.242049\n",
            "MLP_2L  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.080060\n",
            "\n",
            "Test set: Average loss: 0.1763, Accuracy: 14250/15000 (95.00%)\n",
            "\n",
            "new accuracy parameters saved 0.95\n",
            "MLP_2L  Epoch: 5 [0/45056 (0%)]\tLoss: 0.136385\n",
            "MLP_2L  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.115036\n",
            "MLP_2L  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.124182\n",
            "MLP_2L  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.083838\n",
            "\n",
            "Test set: Average loss: 0.1524, Accuracy: 14349/15000 (95.66%)\n",
            "\n",
            "new accuracy parameters saved 0.9566\n",
            "MLP_2L  Epoch: 6 [0/45056 (0%)]\tLoss: 0.054889\n",
            "MLP_2L  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.128996\n",
            "MLP_2L  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.140326\n",
            "MLP_2L  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.146163\n",
            "\n",
            "Test set: Average loss: 0.1412, Accuracy: 14404/15000 (96.03%)\n",
            "\n",
            "new accuracy parameters saved 0.9602666666666667\n",
            "MLP_2L  Epoch: 7 [0/45056 (0%)]\tLoss: 0.056245\n",
            "MLP_2L  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.091021\n",
            "MLP_2L  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.029227\n",
            "MLP_2L  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.044379\n",
            "\n",
            "Test set: Average loss: 0.1289, Accuracy: 14446/15000 (96.31%)\n",
            "\n",
            "new accuracy parameters saved 0.9630666666666666\n",
            "MLP_2L  Epoch: 8 [0/45056 (0%)]\tLoss: 0.112330\n",
            "MLP_2L  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.091420\n",
            "MLP_2L  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.030864\n",
            "MLP_2L  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.083127\n",
            "\n",
            "Test set: Average loss: 0.1175, Accuracy: 14496/15000 (96.64%)\n",
            "\n",
            "new accuracy parameters saved 0.9664\n",
            "MLP_2L  Epoch: 9 [0/45056 (0%)]\tLoss: 0.148715\n",
            "MLP_2L  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.050679\n",
            "MLP_2L  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.071314\n",
            "MLP_2L  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.057915\n",
            "\n",
            "Test set: Average loss: 0.1081, Accuracy: 14549/15000 (96.99%)\n",
            "\n",
            "new accuracy parameters saved 0.9699333333333333\n",
            "MLP_2L  Epoch: 10 [0/45056 (0%)]\tLoss: 0.058526\n",
            "MLP_2L  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.051331\n",
            "MLP_2L  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.040460\n",
            "MLP_2L  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.094246\n",
            "\n",
            "Test set: Average loss: 0.1029, Accuracy: 14557/15000 (97.05%)\n",
            "\n",
            "new accuracy parameters saved 0.9704666666666667\n",
            "Training complete in 1m 5s\n",
            "\n",
            "   --  Run :  4\n",
            "MLP_2L  Epoch: 1 [0/45056 (0%)]\tLoss: 2.304235\n",
            "MLP_2L  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.641460\n",
            "MLP_2L  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.305019\n",
            "MLP_2L  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.428279\n",
            "\n",
            "Test set: Average loss: 0.3480, Accuracy: 13486/15000 (89.91%)\n",
            "\n",
            "new accuracy parameters saved 0.8990666666666667\n",
            "MLP_2L  Epoch: 2 [0/45056 (0%)]\tLoss: 0.378504\n",
            "MLP_2L  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.337514\n",
            "MLP_2L  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.189680\n",
            "MLP_2L  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.278210\n",
            "\n",
            "Test set: Average loss: 0.2565, Accuracy: 13940/15000 (92.93%)\n",
            "\n",
            "new accuracy parameters saved 0.9293333333333333\n",
            "MLP_2L  Epoch: 3 [0/45056 (0%)]\tLoss: 0.204377\n",
            "MLP_2L  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.242702\n",
            "MLP_2L  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.356944\n",
            "MLP_2L  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.134991\n",
            "\n",
            "Test set: Average loss: 0.2093, Accuracy: 14079/15000 (93.86%)\n",
            "\n",
            "new accuracy parameters saved 0.9386\n",
            "MLP_2L  Epoch: 4 [0/45056 (0%)]\tLoss: 0.255487\n",
            "MLP_2L  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.167530\n",
            "MLP_2L  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.181208\n",
            "MLP_2L  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.145053\n",
            "\n",
            "Test set: Average loss: 0.1702, Accuracy: 14289/15000 (95.26%)\n",
            "\n",
            "new accuracy parameters saved 0.9526\n",
            "MLP_2L  Epoch: 5 [0/45056 (0%)]\tLoss: 0.134748\n",
            "MLP_2L  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.136793\n",
            "MLP_2L  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.131420\n",
            "MLP_2L  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.060725\n",
            "\n",
            "Test set: Average loss: 0.1502, Accuracy: 14374/15000 (95.83%)\n",
            "\n",
            "new accuracy parameters saved 0.9582666666666667\n",
            "MLP_2L  Epoch: 6 [0/45056 (0%)]\tLoss: 0.071270\n",
            "MLP_2L  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.124408\n",
            "MLP_2L  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.047970\n",
            "MLP_2L  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.088973\n",
            "\n",
            "Test set: Average loss: 0.1348, Accuracy: 14442/15000 (96.28%)\n",
            "\n",
            "new accuracy parameters saved 0.9628\n",
            "MLP_2L  Epoch: 7 [0/45056 (0%)]\tLoss: 0.065838\n",
            "MLP_2L  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.082717\n",
            "MLP_2L  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.091554\n",
            "MLP_2L  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.079392\n",
            "\n",
            "Test set: Average loss: 0.1237, Accuracy: 14464/15000 (96.43%)\n",
            "\n",
            "new accuracy parameters saved 0.9642666666666667\n",
            "MLP_2L  Epoch: 8 [0/45056 (0%)]\tLoss: 0.055342\n",
            "MLP_2L  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.160264\n",
            "MLP_2L  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.078347\n",
            "MLP_2L  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.062024\n",
            "\n",
            "Test set: Average loss: 0.1166, Accuracy: 14512/15000 (96.75%)\n",
            "\n",
            "new accuracy parameters saved 0.9674666666666667\n",
            "MLP_2L  Epoch: 9 [0/45056 (0%)]\tLoss: 0.068950\n",
            "MLP_2L  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.093493\n",
            "MLP_2L  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.040957\n",
            "MLP_2L  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.054171\n",
            "\n",
            "Test set: Average loss: 0.1103, Accuracy: 14524/15000 (96.83%)\n",
            "\n",
            "new accuracy parameters saved 0.9682666666666667\n",
            "MLP_2L  Epoch: 10 [0/45056 (0%)]\tLoss: 0.032210\n",
            "MLP_2L  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.089101\n",
            "MLP_2L  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.041701\n",
            "MLP_2L  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.149184\n",
            "\n",
            "Test set: Average loss: 0.0996, Accuracy: 14573/15000 (97.15%)\n",
            "\n",
            "new accuracy parameters saved 0.9715333333333334\n",
            "Training complete in 1m 4s\n",
            "\n",
            "   --  Run :  5\n",
            "MLP_2L  Epoch: 1 [0/45056 (0%)]\tLoss: 2.308230\n",
            "MLP_2L  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.581435\n",
            "MLP_2L  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.381413\n",
            "MLP_2L  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.394894\n",
            "\n",
            "Test set: Average loss: 0.3409, Accuracy: 13561/15000 (90.41%)\n",
            "\n",
            "new accuracy parameters saved 0.9040666666666667\n",
            "MLP_2L  Epoch: 2 [0/45056 (0%)]\tLoss: 0.355932\n",
            "MLP_2L  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.189699\n",
            "MLP_2L  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.326537\n",
            "MLP_2L  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.224275\n",
            "\n",
            "Test set: Average loss: 0.2749, Accuracy: 13847/15000 (92.31%)\n",
            "\n",
            "new accuracy parameters saved 0.9231333333333334\n",
            "MLP_2L  Epoch: 3 [0/45056 (0%)]\tLoss: 0.145736\n",
            "MLP_2L  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.219246\n",
            "MLP_2L  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.216669\n",
            "MLP_2L  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.281395\n",
            "\n",
            "Test set: Average loss: 0.2179, Accuracy: 14093/15000 (93.95%)\n",
            "\n",
            "new accuracy parameters saved 0.9395333333333333\n",
            "MLP_2L  Epoch: 4 [0/45056 (0%)]\tLoss: 0.269916\n",
            "MLP_2L  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.129607\n",
            "MLP_2L  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.145807\n",
            "MLP_2L  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.174035\n",
            "\n",
            "Test set: Average loss: 0.1786, Accuracy: 14234/15000 (94.89%)\n",
            "\n",
            "new accuracy parameters saved 0.9489333333333333\n",
            "MLP_2L  Epoch: 5 [0/45056 (0%)]\tLoss: 0.117721\n",
            "MLP_2L  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.151045\n",
            "MLP_2L  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.143924\n",
            "MLP_2L  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.137484\n",
            "\n",
            "Test set: Average loss: 0.1476, Accuracy: 14395/15000 (95.97%)\n",
            "\n",
            "new accuracy parameters saved 0.9596666666666667\n",
            "MLP_2L  Epoch: 6 [0/45056 (0%)]\tLoss: 0.038846\n",
            "MLP_2L  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.116917\n",
            "MLP_2L  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.138514\n",
            "MLP_2L  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.052171\n",
            "\n",
            "Test set: Average loss: 0.1354, Accuracy: 14414/15000 (96.09%)\n",
            "\n",
            "new accuracy parameters saved 0.9609333333333333\n",
            "MLP_2L  Epoch: 7 [0/45056 (0%)]\tLoss: 0.119696\n",
            "MLP_2L  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.091242\n",
            "MLP_2L  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.099428\n",
            "MLP_2L  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.106000\n",
            "\n",
            "Test set: Average loss: 0.1300, Accuracy: 14461/15000 (96.41%)\n",
            "\n",
            "new accuracy parameters saved 0.9640666666666666\n",
            "MLP_2L  Epoch: 8 [0/45056 (0%)]\tLoss: 0.052079\n",
            "MLP_2L  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.117555\n",
            "MLP_2L  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.054795\n",
            "MLP_2L  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.127822\n",
            "\n",
            "Test set: Average loss: 0.1122, Accuracy: 14509/15000 (96.73%)\n",
            "\n",
            "new accuracy parameters saved 0.9672666666666667\n",
            "MLP_2L  Epoch: 9 [0/45056 (0%)]\tLoss: 0.046530\n",
            "MLP_2L  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.155908\n",
            "MLP_2L  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.027885\n",
            "MLP_2L  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.139933\n",
            "\n",
            "Test set: Average loss: 0.1083, Accuracy: 14548/15000 (96.99%)\n",
            "\n",
            "new accuracy parameters saved 0.9698666666666667\n",
            "MLP_2L  Epoch: 10 [0/45056 (0%)]\tLoss: 0.059253\n",
            "MLP_2L  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.045312\n",
            "MLP_2L  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.054909\n",
            "MLP_2L  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.102284\n",
            "\n",
            "Test set: Average loss: 0.0999, Accuracy: 14582/15000 (97.21%)\n",
            "\n",
            "new accuracy parameters saved 0.9721333333333333\n",
            "Training complete in 1m 4s\n",
            "\n",
            "   --  Run :  6\n",
            "MLP_2L  Epoch: 1 [0/45056 (0%)]\tLoss: 2.302422\n",
            "MLP_2L  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.684423\n",
            "MLP_2L  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.380033\n",
            "MLP_2L  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.260001\n",
            "\n",
            "Test set: Average loss: 0.3651, Accuracy: 13383/15000 (89.22%)\n",
            "\n",
            "new accuracy parameters saved 0.8922\n",
            "MLP_2L  Epoch: 2 [0/45056 (0%)]\tLoss: 0.293426\n",
            "MLP_2L  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.296969\n",
            "MLP_2L  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.223985\n",
            "MLP_2L  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.289263\n",
            "\n",
            "Test set: Average loss: 0.2596, Accuracy: 13892/15000 (92.61%)\n",
            "\n",
            "new accuracy parameters saved 0.9261333333333334\n",
            "MLP_2L  Epoch: 3 [0/45056 (0%)]\tLoss: 0.238158\n",
            "MLP_2L  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.243267\n",
            "MLP_2L  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.144305\n",
            "MLP_2L  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.185185\n",
            "\n",
            "Test set: Average loss: 0.2171, Accuracy: 14085/15000 (93.90%)\n",
            "\n",
            "new accuracy parameters saved 0.939\n",
            "MLP_2L  Epoch: 4 [0/45056 (0%)]\tLoss: 0.164206\n",
            "MLP_2L  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.233488\n",
            "MLP_2L  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.146617\n",
            "MLP_2L  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.155306\n",
            "\n",
            "Test set: Average loss: 0.1751, Accuracy: 14264/15000 (95.09%)\n",
            "\n",
            "new accuracy parameters saved 0.9509333333333333\n",
            "MLP_2L  Epoch: 5 [0/45056 (0%)]\tLoss: 0.113417\n",
            "MLP_2L  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.197072\n",
            "MLP_2L  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.128696\n",
            "MLP_2L  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.136102\n",
            "\n",
            "Test set: Average loss: 0.1742, Accuracy: 14267/15000 (95.11%)\n",
            "\n",
            "new accuracy parameters saved 0.9511333333333334\n",
            "MLP_2L  Epoch: 6 [0/45056 (0%)]\tLoss: 0.095577\n",
            "MLP_2L  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.145103\n",
            "MLP_2L  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.093197\n",
            "MLP_2L  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.135624\n",
            "\n",
            "Test set: Average loss: 0.1759, Accuracy: 14223/15000 (94.82%)\n",
            "\n",
            "MLP_2L  Epoch: 7 [0/45056 (0%)]\tLoss: 0.201359\n",
            "MLP_2L  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.058782\n",
            "MLP_2L  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.151657\n",
            "MLP_2L  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.167997\n",
            "\n",
            "Test set: Average loss: 0.1282, Accuracy: 14454/15000 (96.36%)\n",
            "\n",
            "new accuracy parameters saved 0.9636\n",
            "MLP_2L  Epoch: 8 [0/45056 (0%)]\tLoss: 0.091867\n",
            "MLP_2L  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.052766\n",
            "MLP_2L  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.141534\n",
            "MLP_2L  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.084977\n",
            "\n",
            "Test set: Average loss: 0.1184, Accuracy: 14502/15000 (96.68%)\n",
            "\n",
            "new accuracy parameters saved 0.9668\n",
            "MLP_2L  Epoch: 9 [0/45056 (0%)]\tLoss: 0.038090\n",
            "MLP_2L  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.102331\n",
            "MLP_2L  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.069336\n",
            "MLP_2L  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.096003\n",
            "\n",
            "Test set: Average loss: 0.1074, Accuracy: 14534/15000 (96.89%)\n",
            "\n",
            "new accuracy parameters saved 0.9689333333333333\n",
            "MLP_2L  Epoch: 10 [0/45056 (0%)]\tLoss: 0.039231\n",
            "MLP_2L  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.031808\n",
            "MLP_2L  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.043149\n",
            "MLP_2L  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.026538\n",
            "\n",
            "Test set: Average loss: 0.1078, Accuracy: 14521/15000 (96.81%)\n",
            "\n",
            "Training complete in 1m 5s\n",
            "\n",
            "   --  Run :  7\n",
            "MLP_2L  Epoch: 1 [0/45056 (0%)]\tLoss: 2.304960\n",
            "MLP_2L  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.634565\n",
            "MLP_2L  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.388967\n",
            "MLP_2L  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.243624\n",
            "\n",
            "Test set: Average loss: 0.3585, Accuracy: 13455/15000 (89.70%)\n",
            "\n",
            "new accuracy parameters saved 0.897\n",
            "MLP_2L  Epoch: 2 [0/45056 (0%)]\tLoss: 0.346018\n",
            "MLP_2L  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.170049\n",
            "MLP_2L  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.275436\n",
            "MLP_2L  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.263374\n",
            "\n",
            "Test set: Average loss: 0.2519, Accuracy: 13933/15000 (92.89%)\n",
            "\n",
            "new accuracy parameters saved 0.9288666666666666\n",
            "MLP_2L  Epoch: 3 [0/45056 (0%)]\tLoss: 0.193325\n",
            "MLP_2L  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.152374\n",
            "MLP_2L  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.208659\n",
            "MLP_2L  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.298939\n",
            "\n",
            "Test set: Average loss: 0.2051, Accuracy: 14150/15000 (94.33%)\n",
            "\n",
            "new accuracy parameters saved 0.9433333333333334\n",
            "MLP_2L  Epoch: 4 [0/45056 (0%)]\tLoss: 0.112900\n",
            "MLP_2L  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.286252\n",
            "MLP_2L  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.097245\n",
            "MLP_2L  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.217139\n",
            "\n",
            "Test set: Average loss: 0.1776, Accuracy: 14259/15000 (95.06%)\n",
            "\n",
            "new accuracy parameters saved 0.9506\n",
            "MLP_2L  Epoch: 5 [0/45056 (0%)]\tLoss: 0.115599\n",
            "MLP_2L  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.067539\n",
            "MLP_2L  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.210504\n",
            "MLP_2L  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.137680\n",
            "\n",
            "Test set: Average loss: 0.1602, Accuracy: 14315/15000 (95.43%)\n",
            "\n",
            "new accuracy parameters saved 0.9543333333333334\n",
            "MLP_2L  Epoch: 6 [0/45056 (0%)]\tLoss: 0.213062\n",
            "MLP_2L  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.124552\n",
            "MLP_2L  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.118522\n",
            "MLP_2L  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.076538\n",
            "\n",
            "Test set: Average loss: 0.1374, Accuracy: 14434/15000 (96.23%)\n",
            "\n",
            "new accuracy parameters saved 0.9622666666666667\n",
            "MLP_2L  Epoch: 7 [0/45056 (0%)]\tLoss: 0.095862\n",
            "MLP_2L  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.094362\n",
            "MLP_2L  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.104547\n",
            "MLP_2L  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.090910\n",
            "\n",
            "Test set: Average loss: 0.1217, Accuracy: 14462/15000 (96.41%)\n",
            "\n",
            "new accuracy parameters saved 0.9641333333333333\n",
            "MLP_2L  Epoch: 8 [0/45056 (0%)]\tLoss: 0.094466\n",
            "MLP_2L  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.070987\n",
            "MLP_2L  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.083927\n",
            "MLP_2L  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.125998\n",
            "\n",
            "Test set: Average loss: 0.1111, Accuracy: 14531/15000 (96.87%)\n",
            "\n",
            "new accuracy parameters saved 0.9687333333333333\n",
            "MLP_2L  Epoch: 9 [0/45056 (0%)]\tLoss: 0.058049\n",
            "MLP_2L  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.042222\n",
            "MLP_2L  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.056432\n",
            "MLP_2L  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.025381\n",
            "\n",
            "Test set: Average loss: 0.1193, Accuracy: 14483/15000 (96.55%)\n",
            "\n",
            "MLP_2L  Epoch: 10 [0/45056 (0%)]\tLoss: 0.069223\n",
            "MLP_2L  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.041774\n",
            "MLP_2L  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.118684\n",
            "MLP_2L  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.057621\n",
            "\n",
            "Test set: Average loss: 0.1033, Accuracy: 14553/15000 (97.02%)\n",
            "\n",
            "new accuracy parameters saved 0.9702\n",
            "Training complete in 1m 4s\n",
            "\n",
            "   --  Run :  8\n",
            "MLP_2L  Epoch: 1 [0/45056 (0%)]\tLoss: 2.307502\n",
            "MLP_2L  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.645268\n",
            "MLP_2L  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.504459\n",
            "MLP_2L  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.357064\n",
            "\n",
            "Test set: Average loss: 0.3457, Accuracy: 13539/15000 (90.26%)\n",
            "\n",
            "new accuracy parameters saved 0.9026\n",
            "MLP_2L  Epoch: 2 [0/45056 (0%)]\tLoss: 0.333785\n",
            "MLP_2L  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.307906\n",
            "MLP_2L  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.347974\n",
            "MLP_2L  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.202261\n",
            "\n",
            "Test set: Average loss: 0.2673, Accuracy: 13894/15000 (92.63%)\n",
            "\n",
            "new accuracy parameters saved 0.9262666666666667\n",
            "MLP_2L  Epoch: 3 [0/45056 (0%)]\tLoss: 0.170946\n",
            "MLP_2L  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.261215\n",
            "MLP_2L  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.190037\n",
            "MLP_2L  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.167334\n",
            "\n",
            "Test set: Average loss: 0.2053, Accuracy: 14136/15000 (94.24%)\n",
            "\n",
            "new accuracy parameters saved 0.9424\n",
            "MLP_2L  Epoch: 4 [0/45056 (0%)]\tLoss: 0.141623\n",
            "MLP_2L  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.139031\n",
            "MLP_2L  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.133696\n",
            "MLP_2L  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.136204\n",
            "\n",
            "Test set: Average loss: 0.1907, Accuracy: 14201/15000 (94.67%)\n",
            "\n",
            "new accuracy parameters saved 0.9467333333333333\n",
            "MLP_2L  Epoch: 5 [0/45056 (0%)]\tLoss: 0.194278\n",
            "MLP_2L  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.191705\n",
            "MLP_2L  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.110464\n",
            "MLP_2L  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.102012\n",
            "\n",
            "Test set: Average loss: 0.1546, Accuracy: 14325/15000 (95.50%)\n",
            "\n",
            "new accuracy parameters saved 0.955\n",
            "MLP_2L  Epoch: 6 [0/45056 (0%)]\tLoss: 0.148722\n",
            "MLP_2L  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.102326\n",
            "MLP_2L  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.041800\n",
            "MLP_2L  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.158301\n",
            "\n",
            "Test set: Average loss: 0.1378, Accuracy: 14423/15000 (96.15%)\n",
            "\n",
            "new accuracy parameters saved 0.9615333333333334\n",
            "MLP_2L  Epoch: 7 [0/45056 (0%)]\tLoss: 0.219949\n",
            "MLP_2L  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.122792\n",
            "MLP_2L  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.064765\n",
            "MLP_2L  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.115628\n",
            "\n",
            "Test set: Average loss: 0.1302, Accuracy: 14457/15000 (96.38%)\n",
            "\n",
            "new accuracy parameters saved 0.9638\n",
            "MLP_2L  Epoch: 8 [0/45056 (0%)]\tLoss: 0.113515\n",
            "MLP_2L  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.175702\n",
            "MLP_2L  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.073974\n",
            "MLP_2L  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.147149\n",
            "\n",
            "Test set: Average loss: 0.1197, Accuracy: 14509/15000 (96.73%)\n",
            "\n",
            "new accuracy parameters saved 0.9672666666666667\n",
            "MLP_2L  Epoch: 9 [0/45056 (0%)]\tLoss: 0.083493\n",
            "MLP_2L  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.096814\n",
            "MLP_2L  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.042256\n",
            "MLP_2L  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.056532\n",
            "\n",
            "Test set: Average loss: 0.1265, Accuracy: 14466/15000 (96.44%)\n",
            "\n",
            "MLP_2L  Epoch: 10 [0/45056 (0%)]\tLoss: 0.069795\n",
            "MLP_2L  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.078855\n",
            "MLP_2L  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.066797\n",
            "MLP_2L  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.123883\n",
            "\n",
            "Test set: Average loss: 0.1054, Accuracy: 14550/15000 (97.00%)\n",
            "\n",
            "new accuracy parameters saved 0.97\n",
            "Training complete in 1m 4s\n",
            "\n",
            "   --  Run :  9\n",
            "MLP_2L  Epoch: 1 [0/45056 (0%)]\tLoss: 2.302408\n",
            "MLP_2L  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.690372\n",
            "MLP_2L  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.485654\n",
            "MLP_2L  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.335172\n",
            "\n",
            "Test set: Average loss: 0.3338, Accuracy: 13557/15000 (90.38%)\n",
            "\n",
            "new accuracy parameters saved 0.9038\n",
            "MLP_2L  Epoch: 2 [0/45056 (0%)]\tLoss: 0.385946\n",
            "MLP_2L  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.368073\n",
            "MLP_2L  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.328315\n",
            "MLP_2L  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.264836\n",
            "\n",
            "Test set: Average loss: 0.2529, Accuracy: 13945/15000 (92.97%)\n",
            "\n",
            "new accuracy parameters saved 0.9296666666666666\n",
            "MLP_2L  Epoch: 3 [0/45056 (0%)]\tLoss: 0.261496\n",
            "MLP_2L  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.281045\n",
            "MLP_2L  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.152967\n",
            "MLP_2L  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.082301\n",
            "\n",
            "Test set: Average loss: 0.2132, Accuracy: 14067/15000 (93.78%)\n",
            "\n",
            "new accuracy parameters saved 0.9378\n",
            "MLP_2L  Epoch: 4 [0/45056 (0%)]\tLoss: 0.181656\n",
            "MLP_2L  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.146604\n",
            "MLP_2L  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.264097\n",
            "MLP_2L  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.113781\n",
            "\n",
            "Test set: Average loss: 0.1724, Accuracy: 14266/15000 (95.11%)\n",
            "\n",
            "new accuracy parameters saved 0.9510666666666666\n",
            "MLP_2L  Epoch: 5 [0/45056 (0%)]\tLoss: 0.166227\n",
            "MLP_2L  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.111678\n",
            "MLP_2L  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.128974\n",
            "MLP_2L  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.163896\n",
            "\n",
            "Test set: Average loss: 0.1659, Accuracy: 14299/15000 (95.33%)\n",
            "\n",
            "new accuracy parameters saved 0.9532666666666667\n",
            "MLP_2L  Epoch: 6 [0/45056 (0%)]\tLoss: 0.126972\n",
            "MLP_2L  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.123452\n",
            "MLP_2L  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.128977\n",
            "MLP_2L  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.090865\n",
            "\n",
            "Test set: Average loss: 0.1348, Accuracy: 14419/15000 (96.13%)\n",
            "\n",
            "new accuracy parameters saved 0.9612666666666667\n",
            "MLP_2L  Epoch: 7 [0/45056 (0%)]\tLoss: 0.059794\n",
            "MLP_2L  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.055631\n",
            "MLP_2L  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.155746\n",
            "MLP_2L  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.110532\n",
            "\n",
            "Test set: Average loss: 0.1362, Accuracy: 14423/15000 (96.15%)\n",
            "\n",
            "new accuracy parameters saved 0.9615333333333334\n",
            "MLP_2L  Epoch: 8 [0/45056 (0%)]\tLoss: 0.166863\n",
            "MLP_2L  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.031305\n",
            "MLP_2L  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.025580\n",
            "MLP_2L  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.046842\n",
            "\n",
            "Test set: Average loss: 0.1115, Accuracy: 14533/15000 (96.89%)\n",
            "\n",
            "new accuracy parameters saved 0.9688666666666667\n",
            "MLP_2L  Epoch: 9 [0/45056 (0%)]\tLoss: 0.083471\n",
            "MLP_2L  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.051636\n",
            "MLP_2L  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.085780\n",
            "MLP_2L  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.044840\n",
            "\n",
            "Test set: Average loss: 0.1059, Accuracy: 14540/15000 (96.93%)\n",
            "\n",
            "new accuracy parameters saved 0.9693333333333334\n",
            "MLP_2L  Epoch: 10 [0/45056 (0%)]\tLoss: 0.038203\n",
            "MLP_2L  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.047637\n",
            "MLP_2L  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.027957\n",
            "MLP_2L  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.089544\n",
            "\n",
            "Test set: Average loss: 0.1037, Accuracy: 14565/15000 (97.10%)\n",
            "\n",
            "new accuracy parameters saved 0.971\n",
            "Training complete in 1m 4s\n",
            "\n",
            "   --  Run :  10\n",
            "MLP_2L  Epoch: 1 [0/45056 (0%)]\tLoss: 2.300831\n",
            "MLP_2L  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.684729\n",
            "MLP_2L  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.446605\n",
            "MLP_2L  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.481194\n",
            "\n",
            "Test set: Average loss: 0.3845, Accuracy: 13305/15000 (88.70%)\n",
            "\n",
            "new accuracy parameters saved 0.887\n",
            "MLP_2L  Epoch: 2 [0/45056 (0%)]\tLoss: 0.322460\n",
            "MLP_2L  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.304128\n",
            "MLP_2L  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.256672\n",
            "MLP_2L  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.224397\n",
            "\n",
            "Test set: Average loss: 0.2644, Accuracy: 13874/15000 (92.49%)\n",
            "\n",
            "new accuracy parameters saved 0.9249333333333334\n",
            "MLP_2L  Epoch: 3 [0/45056 (0%)]\tLoss: 0.247535\n",
            "MLP_2L  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.172344\n",
            "MLP_2L  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.256764\n",
            "MLP_2L  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.282671\n",
            "\n",
            "Test set: Average loss: 0.2273, Accuracy: 14030/15000 (93.53%)\n",
            "\n",
            "new accuracy parameters saved 0.9353333333333333\n",
            "MLP_2L  Epoch: 4 [0/45056 (0%)]\tLoss: 0.217309\n",
            "MLP_2L  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.260721\n",
            "MLP_2L  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.202330\n",
            "MLP_2L  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.137503\n",
            "\n",
            "Test set: Average loss: 0.1960, Accuracy: 14181/15000 (94.54%)\n",
            "\n",
            "new accuracy parameters saved 0.9454\n",
            "MLP_2L  Epoch: 5 [0/45056 (0%)]\tLoss: 0.094621\n",
            "MLP_2L  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.138082\n",
            "MLP_2L  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.139089\n",
            "MLP_2L  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.120785\n",
            "\n",
            "Test set: Average loss: 0.1620, Accuracy: 14306/15000 (95.37%)\n",
            "\n",
            "new accuracy parameters saved 0.9537333333333333\n",
            "MLP_2L  Epoch: 6 [0/45056 (0%)]\tLoss: 0.095278\n",
            "MLP_2L  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.127803\n",
            "MLP_2L  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.110736\n",
            "MLP_2L  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.127869\n",
            "\n",
            "Test set: Average loss: 0.1336, Accuracy: 14429/15000 (96.19%)\n",
            "\n",
            "new accuracy parameters saved 0.9619333333333333\n",
            "MLP_2L  Epoch: 7 [0/45056 (0%)]\tLoss: 0.083065\n",
            "MLP_2L  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.092723\n",
            "MLP_2L  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.093530\n",
            "MLP_2L  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.093306\n",
            "\n",
            "Test set: Average loss: 0.1344, Accuracy: 14446/15000 (96.31%)\n",
            "\n",
            "new accuracy parameters saved 0.9630666666666666\n",
            "MLP_2L  Epoch: 8 [0/45056 (0%)]\tLoss: 0.130772\n",
            "MLP_2L  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.066917\n",
            "MLP_2L  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.066314\n",
            "MLP_2L  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.072329\n",
            "\n",
            "Test set: Average loss: 0.1112, Accuracy: 14521/15000 (96.81%)\n",
            "\n",
            "new accuracy parameters saved 0.9680666666666666\n",
            "MLP_2L  Epoch: 9 [0/45056 (0%)]\tLoss: 0.036363\n",
            "MLP_2L  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.035697\n",
            "MLP_2L  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.050682\n",
            "MLP_2L  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.158734\n",
            "\n",
            "Test set: Average loss: 0.1034, Accuracy: 14565/15000 (97.10%)\n",
            "\n",
            "new accuracy parameters saved 0.971\n",
            "MLP_2L  Epoch: 10 [0/45056 (0%)]\tLoss: 0.033647\n",
            "MLP_2L  Epoch: 10 [12800/45056 (28%)]\tLoss: 0.080418\n",
            "MLP_2L  Epoch: 10 [25600/45056 (57%)]\tLoss: 0.054271\n",
            "MLP_2L  Epoch: 10 [38400/45056 (85%)]\tLoss: 0.042952\n",
            "\n",
            "Test set: Average loss: 0.1042, Accuracy: 14540/15000 (96.93%)\n",
            "\n",
            "Training complete in 1m 3s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QG7PYeXHFnvb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Compairing Results"
      ]
    },
    {
      "metadata": {
        "id": "xgCPpl4zB7NK",
        "colab_type": "code",
        "outputId": "ae246411-931b-45dc-fa19-2a86de631c64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))\n",
        "\n",
        "for filename in os.listdir(savedir):\n",
        "    if filename.endswith('.pkl'):\n",
        "        with open(os.path.join(savedir, filename),'rb') as fin:\n",
        "            results = pickle.load(fin)\n",
        "            ax1.plot(results['loss'], label = filename[:-4])\n",
        "            ax1.set_ylabel('cross entropy')\n",
        "            ax1.set_xlabel('epochs')\n",
        "            \n",
        "            ax2.plot(results['accuracy'], label = filename[:-4])\n",
        "            ax2.set_ylabel('accuracy')\n",
        "            ax2.set_xlabel('epochs')\n",
        "            \n",
        "plt.legend()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f55b2010d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABJMAAAFzCAYAAACKD0ihAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4UP8KgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1440x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "msLlyF8JOEPX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Final comments"
      ]
    },
    {
      "metadata": {
        "id": "XhL3tsvzUwKU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "When looking at the aboves charts, we have to remember that it is plotted from the average of 10 runs of 10 epochs.\n",
        "So what we see is really the evolution of the models during training.  For exemple: CNN2 always starts with poor results for the first epoch.\n",
        "\n",
        "##### CNN1\n",
        "As this model slightly outperforms the MLP at every epoch, we can imagine it also doing better in the long run.  However, since they end up with the same value on the last epoch, we would need a longer run to confirm that.  With half as many parameters as the MLP, it would be tempting to declare this model simply superior to the MLP, but we should not jump to hasty conclusions as we should remember from part 1 that some models with lower number of parameters also performed well, so there might be a redundent amount of neurons that don't contribute much.\n",
        "\n",
        "Still it is really interesting that we can simply replace 400k parameters with 320 parameters and gain in performance.  It clearly shows that a single layers gets more out of the 32 feature maps then a fully connected layer.\n",
        "\n",
        "##### CNN2 \n",
        "With a second round of convolution the effect is even more pronounced and the model now clearly outperforms the MLP.  However this time we have have a convolution of 512 filters for the missing 400k parameters that make it equivalent to the MLP in number of paramters but much better in performance.\n",
        "\n",
        "##### CNN3\n",
        "Finnaly this model we conserve the same number of paramaters with another round of convolution thanks to more pooling. Compared to CNN2 , mostly only the architecture changed, being deeper and slightly thinner \n",
        "\n",
        "Not only does CNN3, CNN2 and the MLP have similar number of parameters, but they are also distributed and balanced in a similar way, where the parameters in the first fully connected hidden layer are replaced by the parameters of the convolution.\n",
        "\n",
        "It is unknown with this experiment how much convolution depth we can add and still gain perfomance, either plateau or loose performance.    \n",
        "\n",
        "What this experiment has shown though with all three models is how much convolution is better then a fully connected layer for image based classification.  MNIST being a relatively simple dataset, we suspect that with a more complex task the difference would be even more pronounced and we could really see the difference in capacity. \n"
      ]
    },
    {
      "metadata": {
        "id": "_vmMBCGyAnmP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "1vvmSRQRAAUl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}