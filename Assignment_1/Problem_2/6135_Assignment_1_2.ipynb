{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6135-Assignment_1.2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "qK9DEdy97GLG"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "BQKRlHPW64aP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CNN vs MLP on MNIST"
      ]
    },
    {
      "metadata": {
        "id": "qK9DEdy97GLG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Initialisising Colab drive"
      ]
    },
    {
      "metadata": {
        "id": "S1jpXpNS5Bt-",
        "colab_type": "code",
        "outputId": "3b02c9b2-8729-4de6-e7d5-4e5ab6bd930e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install PyDrive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting PyDrive\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e0/0e64788e5dd58ce2d6934549676243dc69d982f198524be9b99e9c2a4fd5/PyDrive-1.3.1.tar.gz (987kB)\n",
            "\u001b[K    100% |████████████████████████████████| 993kB 23.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.6.7)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.11.3)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.11.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.5)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.4)\n",
            "Building wheels for collected packages: PyDrive\n",
            "  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/fa/d2/9a/d3b6b506c2da98289e5d417215ce34b696db856643bad779f4\n",
            "Successfully built PyDrive\n",
            "Installing collected packages: PyDrive\n",
            "Successfully installed PyDrive-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q5rjqlf4JOeM",
        "colab_type": "code",
        "outputId": "341ae47f-b1b2-4f6b-dad5-efcd28282fd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "!pip install torch\n",
        "\n",
        "import torch\n",
        "print(f\"Your Pytorch version is {torch.__version__}.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Your Pytorch version is 1.0.0.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZHmr_oA8JTwc",
        "colab_type": "code",
        "outputId": "1ca4cd49-cd28-42f0-ea68-9b1b223a4fcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls \"/content/drive/My Drive/Colab Notebooks/6135/Assignment_1.2\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "6135-Assignment_1.2.ipynb  best_CNN_1  best_CNN_2  best_CNN_3  best_MLP_2L\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Nlxws8C97gfN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Librairies"
      ]
    },
    {
      "metadata": {
        "id": "uxPstMcgJXU-",
        "colab_type": "code",
        "outputId": "a883343e-75dd-45db-aaff-cddb558f0ec3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import csv\n",
        "import os\n",
        "import copy\n",
        "import random\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "\n",
        "print(\"Libraries ready\")\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "cuda = device.type == 'cuda'\n",
        "print(device, cuda)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Libraries ready\n",
            "cuda:0 True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D_8xxgxj7lCj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ]
    },
    {
      "metadata": {
        "id": "ksHFF1hIMHNf",
        "colab_type": "code",
        "outputId": "1642945f-2c81-4937-db56-a697cad12307",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "cell_type": "code",
      "source": [
        "train_data = datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor()\n",
        "                   ]))\n",
        "\n",
        "test_data = datasets.MNIST('../data', train=False,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor()\n",
        "                   ]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PlAAYLTHNhXw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "\n",
        "valid_size = 15000\n",
        "\n",
        "indices = list(range(len(train_data)))\n",
        "random.shuffle(indices)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size=batch_size,\n",
        "    sampler=SubsetRandomSampler(indices[valid_size:]),\n",
        "    num_workers=4,\n",
        "    pin_memory=cuda\n",
        ")\n",
        "\n",
        "valid_loader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size=batch_size,\n",
        "    sampler=SubsetRandomSampler(indices[:valid_size]),\n",
        "    num_workers=4,\n",
        "    pin_memory=cuda,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_data,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    pin_memory=cuda,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vCf-NhUs1Xjv",
        "colab_type": "code",
        "outputId": "990870d0-49b7-45fc-9313-696cd1361d12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 60000\n",
              "    Split: train\n",
              "    Root Location: ../data\n",
              "    Transforms (if any): Compose(\n",
              "                             ToTensor()\n",
              "                         )\n",
              "    Target Transforms (if any): None"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "O-eRhARCEzoI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def showRandSample(dataset, n_row=5, n_col=5):\n",
        "    nb_img = n_row * n_col\n",
        "    \n",
        "    sample_loader = DataLoader(\n",
        "      dataset,\n",
        "      batch_size=nb_img, \n",
        "      shuffle=True,\n",
        "      num_workers=1,\n",
        "      pin_memory=cuda)\n",
        "\n",
        "    images, labels = next(iter(sample_loader))\n",
        "    images = images.squeeze()\n",
        "    \n",
        "    fig = plt.figure(figsize=(n_row*2, n_col*2))\n",
        "\n",
        "    for i in range(1, nb_img +1):\n",
        "        img = images[i-1]    \n",
        "        ax = fig.add_subplot(n_row, n_col, i)\n",
        "        ax.set_title(str(labels[i-1].item()))\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img,cmap='gray_r')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gZxeGeWRF0ax",
        "colab_type": "code",
        "outputId": "f42498c2-31c1-4867-a0cc-3f38095b36e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "cell_type": "code",
      "source": [
        "showRandSample(train_data) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkwAAAJOCAYAAABFrFjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwIOCCQAAwIOCCQAAwONvShCSgLz7RPwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x720 with 25 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "WrfxvGUdvFM8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### MLP \n",
        "First as our compairason model we chose a strong MLP comming out of part 1, which a simple model with 2 hidden layers of 512 and 768 neurons with Relu as the activation function. "
      ]
    },
    {
      "metadata": {
        "id": "dY0ZZCaAvFb_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MLP_2L(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        h1 = 512\n",
        "        h2 = 768\n",
        "        \n",
        "        # Fully connected layer\n",
        "        self.fc0 = nn.Linear(28*28, h1)\n",
        "        self.fc1 = nn.Linear(h1, h2)\n",
        "        self.fc2 = nn.Linear(h2, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = F.relu(self.fc0(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2hAX3j1AMoT_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### CNN"
      ]
    },
    {
      "metadata": {
        "id": "A4wZwIHSMcTz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To expose the effects of convolution on an image classification task, we build a simple CNN with convolution layers replacing a part of the MLP, in this case the first hidden layer.\n",
        "\n",
        "Simply put : we replace the first hidden layer of the MLP with a convolution block.  \n",
        "So, where we had :\n",
        "\n",
        "[input]->[hidden 1]->[hidden 2]->[ouput]\n",
        "\n",
        "we now have :\n",
        "\n",
        "[input]->[convolution]->[hidden 2]->[ouput]\n",
        "\n",
        "\n",
        "We tested 3 models CNN1, CNN2, CNN3 with a convolution block of 1 round, 2 rounds and 3 rounds of convolution.\n",
        "\n",
        "Since our MLP's hidden layers have 512 and 768 neurons respectively, we have two choices in designing this experiment.   The first is to simply choose a similar total of number of parameters (~800k).  The other option is to have the same number of neurons in the corresponding hidden layer (768) and the also the same number coming out of the last pooling (512).\n",
        "\n",
        "So that we effectively only **replace** the first hidden layer, we chose the second option.  It also leads to a nicely balanced number of parameters in CNN3 compared to the MLP. (ie: the convotion part $\\approx$ h1 of the MLP).   This way, the design patterns stay consistant troughout all of the models.\n",
        "The downside is of course that CNN1 and CNN2 have a much lower number of parameters, so it may not be a fair comparaison\n",
        "\n",
        "\n",
        "Here are the designs of the convolution block for the 3 models which all result in a layer of 512 neurons :\n",
        "\n",
        "##### CNN1\n",
        "\n",
        "{[  $32$ , $3\\times3$ ]->[pooling $8$]}\n",
        "\n",
        "\n",
        "##### CNN2\n",
        "\n",
        "{[  $32$ , $3\\times3$ ]->[pooling $4$]} -> {[  $16$ , $3\\times3$ ]->[pooling $8$]}\n",
        "\n",
        "\n",
        "##### CNN3\n",
        "\n",
        "{[  $32$ , $3\\times3$ ]->[pooling $2$]} -> {[  $2$ , $3\\times3$ ]->[pooling $2$]} -> {[  $8$ , $3\\times3$ ]->[pooling $8$]}\n"
      ]
    },
    {
      "metadata": {
        "id": "azpQfcc9MmVw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CNN_1(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Conv block\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=3)\n",
        "        \n",
        "        # Fully connected layer\n",
        "        self.fc1 = nn.Linear(32*4*4, 768)\n",
        "        self.fc2 = nn.Linear(768, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 8)\n",
        "        \n",
        "        x = x.view(x.size(0), -1) # flatten\n",
        "        \n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x\n",
        "      \n",
        "class CNN_2(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Conv block\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=3)\n",
        "        self.conv2 = nn.Conv2d(32, 512, 3, padding=1)\n",
        "        \n",
        "        # Fully connected layer\n",
        "        self.fc1 = nn.Linear(512*1*1, 768)\n",
        "        self.fc2 = nn.Linear(768, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 4)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 8)\n",
        "        \n",
        "        x = x.view(x.size(0), -1) # flatten\n",
        "        \n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x     \n",
        "      \n",
        "class CNN_3(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Conv block\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=3)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 512, 3, padding=1)\n",
        "        \n",
        "        # Fully connected layer\n",
        "        self.fc1 = nn.Linear(512*1*1, 768)\n",
        "        self.fc2 = nn.Linear(768, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.max_pool2d(x, 8)\n",
        "        \n",
        "        x = x.view(x.size(0), -1) # flatten\n",
        "        \n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aR-NN1BELCMB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Model verification and number of parameters :"
      ]
    },
    {
      "metadata": {
        "id": "8PdkQ4lMKz9r",
        "colab_type": "code",
        "outputId": "69c1c7f5-fc98-4e3a-8c90-8deee0bf193f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1286
        }
      },
      "cell_type": "code",
      "source": [
        "#Number of parameters\n",
        "from torchsummary import summary\n",
        "\n",
        "model = CNN_1().to(device)\n",
        "\n",
        "print(\"CNN_1 : \",sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "summary(model, (1, 28, 28))\n",
        "\n",
        "\n",
        "model = CNN_2().to(device)\n",
        "\n",
        "print(\"\\nCNN_2 : \",sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "summary(model, (1, 28, 28))\n",
        "\n",
        "model = CNN_3().to(device)\n",
        "\n",
        "print(\"\\nCNN_3 : \",sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "summary(model, (1, 28, 28))\n",
        "\n",
        "model = MLP_2L().to(device)\n",
        "\n",
        "print(\"\\nMLP_2L : \",sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "summary(model, (1, 28, 28))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN_1 :  401994\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             320\n",
            "            Linear-2                  [-1, 768]         393,984\n",
            "            Linear-3                   [-1, 10]           7,690\n",
            "================================================================\n",
            "Total params: 401,994\n",
            "Trainable params: 401,994\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.26\n",
            "Params size (MB): 1.53\n",
            "Estimated Total Size (MB): 1.79\n",
            "----------------------------------------------------------------\n",
            "\n",
            "CNN_2 :  549962\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             320\n",
            "            Conv2d-2            [-1, 512, 8, 8]         147,968\n",
            "            Linear-3                  [-1, 768]         393,984\n",
            "            Linear-4                   [-1, 10]           7,690\n",
            "================================================================\n",
            "Total params: 549,962\n",
            "Trainable params: 549,962\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.51\n",
            "Params size (MB): 2.10\n",
            "Estimated Total Size (MB): 2.61\n",
            "----------------------------------------------------------------\n",
            "\n",
            "CNN_3 :  715914\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             320\n",
            "            Conv2d-2           [-1, 64, 16, 16]          18,496\n",
            "            Conv2d-3            [-1, 512, 8, 8]         295,424\n",
            "            Linear-4                  [-1, 768]         393,984\n",
            "            Linear-5                   [-1, 10]           7,690\n",
            "================================================================\n",
            "Total params: 715,914\n",
            "Trainable params: 715,914\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.63\n",
            "Params size (MB): 2.73\n",
            "Estimated Total Size (MB): 3.36\n",
            "----------------------------------------------------------------\n",
            "\n",
            "MLP_2L :  803594\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                  [-1, 512]         401,920\n",
            "            Linear-2                  [-1, 768]         393,984\n",
            "            Linear-3                   [-1, 10]           7,690\n",
            "================================================================\n",
            "Total params: 803,594\n",
            "Trainable params: 803,594\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.01\n",
            "Params size (MB): 3.07\n",
            "Estimated Total Size (MB): 3.08\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5IpMJsu4-MOj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "test_loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "#Results directory\n",
        "savedir = 'results'\n",
        "if not os.path.exists(savedir):\n",
        "    os.makedirs(savedir)\n",
        "\n",
        "\n",
        "## Saved states are ignored for this expirement\n",
        "\n",
        "#checkpoint = torch.load('/content/drive/My Drive/Colab Notebooks//6135/Assignment_1.2')\n",
        "#model.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6l6w7Zqp8nvP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(model,train_loader, optimizer, epoch, scheduler):\n",
        "    model.train()\n",
        "    \n",
        "    for batch_idx, (inputs, target) in enumerate(train_loader):\n",
        "        inputs = inputs.to(device, dtype=torch.float)\n",
        "        target = target.to(device, dtype=torch.long)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(inputs)\n",
        "        loss = loss_fn(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch_idx % 100 == 0:  \n",
        "            print(model.__class__.__name__,' Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch +1, batch_idx * len(inputs), len(train_loader) *len(inputs) ,\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    \n",
        "    scheduler.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KqB9Hfl99sgT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    \n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    test_size = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, target in valid_loader:\n",
        "            inputs = inputs.to(device,dtype=torch.float) \n",
        "            target = target.to(device, dtype=torch.long)\n",
        "            \n",
        "            output = model(inputs)\n",
        "            test_size += len(inputs)\n",
        "            test_loss += test_loss_fn(output, target).item() \n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= test_size\n",
        "    accuracy = correct / test_size\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, test_size,\n",
        "        100. * accuracy))\n",
        "    \n",
        "    return test_loss, accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tCxBDgWvdCzz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Running the experiment\n",
        "\n",
        "The experiment being non-determinist, depending on weight initialisation and the draw of the training set, \n",
        "we run the experiment multiple times and take the average results for each correspoding epochs, reseting all weights and sets for each run.\n",
        "\n",
        "Here : 10 runs of 10 epochs."
      ]
    },
    {
      "metadata": {
        "id": "yrKFuF8xfvEn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "runs = 10\n",
        "epochs = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "We1gNp5kIPES",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "def weight_reset(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "        m.reset_parameters()\n",
        "\n",
        "\n",
        "def MultipleRun(runs,epochs,model):\n",
        "  \n",
        "  model = model.to(device)\n",
        "  name =  model.__class__.__name__\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "  exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=2, gamma=1.0)\n",
        "\n",
        "\n",
        "  results = {'name':name, 'loss': [0]*epochs, 'accuracy':[0]*epochs}\n",
        "  savefile = os.path.join(savedir, results['name']+'.pkl' )\n",
        "\n",
        "  best_net = '/content/drive/My Drive/Colab Notebooks//6135/Assignment_1.2/best_' + name\n",
        "  \n",
        "  for run in range(runs):\n",
        "      print(\"\\n\",\"  --  Run : \", run+1)\n",
        "      since = time.time()\n",
        "      best_accuracy = 0\n",
        "      \n",
        "      #Reseting all weights for new run\n",
        "      model.apply(weight_reset)\n",
        "\n",
        "      for epoch in range(epochs):\n",
        "\n",
        "          train(model, train_loader, optimizer, epoch,exp_lr_scheduler)\n",
        "          loss, acc = test(model, valid_loader)\n",
        "\n",
        "          if acc > best_accuracy:\n",
        "            best_accuracy = acc\n",
        "                        \n",
        "            #Saving best model parameters\n",
        "            torch.save({\n",
        "                'epoch_based0': epoch,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'accuracy': acc,\n",
        "                'loss' : loss,\n",
        "                'optimizer' : optimizer.state_dict(),\n",
        "            }, best_net)\n",
        "\n",
        "            print('new accuracy parameters saved {}'.format(best_accuracy))\n",
        "\n",
        "          #Updating average results\n",
        "          results['loss'][epoch] += loss/runs\n",
        "          results['accuracy'][epoch] += acc/runs\n",
        "\n",
        "\n",
        "      time_elapsed = time.time() - since\n",
        "      print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "              time_elapsed // 60, time_elapsed % 60))\n",
        "    \n",
        "  with open(savefile, 'wb') as fout:\n",
        "    pickle.dump(results, fout)\n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C3-Ht73dKDJQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Running CNN1"
      ]
    },
    {
      "metadata": {
        "id": "gSdXrGRB_LW0",
        "colab_type": "code",
        "outputId": "45b29a84-7810-4330-d389-5eff24a9dbd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 13868
        }
      },
      "cell_type": "code",
      "source": [
        "MultipleRun(runs,epochs,CNN_1())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   --  Run :  1\n",
            "CNN_1  Epoch: 0 [0/45056 (0%)]\tLoss: 2.309378\n",
            "CNN_1  Epoch: 0 [12800/45056 (28%)]\tLoss: 0.923110\n",
            "CNN_1  Epoch: 0 [25600/45056 (57%)]\tLoss: 0.473931\n",
            "CNN_1  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.460117\n",
            "\n",
            "Test set: Average loss: 0.3346, Accuracy: 13380/15000 (89.20%)\n",
            "\n",
            "new accuracy parameters saved 0.892\n",
            "CNN_1  Epoch: 1 [0/45056 (0%)]\tLoss: 0.227225\n",
            "CNN_1  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.305211\n",
            "CNN_1  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.274623\n",
            "CNN_1  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.356657\n",
            "\n",
            "Test set: Average loss: 0.1964, Accuracy: 14049/15000 (93.66%)\n",
            "\n",
            "new accuracy parameters saved 0.9366\n",
            "CNN_1  Epoch: 2 [0/45056 (0%)]\tLoss: 0.167362\n",
            "CNN_1  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.162997\n",
            "CNN_1  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.179612\n",
            "CNN_1  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.168183\n",
            "\n",
            "Test set: Average loss: 0.1554, Accuracy: 14285/15000 (95.23%)\n",
            "\n",
            "new accuracy parameters saved 0.9523333333333334\n",
            "CNN_1  Epoch: 3 [0/45056 (0%)]\tLoss: 0.117867\n",
            "CNN_1  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.146401\n",
            "CNN_1  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.077150\n",
            "CNN_1  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.092644\n",
            "\n",
            "Test set: Average loss: 0.1200, Accuracy: 14425/15000 (96.17%)\n",
            "\n",
            "new accuracy parameters saved 0.9616666666666667\n",
            "CNN_1  Epoch: 4 [0/45056 (0%)]\tLoss: 0.053328\n",
            "CNN_1  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.088093\n",
            "CNN_1  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.138048\n",
            "CNN_1  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.140012\n",
            "\n",
            "Test set: Average loss: 0.1350, Accuracy: 14373/15000 (95.82%)\n",
            "\n",
            "CNN_1  Epoch: 5 [0/45056 (0%)]\tLoss: 0.113022\n",
            "CNN_1  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.149157\n",
            "CNN_1  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.158413\n",
            "CNN_1  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.069514\n",
            "\n",
            "Test set: Average loss: 0.1325, Accuracy: 14381/15000 (95.87%)\n",
            "\n",
            "CNN_1  Epoch: 6 [0/45056 (0%)]\tLoss: 0.131163\n",
            "CNN_1  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.088558\n",
            "CNN_1  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.103684\n",
            "CNN_1  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.092999\n",
            "\n",
            "Test set: Average loss: 0.0975, Accuracy: 14543/15000 (96.95%)\n",
            "\n",
            "new accuracy parameters saved 0.9695333333333334\n",
            "CNN_1  Epoch: 7 [0/45056 (0%)]\tLoss: 0.077164\n",
            "CNN_1  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.090431\n",
            "CNN_1  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.083366\n",
            "CNN_1  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.105719\n",
            "\n",
            "Test set: Average loss: 0.0887, Accuracy: 14578/15000 (97.19%)\n",
            "\n",
            "new accuracy parameters saved 0.9718666666666667\n",
            "CNN_1  Epoch: 8 [0/45056 (0%)]\tLoss: 0.097732\n",
            "CNN_1  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.076873\n",
            "CNN_1  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.060018\n",
            "CNN_1  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.128150\n",
            "\n",
            "Test set: Average loss: 0.0817, Accuracy: 14608/15000 (97.39%)\n",
            "\n",
            "new accuracy parameters saved 0.9738666666666667\n",
            "CNN_1  Epoch: 9 [0/45056 (0%)]\tLoss: 0.103135\n",
            "CNN_1  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.061012\n",
            "CNN_1  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.072294\n",
            "CNN_1  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.107927\n",
            "\n",
            "Test set: Average loss: 0.0928, Accuracy: 14547/15000 (96.98%)\n",
            "\n",
            "Training complete in 1m 8s\n",
            "\n",
            "   --  Run :  2\n",
            "CNN_1  Epoch: 0 [0/45056 (0%)]\tLoss: 2.304826\n",
            "CNN_1  Epoch: 0 [12800/45056 (28%)]\tLoss: 0.976075\n",
            "CNN_1  Epoch: 0 [25600/45056 (57%)]\tLoss: 0.497299\n",
            "CNN_1  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.367857\n",
            "\n",
            "Test set: Average loss: 0.3539, Accuracy: 13408/15000 (89.39%)\n",
            "\n",
            "new accuracy parameters saved 0.8938666666666667\n",
            "CNN_1  Epoch: 1 [0/45056 (0%)]\tLoss: 0.298152\n",
            "CNN_1  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.317216\n",
            "CNN_1  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.225170\n",
            "CNN_1  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.157172\n",
            "\n",
            "Test set: Average loss: 0.2219, Accuracy: 13916/15000 (92.77%)\n",
            "\n",
            "new accuracy parameters saved 0.9277333333333333\n",
            "CNN_1  Epoch: 2 [0/45056 (0%)]\tLoss: 0.186583\n",
            "CNN_1  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.207668\n",
            "CNN_1  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.206922\n",
            "CNN_1  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.158665\n",
            "\n",
            "Test set: Average loss: 0.2000, Accuracy: 14041/15000 (93.61%)\n",
            "\n",
            "new accuracy parameters saved 0.9360666666666667\n",
            "CNN_1  Epoch: 3 [0/45056 (0%)]\tLoss: 0.213268\n",
            "CNN_1  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.146382\n",
            "CNN_1  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.108879\n",
            "CNN_1  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.163384\n",
            "\n",
            "Test set: Average loss: 0.1397, Accuracy: 14342/15000 (95.61%)\n",
            "\n",
            "new accuracy parameters saved 0.9561333333333333\n",
            "CNN_1  Epoch: 4 [0/45056 (0%)]\tLoss: 0.108506\n",
            "CNN_1  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.159042\n",
            "CNN_1  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.227200\n",
            "CNN_1  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.090427\n",
            "\n",
            "Test set: Average loss: 0.1148, Accuracy: 14479/15000 (96.53%)\n",
            "\n",
            "new accuracy parameters saved 0.9652666666666667\n",
            "CNN_1  Epoch: 5 [0/45056 (0%)]\tLoss: 0.106540\n",
            "CNN_1  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.116706\n",
            "CNN_1  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.037223\n",
            "CNN_1  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.113441\n",
            "\n",
            "Test set: Average loss: 0.1062, Accuracy: 14506/15000 (96.71%)\n",
            "\n",
            "new accuracy parameters saved 0.9670666666666666\n",
            "CNN_1  Epoch: 6 [0/45056 (0%)]\tLoss: 0.114219\n",
            "CNN_1  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.155727\n",
            "CNN_1  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.053477\n",
            "CNN_1  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.077992\n",
            "\n",
            "Test set: Average loss: 0.1005, Accuracy: 14536/15000 (96.91%)\n",
            "\n",
            "new accuracy parameters saved 0.9690666666666666\n",
            "CNN_1  Epoch: 7 [0/45056 (0%)]\tLoss: 0.080900\n",
            "CNN_1  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.166479\n",
            "CNN_1  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.093693\n",
            "CNN_1  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.089245\n",
            "\n",
            "Test set: Average loss: 0.0905, Accuracy: 14588/15000 (97.25%)\n",
            "\n",
            "new accuracy parameters saved 0.9725333333333334\n",
            "CNN_1  Epoch: 8 [0/45056 (0%)]\tLoss: 0.055682\n",
            "CNN_1  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.086229\n",
            "CNN_1  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.048058\n",
            "CNN_1  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.132610\n",
            "\n",
            "Test set: Average loss: 0.0824, Accuracy: 14618/15000 (97.45%)\n",
            "\n",
            "new accuracy parameters saved 0.9745333333333334\n",
            "CNN_1  Epoch: 9 [0/45056 (0%)]\tLoss: 0.074604\n",
            "CNN_1  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.031637\n",
            "CNN_1  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.100471\n",
            "CNN_1  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.172817\n",
            "\n",
            "Test set: Average loss: 0.0796, Accuracy: 14626/15000 (97.51%)\n",
            "\n",
            "new accuracy parameters saved 0.9750666666666666\n",
            "Training complete in 1m 8s\n",
            "\n",
            "   --  Run :  3\n",
            "CNN_1  Epoch: 0 [0/45056 (0%)]\tLoss: 2.294506\n",
            "CNN_1  Epoch: 0 [12800/45056 (28%)]\tLoss: 1.010446\n",
            "CNN_1  Epoch: 0 [25600/45056 (57%)]\tLoss: 0.495438\n",
            "CNN_1  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.392071\n",
            "\n",
            "Test set: Average loss: 0.3254, Accuracy: 13554/15000 (90.36%)\n",
            "\n",
            "new accuracy parameters saved 0.9036\n",
            "CNN_1  Epoch: 1 [0/45056 (0%)]\tLoss: 0.487321\n",
            "CNN_1  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.343085\n",
            "CNN_1  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.215253\n",
            "CNN_1  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.122701\n",
            "\n",
            "Test set: Average loss: 0.1711, Accuracy: 14232/15000 (94.88%)\n",
            "\n",
            "new accuracy parameters saved 0.9488\n",
            "CNN_1  Epoch: 2 [0/45056 (0%)]\tLoss: 0.206241\n",
            "CNN_1  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.214114\n",
            "CNN_1  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.195182\n",
            "CNN_1  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.102773\n",
            "\n",
            "Test set: Average loss: 0.1453, Accuracy: 14342/15000 (95.61%)\n",
            "\n",
            "new accuracy parameters saved 0.9561333333333333\n",
            "CNN_1  Epoch: 3 [0/45056 (0%)]\tLoss: 0.161760\n",
            "CNN_1  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.124599\n",
            "CNN_1  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.092635\n",
            "CNN_1  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.077465\n",
            "\n",
            "Test set: Average loss: 0.1213, Accuracy: 14425/15000 (96.17%)\n",
            "\n",
            "new accuracy parameters saved 0.9616666666666667\n",
            "CNN_1  Epoch: 4 [0/45056 (0%)]\tLoss: 0.055687\n",
            "CNN_1  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.166485\n",
            "CNN_1  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.222994\n",
            "CNN_1  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.079605\n",
            "\n",
            "Test set: Average loss: 0.1195, Accuracy: 14450/15000 (96.33%)\n",
            "\n",
            "new accuracy parameters saved 0.9633333333333334\n",
            "CNN_1  Epoch: 5 [0/45056 (0%)]\tLoss: 0.116675\n",
            "CNN_1  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.131364\n",
            "CNN_1  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.092110\n",
            "CNN_1  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.070851\n",
            "\n",
            "Test set: Average loss: 0.0977, Accuracy: 14537/15000 (96.91%)\n",
            "\n",
            "new accuracy parameters saved 0.9691333333333333\n",
            "CNN_1  Epoch: 6 [0/45056 (0%)]\tLoss: 0.070546\n",
            "CNN_1  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.080857\n",
            "CNN_1  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.052887\n",
            "CNN_1  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.100584\n",
            "\n",
            "Test set: Average loss: 0.1101, Accuracy: 14471/15000 (96.47%)\n",
            "\n",
            "CNN_1  Epoch: 7 [0/45056 (0%)]\tLoss: 0.179970\n",
            "CNN_1  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.069474\n",
            "CNN_1  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.153961\n",
            "CNN_1  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.117609\n",
            "\n",
            "Test set: Average loss: 0.0967, Accuracy: 14530/15000 (96.87%)\n",
            "\n",
            "CNN_1  Epoch: 8 [0/45056 (0%)]\tLoss: 0.103254\n",
            "CNN_1  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.064698\n",
            "CNN_1  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.044846\n",
            "CNN_1  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.064668\n",
            "\n",
            "Test set: Average loss: 0.1023, Accuracy: 14513/15000 (96.75%)\n",
            "\n",
            "CNN_1  Epoch: 9 [0/45056 (0%)]\tLoss: 0.166462\n",
            "CNN_1  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.062932\n",
            "CNN_1  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.068549\n",
            "CNN_1  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.103199\n",
            "\n",
            "Test set: Average loss: 0.0824, Accuracy: 14621/15000 (97.47%)\n",
            "\n",
            "new accuracy parameters saved 0.9747333333333333\n",
            "Training complete in 1m 8s\n",
            "\n",
            "   --  Run :  4\n",
            "CNN_1  Epoch: 0 [0/45056 (0%)]\tLoss: 2.303537\n",
            "CNN_1  Epoch: 0 [12800/45056 (28%)]\tLoss: 1.150596\n",
            "CNN_1  Epoch: 0 [25600/45056 (57%)]\tLoss: 0.776942\n",
            "CNN_1  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.610367\n",
            "\n",
            "Test set: Average loss: 0.5151, Accuracy: 12384/15000 (82.56%)\n",
            "\n",
            "new accuracy parameters saved 0.8256\n",
            "CNN_1  Epoch: 1 [0/45056 (0%)]\tLoss: 0.555234\n",
            "CNN_1  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.237845\n",
            "CNN_1  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.292085\n",
            "CNN_1  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.099156\n",
            "\n",
            "Test set: Average loss: 0.2420, Accuracy: 13812/15000 (92.08%)\n",
            "\n",
            "new accuracy parameters saved 0.9208\n",
            "CNN_1  Epoch: 2 [0/45056 (0%)]\tLoss: 0.265807\n",
            "CNN_1  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.160001\n",
            "CNN_1  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.174758\n",
            "CNN_1  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.127715\n",
            "\n",
            "Test set: Average loss: 0.1538, Accuracy: 14285/15000 (95.23%)\n",
            "\n",
            "new accuracy parameters saved 0.9523333333333334\n",
            "CNN_1  Epoch: 3 [0/45056 (0%)]\tLoss: 0.144866\n",
            "CNN_1  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.148022\n",
            "CNN_1  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.108891\n",
            "CNN_1  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.128957\n",
            "\n",
            "Test set: Average loss: 0.1375, Accuracy: 14344/15000 (95.63%)\n",
            "\n",
            "new accuracy parameters saved 0.9562666666666667\n",
            "CNN_1  Epoch: 4 [0/45056 (0%)]\tLoss: 0.125918\n",
            "CNN_1  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.059539\n",
            "CNN_1  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.059135\n",
            "CNN_1  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.108563\n",
            "\n",
            "Test set: Average loss: 0.1230, Accuracy: 14420/15000 (96.13%)\n",
            "\n",
            "new accuracy parameters saved 0.9613333333333334\n",
            "CNN_1  Epoch: 5 [0/45056 (0%)]\tLoss: 0.111241\n",
            "CNN_1  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.058426\n",
            "CNN_1  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.097233\n",
            "CNN_1  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.045076\n",
            "\n",
            "Test set: Average loss: 0.1038, Accuracy: 14512/15000 (96.75%)\n",
            "\n",
            "new accuracy parameters saved 0.9674666666666667\n",
            "CNN_1  Epoch: 6 [0/45056 (0%)]\tLoss: 0.139097\n",
            "CNN_1  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.038381\n",
            "CNN_1  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.132478\n",
            "CNN_1  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.125797\n",
            "\n",
            "Test set: Average loss: 0.0945, Accuracy: 14563/15000 (97.09%)\n",
            "\n",
            "new accuracy parameters saved 0.9708666666666667\n",
            "CNN_1  Epoch: 7 [0/45056 (0%)]\tLoss: 0.156544\n",
            "CNN_1  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.057847\n",
            "CNN_1  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.037332\n",
            "CNN_1  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.061281\n",
            "\n",
            "Test set: Average loss: 0.0915, Accuracy: 14565/15000 (97.10%)\n",
            "\n",
            "new accuracy parameters saved 0.971\n",
            "CNN_1  Epoch: 8 [0/45056 (0%)]\tLoss: 0.116438\n",
            "CNN_1  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.097539\n",
            "CNN_1  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.041716\n",
            "CNN_1  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.027439\n",
            "\n",
            "Test set: Average loss: 0.0835, Accuracy: 14596/15000 (97.31%)\n",
            "\n",
            "new accuracy parameters saved 0.9730666666666666\n",
            "CNN_1  Epoch: 9 [0/45056 (0%)]\tLoss: 0.079229\n",
            "CNN_1  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.084373\n",
            "CNN_1  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.096909\n",
            "CNN_1  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.057055\n",
            "\n",
            "Test set: Average loss: 0.0859, Accuracy: 14603/15000 (97.35%)\n",
            "\n",
            "new accuracy parameters saved 0.9735333333333334\n",
            "Training complete in 1m 8s\n",
            "\n",
            "   --  Run :  5\n",
            "CNN_1  Epoch: 0 [0/45056 (0%)]\tLoss: 2.305897\n",
            "CNN_1  Epoch: 0 [12800/45056 (28%)]\tLoss: 1.006659\n",
            "CNN_1  Epoch: 0 [25600/45056 (57%)]\tLoss: 0.701872\n",
            "CNN_1  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.507853\n",
            "\n",
            "Test set: Average loss: 0.3273, Accuracy: 13499/15000 (89.99%)\n",
            "\n",
            "new accuracy parameters saved 0.8999333333333334\n",
            "CNN_1  Epoch: 1 [0/45056 (0%)]\tLoss: 0.283919\n",
            "CNN_1  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.212602\n",
            "CNN_1  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.220919\n",
            "CNN_1  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.188859\n",
            "\n",
            "Test set: Average loss: 0.1845, Accuracy: 14167/15000 (94.45%)\n",
            "\n",
            "new accuracy parameters saved 0.9444666666666667\n",
            "CNN_1  Epoch: 2 [0/45056 (0%)]\tLoss: 0.235602\n",
            "CNN_1  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.202070\n",
            "CNN_1  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.285076\n",
            "CNN_1  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.186086\n",
            "\n",
            "Test set: Average loss: 0.2136, Accuracy: 13968/15000 (93.12%)\n",
            "\n",
            "CNN_1  Epoch: 3 [0/45056 (0%)]\tLoss: 0.183810\n",
            "CNN_1  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.126986\n",
            "CNN_1  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.100237\n",
            "CNN_1  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.198663\n",
            "\n",
            "Test set: Average loss: 0.1342, Accuracy: 14368/15000 (95.79%)\n",
            "\n",
            "new accuracy parameters saved 0.9578666666666666\n",
            "CNN_1  Epoch: 4 [0/45056 (0%)]\tLoss: 0.117287\n",
            "CNN_1  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.132648\n",
            "CNN_1  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.147993\n",
            "CNN_1  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.062982\n",
            "\n",
            "Test set: Average loss: 0.1342, Accuracy: 14362/15000 (95.75%)\n",
            "\n",
            "CNN_1  Epoch: 5 [0/45056 (0%)]\tLoss: 0.236186\n",
            "CNN_1  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.059206\n",
            "CNN_1  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.160618\n",
            "CNN_1  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.055717\n",
            "\n",
            "Test set: Average loss: 0.1138, Accuracy: 14468/15000 (96.45%)\n",
            "\n",
            "new accuracy parameters saved 0.9645333333333334\n",
            "CNN_1  Epoch: 6 [0/45056 (0%)]\tLoss: 0.123832\n",
            "CNN_1  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.074412\n",
            "CNN_1  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.067673\n",
            "CNN_1  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.221825\n",
            "\n",
            "Test set: Average loss: 0.1120, Accuracy: 14430/15000 (96.20%)\n",
            "\n",
            "CNN_1  Epoch: 7 [0/45056 (0%)]\tLoss: 0.114195\n",
            "CNN_1  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.162933\n",
            "CNN_1  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.060306\n",
            "CNN_1  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.088737\n",
            "\n",
            "Test set: Average loss: 0.0908, Accuracy: 14570/15000 (97.13%)\n",
            "\n",
            "new accuracy parameters saved 0.9713333333333334\n",
            "CNN_1  Epoch: 8 [0/45056 (0%)]\tLoss: 0.115240\n",
            "CNN_1  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.070993\n",
            "CNN_1  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.093273\n",
            "CNN_1  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.037830\n",
            "\n",
            "Test set: Average loss: 0.0846, Accuracy: 14621/15000 (97.47%)\n",
            "\n",
            "new accuracy parameters saved 0.9747333333333333\n",
            "CNN_1  Epoch: 9 [0/45056 (0%)]\tLoss: 0.037053\n",
            "CNN_1  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.128562\n",
            "CNN_1  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.039103\n",
            "CNN_1  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.076490\n",
            "\n",
            "Test set: Average loss: 0.0961, Accuracy: 14530/15000 (96.87%)\n",
            "\n",
            "Training complete in 1m 8s\n",
            "\n",
            "   --  Run :  6\n",
            "CNN_1  Epoch: 0 [0/45056 (0%)]\tLoss: 2.320510\n",
            "CNN_1  Epoch: 0 [12800/45056 (28%)]\tLoss: 0.957662\n",
            "CNN_1  Epoch: 0 [25600/45056 (57%)]\tLoss: 0.532306\n",
            "CNN_1  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.441024\n",
            "\n",
            "Test set: Average loss: 0.3197, Accuracy: 13579/15000 (90.53%)\n",
            "\n",
            "new accuracy parameters saved 0.9052666666666667\n",
            "CNN_1  Epoch: 1 [0/45056 (0%)]\tLoss: 0.331184\n",
            "CNN_1  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.254467\n",
            "CNN_1  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.289898\n",
            "CNN_1  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.182325\n",
            "\n",
            "Test set: Average loss: 0.2128, Accuracy: 13998/15000 (93.32%)\n",
            "\n",
            "new accuracy parameters saved 0.9332\n",
            "CNN_1  Epoch: 2 [0/45056 (0%)]\tLoss: 0.117787\n",
            "CNN_1  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.183784\n",
            "CNN_1  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.227581\n",
            "CNN_1  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.122754\n",
            "\n",
            "Test set: Average loss: 0.1676, Accuracy: 14180/15000 (94.53%)\n",
            "\n",
            "new accuracy parameters saved 0.9453333333333334\n",
            "CNN_1  Epoch: 3 [0/45056 (0%)]\tLoss: 0.093678\n",
            "CNN_1  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.114692\n",
            "CNN_1  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.067832\n",
            "CNN_1  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.209911\n",
            "\n",
            "Test set: Average loss: 0.1332, Accuracy: 14394/15000 (95.96%)\n",
            "\n",
            "new accuracy parameters saved 0.9596\n",
            "CNN_1  Epoch: 4 [0/45056 (0%)]\tLoss: 0.240800\n",
            "CNN_1  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.160796\n",
            "CNN_1  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.115278\n",
            "CNN_1  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.062394\n",
            "\n",
            "Test set: Average loss: 0.1475, Accuracy: 14296/15000 (95.31%)\n",
            "\n",
            "CNN_1  Epoch: 5 [0/45056 (0%)]\tLoss: 0.207410\n",
            "CNN_1  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.086203\n",
            "CNN_1  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.084853\n",
            "CNN_1  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.144441\n",
            "\n",
            "Test set: Average loss: 0.1165, Accuracy: 14445/15000 (96.30%)\n",
            "\n",
            "new accuracy parameters saved 0.963\n",
            "CNN_1  Epoch: 6 [0/45056 (0%)]\tLoss: 0.121838\n",
            "CNN_1  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.032221\n",
            "CNN_1  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.115515\n",
            "CNN_1  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.044643\n",
            "\n",
            "Test set: Average loss: 0.1065, Accuracy: 14501/15000 (96.67%)\n",
            "\n",
            "new accuracy parameters saved 0.9667333333333333\n",
            "CNN_1  Epoch: 7 [0/45056 (0%)]\tLoss: 0.046608\n",
            "CNN_1  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.066317\n",
            "CNN_1  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.116896\n",
            "CNN_1  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.163553\n",
            "\n",
            "Test set: Average loss: 0.1103, Accuracy: 14486/15000 (96.57%)\n",
            "\n",
            "CNN_1  Epoch: 8 [0/45056 (0%)]\tLoss: 0.173115\n",
            "CNN_1  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.061519\n",
            "CNN_1  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.063941\n",
            "CNN_1  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.027255\n",
            "\n",
            "Test set: Average loss: 0.0942, Accuracy: 14545/15000 (96.97%)\n",
            "\n",
            "new accuracy parameters saved 0.9696666666666667\n",
            "CNN_1  Epoch: 9 [0/45056 (0%)]\tLoss: 0.037595\n",
            "CNN_1  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.116388\n",
            "CNN_1  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.106719\n",
            "CNN_1  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.061512\n",
            "\n",
            "Test set: Average loss: 0.1028, Accuracy: 14527/15000 (96.85%)\n",
            "\n",
            "Training complete in 1m 8s\n",
            "\n",
            "   --  Run :  7\n",
            "CNN_1  Epoch: 0 [0/45056 (0%)]\tLoss: 2.320000\n",
            "CNN_1  Epoch: 0 [12800/45056 (28%)]\tLoss: 1.251443\n",
            "CNN_1  Epoch: 0 [25600/45056 (57%)]\tLoss: 0.632319\n",
            "CNN_1  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.326244\n",
            "\n",
            "Test set: Average loss: 0.4459, Accuracy: 12762/15000 (85.08%)\n",
            "\n",
            "new accuracy parameters saved 0.8508\n",
            "CNN_1  Epoch: 1 [0/45056 (0%)]\tLoss: 0.436556\n",
            "CNN_1  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.234193\n",
            "CNN_1  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.271535\n",
            "CNN_1  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.149507\n",
            "\n",
            "Test set: Average loss: 0.1843, Accuracy: 14163/15000 (94.42%)\n",
            "\n",
            "new accuracy parameters saved 0.9442\n",
            "CNN_1  Epoch: 2 [0/45056 (0%)]\tLoss: 0.132001\n",
            "CNN_1  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.158795\n",
            "CNN_1  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.238198\n",
            "CNN_1  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.235332\n",
            "\n",
            "Test set: Average loss: 0.3627, Accuracy: 13092/15000 (87.28%)\n",
            "\n",
            "CNN_1  Epoch: 3 [0/45056 (0%)]\tLoss: 0.310067\n",
            "CNN_1  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.136707\n",
            "CNN_1  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.095004\n",
            "CNN_1  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.116337\n",
            "\n",
            "Test set: Average loss: 0.1698, Accuracy: 14152/15000 (94.35%)\n",
            "\n",
            "CNN_1  Epoch: 4 [0/45056 (0%)]\tLoss: 0.164601\n",
            "CNN_1  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.098891\n",
            "CNN_1  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.198559\n",
            "CNN_1  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.097339\n",
            "\n",
            "Test set: Average loss: 0.1127, Accuracy: 14460/15000 (96.40%)\n",
            "\n",
            "new accuracy parameters saved 0.964\n",
            "CNN_1  Epoch: 5 [0/45056 (0%)]\tLoss: 0.072406\n",
            "CNN_1  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.124709\n",
            "CNN_1  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.109830\n",
            "CNN_1  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.110909\n",
            "\n",
            "Test set: Average loss: 0.1066, Accuracy: 14489/15000 (96.59%)\n",
            "\n",
            "new accuracy parameters saved 0.9659333333333333\n",
            "CNN_1  Epoch: 6 [0/45056 (0%)]\tLoss: 0.080168\n",
            "CNN_1  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.052563\n",
            "CNN_1  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.158271\n",
            "CNN_1  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.067395\n",
            "\n",
            "Test set: Average loss: 0.1037, Accuracy: 14515/15000 (96.77%)\n",
            "\n",
            "new accuracy parameters saved 0.9676666666666667\n",
            "CNN_1  Epoch: 7 [0/45056 (0%)]\tLoss: 0.065422\n",
            "CNN_1  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.119153\n",
            "CNN_1  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.077608\n",
            "CNN_1  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.077975\n",
            "\n",
            "Test set: Average loss: 0.0897, Accuracy: 14586/15000 (97.24%)\n",
            "\n",
            "new accuracy parameters saved 0.9724\n",
            "CNN_1  Epoch: 8 [0/45056 (0%)]\tLoss: 0.106547\n",
            "CNN_1  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.039109\n",
            "CNN_1  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.106194\n",
            "CNN_1  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.117197\n",
            "\n",
            "Test set: Average loss: 0.0931, Accuracy: 14550/15000 (97.00%)\n",
            "\n",
            "CNN_1  Epoch: 9 [0/45056 (0%)]\tLoss: 0.060492\n",
            "CNN_1  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.041458\n",
            "CNN_1  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.055986\n",
            "CNN_1  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.107768\n",
            "\n",
            "Test set: Average loss: 0.1000, Accuracy: 14546/15000 (96.97%)\n",
            "\n",
            "Training complete in 1m 8s\n",
            "\n",
            "   --  Run :  8\n",
            "CNN_1  Epoch: 0 [0/45056 (0%)]\tLoss: 2.304100\n",
            "CNN_1  Epoch: 0 [12800/45056 (28%)]\tLoss: 1.104725\n",
            "CNN_1  Epoch: 0 [25600/45056 (57%)]\tLoss: 0.819919\n",
            "CNN_1  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.350017\n",
            "\n",
            "Test set: Average loss: 0.4775, Accuracy: 12458/15000 (83.05%)\n",
            "\n",
            "new accuracy parameters saved 0.8305333333333333\n",
            "CNN_1  Epoch: 1 [0/45056 (0%)]\tLoss: 0.453816\n",
            "CNN_1  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.281548\n",
            "CNN_1  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.161403\n",
            "CNN_1  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.186063\n",
            "\n",
            "Test set: Average loss: 0.1889, Accuracy: 14146/15000 (94.31%)\n",
            "\n",
            "new accuracy parameters saved 0.9430666666666667\n",
            "CNN_1  Epoch: 2 [0/45056 (0%)]\tLoss: 0.284624\n",
            "CNN_1  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.184211\n",
            "CNN_1  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.186215\n",
            "CNN_1  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.158255\n",
            "\n",
            "Test set: Average loss: 0.1538, Accuracy: 14271/15000 (95.14%)\n",
            "\n",
            "new accuracy parameters saved 0.9514\n",
            "CNN_1  Epoch: 3 [0/45056 (0%)]\tLoss: 0.204189\n",
            "CNN_1  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.245631\n",
            "CNN_1  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.182663\n",
            "CNN_1  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.114401\n",
            "\n",
            "Test set: Average loss: 0.1241, Accuracy: 14406/15000 (96.04%)\n",
            "\n",
            "new accuracy parameters saved 0.9604\n",
            "CNN_1  Epoch: 4 [0/45056 (0%)]\tLoss: 0.114496\n",
            "CNN_1  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.158708\n",
            "CNN_1  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.132998\n",
            "CNN_1  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.171187\n",
            "\n",
            "Test set: Average loss: 0.1158, Accuracy: 14452/15000 (96.35%)\n",
            "\n",
            "new accuracy parameters saved 0.9634666666666667\n",
            "CNN_1  Epoch: 5 [0/45056 (0%)]\tLoss: 0.058247\n",
            "CNN_1  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.094302\n",
            "CNN_1  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.048657\n",
            "CNN_1  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.064759\n",
            "\n",
            "Test set: Average loss: 0.1520, Accuracy: 14264/15000 (95.09%)\n",
            "\n",
            "CNN_1  Epoch: 6 [0/45056 (0%)]\tLoss: 0.102225\n",
            "CNN_1  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.087169\n",
            "CNN_1  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.173913\n",
            "CNN_1  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.058098\n",
            "\n",
            "Test set: Average loss: 0.1036, Accuracy: 14484/15000 (96.56%)\n",
            "\n",
            "new accuracy parameters saved 0.9656\n",
            "CNN_1  Epoch: 7 [0/45056 (0%)]\tLoss: 0.135259\n",
            "CNN_1  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.069037\n",
            "CNN_1  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.171282\n",
            "CNN_1  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.043578\n",
            "\n",
            "Test set: Average loss: 0.0955, Accuracy: 14518/15000 (96.79%)\n",
            "\n",
            "new accuracy parameters saved 0.9678666666666667\n",
            "CNN_1  Epoch: 8 [0/45056 (0%)]\tLoss: 0.200132\n",
            "CNN_1  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.035379\n",
            "CNN_1  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.099971\n",
            "CNN_1  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.084042\n",
            "\n",
            "Test set: Average loss: 0.0825, Accuracy: 14601/15000 (97.34%)\n",
            "\n",
            "new accuracy parameters saved 0.9734\n",
            "CNN_1  Epoch: 9 [0/45056 (0%)]\tLoss: 0.036446\n",
            "CNN_1  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.042783\n",
            "CNN_1  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.139507\n",
            "CNN_1  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.084542\n",
            "\n",
            "Test set: Average loss: 0.0761, Accuracy: 14631/15000 (97.54%)\n",
            "\n",
            "new accuracy parameters saved 0.9754\n",
            "Training complete in 1m 8s\n",
            "\n",
            "   --  Run :  9\n",
            "CNN_1  Epoch: 0 [0/45056 (0%)]\tLoss: 2.303706\n",
            "CNN_1  Epoch: 0 [12800/45056 (28%)]\tLoss: 1.168419\n",
            "CNN_1  Epoch: 0 [25600/45056 (57%)]\tLoss: 0.589943\n",
            "CNN_1  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.235599\n",
            "\n",
            "Test set: Average loss: 0.3058, Accuracy: 13635/15000 (90.90%)\n",
            "\n",
            "new accuracy parameters saved 0.909\n",
            "CNN_1  Epoch: 1 [0/45056 (0%)]\tLoss: 0.324626\n",
            "CNN_1  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.208306\n",
            "CNN_1  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.243874\n",
            "CNN_1  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.201611\n",
            "\n",
            "Test set: Average loss: 0.1730, Accuracy: 14224/15000 (94.83%)\n",
            "\n",
            "new accuracy parameters saved 0.9482666666666667\n",
            "CNN_1  Epoch: 2 [0/45056 (0%)]\tLoss: 0.168991\n",
            "CNN_1  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.251508\n",
            "CNN_1  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.149981\n",
            "CNN_1  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.123612\n",
            "\n",
            "Test set: Average loss: 0.1434, Accuracy: 14346/15000 (95.64%)\n",
            "\n",
            "new accuracy parameters saved 0.9564\n",
            "CNN_1  Epoch: 3 [0/45056 (0%)]\tLoss: 0.125669\n",
            "CNN_1  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.083448\n",
            "CNN_1  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.183416\n",
            "CNN_1  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.104656\n",
            "\n",
            "Test set: Average loss: 0.1159, Accuracy: 14455/15000 (96.37%)\n",
            "\n",
            "new accuracy parameters saved 0.9636666666666667\n",
            "CNN_1  Epoch: 4 [0/45056 (0%)]\tLoss: 0.111666\n",
            "CNN_1  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.074053\n",
            "CNN_1  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.118428\n",
            "CNN_1  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.129810\n",
            "\n",
            "Test set: Average loss: 0.1148, Accuracy: 14470/15000 (96.47%)\n",
            "\n",
            "new accuracy parameters saved 0.9646666666666667\n",
            "CNN_1  Epoch: 5 [0/45056 (0%)]\tLoss: 0.066196\n",
            "CNN_1  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.109283\n",
            "CNN_1  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.102723\n",
            "CNN_1  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.131054\n",
            "\n",
            "Test set: Average loss: 0.1164, Accuracy: 14456/15000 (96.37%)\n",
            "\n",
            "CNN_1  Epoch: 6 [0/45056 (0%)]\tLoss: 0.062182\n",
            "CNN_1  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.060428\n",
            "CNN_1  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.091678\n",
            "CNN_1  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.086539\n",
            "\n",
            "Test set: Average loss: 0.0875, Accuracy: 14579/15000 (97.19%)\n",
            "\n",
            "new accuracy parameters saved 0.9719333333333333\n",
            "CNN_1  Epoch: 7 [0/45056 (0%)]\tLoss: 0.108911\n",
            "CNN_1  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.114046\n",
            "CNN_1  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.087022\n",
            "CNN_1  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.079004\n",
            "\n",
            "Test set: Average loss: 0.0838, Accuracy: 14622/15000 (97.48%)\n",
            "\n",
            "new accuracy parameters saved 0.9748\n",
            "CNN_1  Epoch: 8 [0/45056 (0%)]\tLoss: 0.077132\n",
            "CNN_1  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.111553\n",
            "CNN_1  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.029138\n",
            "CNN_1  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.120472\n",
            "\n",
            "Test set: Average loss: 0.0796, Accuracy: 14616/15000 (97.44%)\n",
            "\n",
            "CNN_1  Epoch: 9 [0/45056 (0%)]\tLoss: 0.041657\n",
            "CNN_1  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.169966\n",
            "CNN_1  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.079418\n",
            "CNN_1  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.091009\n",
            "\n",
            "Test set: Average loss: 0.0969, Accuracy: 14533/15000 (96.89%)\n",
            "\n",
            "Training complete in 1m 8s\n",
            "\n",
            "   --  Run :  10\n",
            "CNN_1  Epoch: 0 [0/45056 (0%)]\tLoss: 2.297070\n",
            "CNN_1  Epoch: 0 [12800/45056 (28%)]\tLoss: 0.988986\n",
            "CNN_1  Epoch: 0 [25600/45056 (57%)]\tLoss: 0.564495\n",
            "CNN_1  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.285783\n",
            "\n",
            "Test set: Average loss: 0.3045, Accuracy: 13654/15000 (91.03%)\n",
            "\n",
            "new accuracy parameters saved 0.9102666666666667\n",
            "CNN_1  Epoch: 1 [0/45056 (0%)]\tLoss: 0.204247\n",
            "CNN_1  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.200221\n",
            "CNN_1  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.305929\n",
            "CNN_1  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.212561\n",
            "\n",
            "Test set: Average loss: 0.2116, Accuracy: 13979/15000 (93.19%)\n",
            "\n",
            "new accuracy parameters saved 0.9319333333333333\n",
            "CNN_1  Epoch: 2 [0/45056 (0%)]\tLoss: 0.236933\n",
            "CNN_1  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.191060\n",
            "CNN_1  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.102540\n",
            "CNN_1  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.104926\n",
            "\n",
            "Test set: Average loss: 0.1540, Accuracy: 14276/15000 (95.17%)\n",
            "\n",
            "new accuracy parameters saved 0.9517333333333333\n",
            "CNN_1  Epoch: 3 [0/45056 (0%)]\tLoss: 0.158910\n",
            "CNN_1  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.140458\n",
            "CNN_1  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.174216\n",
            "CNN_1  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.177657\n",
            "\n",
            "Test set: Average loss: 0.1180, Accuracy: 14457/15000 (96.38%)\n",
            "\n",
            "new accuracy parameters saved 0.9638\n",
            "CNN_1  Epoch: 4 [0/45056 (0%)]\tLoss: 0.079412\n",
            "CNN_1  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.169803\n",
            "CNN_1  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.134750\n",
            "CNN_1  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.198613\n",
            "\n",
            "Test set: Average loss: 0.1119, Accuracy: 14472/15000 (96.48%)\n",
            "\n",
            "new accuracy parameters saved 0.9648\n",
            "CNN_1  Epoch: 5 [0/45056 (0%)]\tLoss: 0.100372\n",
            "CNN_1  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.072498\n",
            "CNN_1  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.044290\n",
            "CNN_1  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.109136\n",
            "\n",
            "Test set: Average loss: 0.0986, Accuracy: 14532/15000 (96.88%)\n",
            "\n",
            "new accuracy parameters saved 0.9688\n",
            "CNN_1  Epoch: 6 [0/45056 (0%)]\tLoss: 0.129590\n",
            "CNN_1  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.073251\n",
            "CNN_1  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.137379\n",
            "CNN_1  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.064363\n",
            "\n",
            "Test set: Average loss: 0.0849, Accuracy: 14588/15000 (97.25%)\n",
            "\n",
            "new accuracy parameters saved 0.9725333333333334\n",
            "CNN_1  Epoch: 7 [0/45056 (0%)]\tLoss: 0.039459\n",
            "CNN_1  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.084012\n",
            "CNN_1  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.130992\n",
            "CNN_1  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.124550\n",
            "\n",
            "Test set: Average loss: 0.0915, Accuracy: 14553/15000 (97.02%)\n",
            "\n",
            "CNN_1  Epoch: 8 [0/45056 (0%)]\tLoss: 0.105972\n",
            "CNN_1  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.102523\n",
            "CNN_1  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.045222\n",
            "CNN_1  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.056991\n",
            "\n",
            "Test set: Average loss: 0.0834, Accuracy: 14610/15000 (97.40%)\n",
            "\n",
            "new accuracy parameters saved 0.974\n",
            "CNN_1  Epoch: 9 [0/45056 (0%)]\tLoss: 0.137899\n",
            "CNN_1  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.082537\n",
            "CNN_1  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.068746\n",
            "CNN_1  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.076242\n",
            "\n",
            "Test set: Average loss: 0.0732, Accuracy: 14660/15000 (97.73%)\n",
            "\n",
            "new accuracy parameters saved 0.9773333333333334\n",
            "Training complete in 1m 8s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pY-mUrGLKJnD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Running CNN2"
      ]
    },
    {
      "metadata": {
        "id": "qSQqcY_KKMFO",
        "colab_type": "code",
        "outputId": "29a885ee-bfb5-404e-9a0c-ae3e38425028",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 13731
        }
      },
      "cell_type": "code",
      "source": [
        "MultipleRun(runs,epochs,CNN_2())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   --  Run :  1\n",
            "CNN_2  Epoch: 0 [0/45056 (0%)]\tLoss: 2.308421\n",
            "CNN_2  Epoch: 0 [12800/45056 (28%)]\tLoss: 1.892709\n",
            "CNN_2  Epoch: 0 [25600/45056 (57%)]\tLoss: 1.317698\n",
            "CNN_2  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.710001\n",
            "\n",
            "Test set: Average loss: 1.2572, Accuracy: 9396/15000 (62.64%)\n",
            "\n",
            "new accuracy parameters saved 0.6264\n",
            "CNN_2  Epoch: 1 [0/45056 (0%)]\tLoss: 1.218316\n",
            "CNN_2  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.365753\n",
            "CNN_2  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.146704\n",
            "CNN_2  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.146768\n",
            "\n",
            "Test set: Average loss: 0.2191, Accuracy: 13924/15000 (92.83%)\n",
            "\n",
            "new accuracy parameters saved 0.9282666666666667\n",
            "CNN_2  Epoch: 2 [0/45056 (0%)]\tLoss: 0.154904\n",
            "CNN_2  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.139022\n",
            "CNN_2  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.110844\n",
            "CNN_2  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.076897\n",
            "\n",
            "Test set: Average loss: 0.1152, Accuracy: 14470/15000 (96.47%)\n",
            "\n",
            "new accuracy parameters saved 0.9646666666666667\n",
            "CNN_2  Epoch: 3 [0/45056 (0%)]\tLoss: 0.128845\n",
            "CNN_2  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.198111\n",
            "CNN_2  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.076983\n",
            "CNN_2  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.067302\n",
            "\n",
            "Test set: Average loss: 0.1156, Accuracy: 14461/15000 (96.41%)\n",
            "\n",
            "CNN_2  Epoch: 4 [0/45056 (0%)]\tLoss: 0.068448\n",
            "CNN_2  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.086214\n",
            "CNN_2  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.076170\n",
            "CNN_2  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.055922\n",
            "\n",
            "Test set: Average loss: 0.1060, Accuracy: 14502/15000 (96.68%)\n",
            "\n",
            "new accuracy parameters saved 0.9668\n",
            "CNN_2  Epoch: 5 [0/45056 (0%)]\tLoss: 0.162144\n",
            "CNN_2  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.016048\n",
            "CNN_2  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.071589\n",
            "CNN_2  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.061892\n",
            "\n",
            "Test set: Average loss: 0.0750, Accuracy: 14675/15000 (97.83%)\n",
            "\n",
            "new accuracy parameters saved 0.9783333333333334\n",
            "CNN_2  Epoch: 6 [0/45056 (0%)]\tLoss: 0.066724\n",
            "CNN_2  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.044745\n",
            "CNN_2  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.059694\n",
            "CNN_2  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.011752\n",
            "\n",
            "Test set: Average loss: 0.0649, Accuracy: 14686/15000 (97.91%)\n",
            "\n",
            "new accuracy parameters saved 0.9790666666666666\n",
            "CNN_2  Epoch: 7 [0/45056 (0%)]\tLoss: 0.073386\n",
            "CNN_2  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.028549\n",
            "CNN_2  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.008189\n",
            "CNN_2  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.031141\n",
            "\n",
            "Test set: Average loss: 0.0479, Accuracy: 14769/15000 (98.46%)\n",
            "\n",
            "new accuracy parameters saved 0.9846\n",
            "CNN_2  Epoch: 8 [0/45056 (0%)]\tLoss: 0.050088\n",
            "CNN_2  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.042458\n",
            "CNN_2  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.006815\n",
            "CNN_2  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.037539\n",
            "\n",
            "Test set: Average loss: 0.0710, Accuracy: 14685/15000 (97.90%)\n",
            "\n",
            "CNN_2  Epoch: 9 [0/45056 (0%)]\tLoss: 0.043393\n",
            "CNN_2  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.026880\n",
            "CNN_2  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.057136\n",
            "CNN_2  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.016585\n",
            "\n",
            "Test set: Average loss: 0.0551, Accuracy: 14732/15000 (98.21%)\n",
            "\n",
            "Training complete in 1m 23s\n",
            "\n",
            "   --  Run :  2\n",
            "CNN_2  Epoch: 0 [0/45056 (0%)]\tLoss: 2.301319\n",
            "CNN_2  Epoch: 0 [12800/45056 (28%)]\tLoss: 1.974545\n",
            "CNN_2  Epoch: 0 [25600/45056 (57%)]\tLoss: 1.125476\n",
            "CNN_2  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.543031\n",
            "\n",
            "Test set: Average loss: 0.4227, Accuracy: 13150/15000 (87.67%)\n",
            "\n",
            "new accuracy parameters saved 0.8766666666666667\n",
            "CNN_2  Epoch: 1 [0/45056 (0%)]\tLoss: 0.347028\n",
            "CNN_2  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.256197\n",
            "CNN_2  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.152661\n",
            "CNN_2  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.158411\n",
            "\n",
            "Test set: Average loss: 0.1539, Accuracy: 14298/15000 (95.32%)\n",
            "\n",
            "new accuracy parameters saved 0.9532\n",
            "CNN_2  Epoch: 2 [0/45056 (0%)]\tLoss: 0.091481\n",
            "CNN_2  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.291343\n",
            "CNN_2  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.107055\n",
            "CNN_2  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.144480\n",
            "\n",
            "Test set: Average loss: 0.0986, Accuracy: 14565/15000 (97.10%)\n",
            "\n",
            "new accuracy parameters saved 0.971\n",
            "CNN_2  Epoch: 3 [0/45056 (0%)]\tLoss: 0.160611\n",
            "CNN_2  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.088827\n",
            "CNN_2  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.126931\n",
            "CNN_2  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.168624\n",
            "\n",
            "Test set: Average loss: 0.2085, Accuracy: 14072/15000 (93.81%)\n",
            "\n",
            "CNN_2  Epoch: 4 [0/45056 (0%)]\tLoss: 0.207179\n",
            "CNN_2  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.068432\n",
            "CNN_2  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.074465\n",
            "CNN_2  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.049204\n",
            "\n",
            "Test set: Average loss: 0.0751, Accuracy: 14648/15000 (97.65%)\n",
            "\n",
            "new accuracy parameters saved 0.9765333333333334\n",
            "CNN_2  Epoch: 5 [0/45056 (0%)]\tLoss: 0.033409\n",
            "CNN_2  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.066086\n",
            "CNN_2  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.100650\n",
            "CNN_2  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.037635\n",
            "\n",
            "Test set: Average loss: 0.1066, Accuracy: 14496/15000 (96.64%)\n",
            "\n",
            "CNN_2  Epoch: 6 [0/45056 (0%)]\tLoss: 0.070378\n",
            "CNN_2  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.085704\n",
            "CNN_2  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.055605\n",
            "CNN_2  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.016711\n",
            "\n",
            "Test set: Average loss: 0.0750, Accuracy: 14668/15000 (97.79%)\n",
            "\n",
            "new accuracy parameters saved 0.9778666666666667\n",
            "CNN_2  Epoch: 7 [0/45056 (0%)]\tLoss: 0.041060\n",
            "CNN_2  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.014291\n",
            "CNN_2  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.037024\n",
            "CNN_2  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.076425\n",
            "\n",
            "Test set: Average loss: 0.1577, Accuracy: 14250/15000 (95.00%)\n",
            "\n",
            "CNN_2  Epoch: 8 [0/45056 (0%)]\tLoss: 0.099203\n",
            "CNN_2  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.083730\n",
            "CNN_2  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.064379\n",
            "CNN_2  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.014524\n",
            "\n",
            "Test set: Average loss: 0.1004, Accuracy: 14495/15000 (96.63%)\n",
            "\n",
            "CNN_2  Epoch: 9 [0/45056 (0%)]\tLoss: 0.136619\n",
            "CNN_2  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.015203\n",
            "CNN_2  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.057728\n",
            "CNN_2  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.117500\n",
            "\n",
            "Test set: Average loss: 0.0527, Accuracy: 14768/15000 (98.45%)\n",
            "\n",
            "new accuracy parameters saved 0.9845333333333334\n",
            "Training complete in 1m 22s\n",
            "\n",
            "   --  Run :  3\n",
            "CNN_2  Epoch: 0 [0/45056 (0%)]\tLoss: 2.314950\n",
            "CNN_2  Epoch: 0 [12800/45056 (28%)]\tLoss: 1.959404\n",
            "CNN_2  Epoch: 0 [25600/45056 (57%)]\tLoss: 1.718722\n",
            "CNN_2  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.657005\n",
            "\n",
            "Test set: Average loss: 0.3229, Accuracy: 13772/15000 (91.81%)\n",
            "\n",
            "new accuracy parameters saved 0.9181333333333334\n",
            "CNN_2  Epoch: 1 [0/45056 (0%)]\tLoss: 0.387715\n",
            "CNN_2  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.128034\n",
            "CNN_2  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.207157\n",
            "CNN_2  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.256858\n",
            "\n",
            "Test set: Average loss: 0.1306, Accuracy: 14417/15000 (96.11%)\n",
            "\n",
            "new accuracy parameters saved 0.9611333333333333\n",
            "CNN_2  Epoch: 2 [0/45056 (0%)]\tLoss: 0.088773\n",
            "CNN_2  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.114482\n",
            "CNN_2  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.116008\n",
            "CNN_2  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.097433\n",
            "\n",
            "Test set: Average loss: 0.1895, Accuracy: 14174/15000 (94.49%)\n",
            "\n",
            "CNN_2  Epoch: 3 [0/45056 (0%)]\tLoss: 0.229686\n",
            "CNN_2  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.049564\n",
            "CNN_2  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.109718\n",
            "CNN_2  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.117129\n",
            "\n",
            "Test set: Average loss: 0.0949, Accuracy: 14553/15000 (97.02%)\n",
            "\n",
            "new accuracy parameters saved 0.9702\n",
            "CNN_2  Epoch: 4 [0/45056 (0%)]\tLoss: 0.110974\n",
            "CNN_2  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.052846\n",
            "CNN_2  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.083109\n",
            "CNN_2  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.096142\n",
            "\n",
            "Test set: Average loss: 0.0622, Accuracy: 14696/15000 (97.97%)\n",
            "\n",
            "new accuracy parameters saved 0.9797333333333333\n",
            "CNN_2  Epoch: 5 [0/45056 (0%)]\tLoss: 0.053249\n",
            "CNN_2  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.093809\n",
            "CNN_2  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.063073\n",
            "CNN_2  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.036339\n",
            "\n",
            "Test set: Average loss: 0.0683, Accuracy: 14682/15000 (97.88%)\n",
            "\n",
            "CNN_2  Epoch: 6 [0/45056 (0%)]\tLoss: 0.010030\n",
            "CNN_2  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.023671\n",
            "CNN_2  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.010108\n",
            "CNN_2  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.041933\n",
            "\n",
            "Test set: Average loss: 0.0608, Accuracy: 14714/15000 (98.09%)\n",
            "\n",
            "new accuracy parameters saved 0.9809333333333333\n",
            "CNN_2  Epoch: 7 [0/45056 (0%)]\tLoss: 0.063611\n",
            "CNN_2  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.027623\n",
            "CNN_2  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.023169\n",
            "CNN_2  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.023290\n",
            "\n",
            "Test set: Average loss: 0.1569, Accuracy: 14246/15000 (94.97%)\n",
            "\n",
            "CNN_2  Epoch: 8 [0/45056 (0%)]\tLoss: 0.028160\n",
            "CNN_2  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.039857\n",
            "CNN_2  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.040177\n",
            "CNN_2  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.078799\n",
            "\n",
            "Test set: Average loss: 0.0476, Accuracy: 14769/15000 (98.46%)\n",
            "\n",
            "new accuracy parameters saved 0.9846\n",
            "CNN_2  Epoch: 9 [0/45056 (0%)]\tLoss: 0.031483\n",
            "CNN_2  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.013491\n",
            "CNN_2  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.062508\n",
            "CNN_2  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.008296\n",
            "\n",
            "Test set: Average loss: 0.0458, Accuracy: 14774/15000 (98.49%)\n",
            "\n",
            "new accuracy parameters saved 0.9849333333333333\n",
            "Training complete in 1m 23s\n",
            "\n",
            "   --  Run :  4\n",
            "CNN_2  Epoch: 0 [0/45056 (0%)]\tLoss: 2.304063\n",
            "CNN_2  Epoch: 0 [12800/45056 (28%)]\tLoss: 2.100583\n",
            "CNN_2  Epoch: 0 [25600/45056 (57%)]\tLoss: 1.174149\n",
            "CNN_2  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.576878\n",
            "\n",
            "Test set: Average loss: 0.4491, Accuracy: 12753/15000 (85.02%)\n",
            "\n",
            "new accuracy parameters saved 0.8502\n",
            "CNN_2  Epoch: 1 [0/45056 (0%)]\tLoss: 0.526490\n",
            "CNN_2  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.288887\n",
            "CNN_2  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.146820\n",
            "CNN_2  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.176951\n",
            "\n",
            "Test set: Average loss: 0.1756, Accuracy: 14179/15000 (94.53%)\n",
            "\n",
            "new accuracy parameters saved 0.9452666666666667\n",
            "CNN_2  Epoch: 2 [0/45056 (0%)]\tLoss: 0.237123\n",
            "CNN_2  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.149551\n",
            "CNN_2  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.069702\n",
            "CNN_2  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.165632\n",
            "\n",
            "Test set: Average loss: 0.1680, Accuracy: 14227/15000 (94.85%)\n",
            "\n",
            "new accuracy parameters saved 0.9484666666666667\n",
            "CNN_2  Epoch: 3 [0/45056 (0%)]\tLoss: 0.148940\n",
            "CNN_2  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.120674\n",
            "CNN_2  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.047075\n",
            "CNN_2  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.077625\n",
            "\n",
            "Test set: Average loss: 0.1194, Accuracy: 14462/15000 (96.41%)\n",
            "\n",
            "new accuracy parameters saved 0.9641333333333333\n",
            "CNN_2  Epoch: 4 [0/45056 (0%)]\tLoss: 0.142820\n",
            "CNN_2  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.104083\n",
            "CNN_2  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.132388\n",
            "CNN_2  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.063392\n",
            "\n",
            "Test set: Average loss: 0.1075, Accuracy: 14469/15000 (96.46%)\n",
            "\n",
            "new accuracy parameters saved 0.9646\n",
            "CNN_2  Epoch: 5 [0/45056 (0%)]\tLoss: 0.132096\n",
            "CNN_2  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.025963\n",
            "CNN_2  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.034486\n",
            "CNN_2  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.024752\n",
            "\n",
            "Test set: Average loss: 0.0605, Accuracy: 14725/15000 (98.17%)\n",
            "\n",
            "new accuracy parameters saved 0.9816666666666667\n",
            "CNN_2  Epoch: 6 [0/45056 (0%)]\tLoss: 0.027661\n",
            "CNN_2  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.066750\n",
            "CNN_2  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.031864\n",
            "CNN_2  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.069238\n",
            "\n",
            "Test set: Average loss: 0.0995, Accuracy: 14519/15000 (96.79%)\n",
            "\n",
            "CNN_2  Epoch: 7 [0/45056 (0%)]\tLoss: 0.064453\n",
            "CNN_2  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.025805\n",
            "CNN_2  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.043789\n",
            "CNN_2  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.053202\n",
            "\n",
            "Test set: Average loss: 0.0550, Accuracy: 14739/15000 (98.26%)\n",
            "\n",
            "new accuracy parameters saved 0.9826\n",
            "CNN_2  Epoch: 8 [0/45056 (0%)]\tLoss: 0.013507\n",
            "CNN_2  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.021837\n",
            "CNN_2  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.026826\n",
            "CNN_2  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.039912\n",
            "\n",
            "Test set: Average loss: 0.0449, Accuracy: 14792/15000 (98.61%)\n",
            "\n",
            "new accuracy parameters saved 0.9861333333333333\n",
            "CNN_2  Epoch: 9 [0/45056 (0%)]\tLoss: 0.040924\n",
            "CNN_2  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.026242\n",
            "CNN_2  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.058706\n",
            "CNN_2  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.075608\n",
            "\n",
            "Test set: Average loss: 0.0493, Accuracy: 14768/15000 (98.45%)\n",
            "\n",
            "Training complete in 1m 23s\n",
            "\n",
            "   --  Run :  5\n",
            "CNN_2  Epoch: 0 [0/45056 (0%)]\tLoss: 2.303797\n",
            "CNN_2  Epoch: 0 [12800/45056 (28%)]\tLoss: 2.093316\n",
            "CNN_2  Epoch: 0 [25600/45056 (57%)]\tLoss: 2.244138\n",
            "CNN_2  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.569068\n",
            "\n",
            "Test set: Average loss: 0.5401, Accuracy: 12539/15000 (83.59%)\n",
            "\n",
            "new accuracy parameters saved 0.8359333333333333\n",
            "CNN_2  Epoch: 1 [0/45056 (0%)]\tLoss: 0.560470\n",
            "CNN_2  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.363407\n",
            "CNN_2  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.137802\n",
            "CNN_2  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.149444\n",
            "\n",
            "Test set: Average loss: 0.1823, Accuracy: 14129/15000 (94.19%)\n",
            "\n",
            "new accuracy parameters saved 0.9419333333333333\n",
            "CNN_2  Epoch: 2 [0/45056 (0%)]\tLoss: 0.186349\n",
            "CNN_2  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.185575\n",
            "CNN_2  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.058036\n",
            "CNN_2  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.195660\n",
            "\n",
            "Test set: Average loss: 0.1041, Accuracy: 14522/15000 (96.81%)\n",
            "\n",
            "new accuracy parameters saved 0.9681333333333333\n",
            "CNN_2  Epoch: 3 [0/45056 (0%)]\tLoss: 0.126324\n",
            "CNN_2  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.075877\n",
            "CNN_2  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.039070\n",
            "CNN_2  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.088707\n",
            "\n",
            "Test set: Average loss: 0.0976, Accuracy: 14593/15000 (97.29%)\n",
            "\n",
            "new accuracy parameters saved 0.9728666666666667\n",
            "CNN_2  Epoch: 4 [0/45056 (0%)]\tLoss: 0.079583\n",
            "CNN_2  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.056937\n",
            "CNN_2  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.069120\n",
            "CNN_2  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.042713\n",
            "\n",
            "Test set: Average loss: 0.0653, Accuracy: 14670/15000 (97.80%)\n",
            "\n",
            "new accuracy parameters saved 0.978\n",
            "CNN_2  Epoch: 5 [0/45056 (0%)]\tLoss: 0.050679\n",
            "CNN_2  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.029285\n",
            "CNN_2  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.049047\n",
            "CNN_2  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.063233\n",
            "\n",
            "Test set: Average loss: 0.0607, Accuracy: 14723/15000 (98.15%)\n",
            "\n",
            "new accuracy parameters saved 0.9815333333333334\n",
            "CNN_2  Epoch: 6 [0/45056 (0%)]\tLoss: 0.094045\n",
            "CNN_2  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.043158\n",
            "CNN_2  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.075558\n",
            "CNN_2  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.043249\n",
            "\n",
            "Test set: Average loss: 0.0504, Accuracy: 14768/15000 (98.45%)\n",
            "\n",
            "new accuracy parameters saved 0.9845333333333334\n",
            "CNN_2  Epoch: 7 [0/45056 (0%)]\tLoss: 0.063501\n",
            "CNN_2  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.012269\n",
            "CNN_2  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.077853\n",
            "CNN_2  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.076834\n",
            "\n",
            "Test set: Average loss: 0.0505, Accuracy: 14777/15000 (98.51%)\n",
            "\n",
            "new accuracy parameters saved 0.9851333333333333\n",
            "CNN_2  Epoch: 8 [0/45056 (0%)]\tLoss: 0.032043\n",
            "CNN_2  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.029133\n",
            "CNN_2  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.146976\n",
            "CNN_2  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.024296\n",
            "\n",
            "Test set: Average loss: 0.0487, Accuracy: 14760/15000 (98.40%)\n",
            "\n",
            "CNN_2  Epoch: 9 [0/45056 (0%)]\tLoss: 0.010339\n",
            "CNN_2  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.017475\n",
            "CNN_2  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.029650\n",
            "CNN_2  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.018553\n",
            "\n",
            "Test set: Average loss: 0.0485, Accuracy: 14771/15000 (98.47%)\n",
            "\n",
            "Training complete in 1m 23s\n",
            "\n",
            "   --  Run :  6\n",
            "CNN_2  Epoch: 0 [0/45056 (0%)]\tLoss: 2.301774\n",
            "CNN_2  Epoch: 0 [12800/45056 (28%)]\tLoss: 2.021868\n",
            "CNN_2  Epoch: 0 [25600/45056 (57%)]\tLoss: 1.018508\n",
            "CNN_2  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.362029\n",
            "\n",
            "Test set: Average loss: 0.8231, Accuracy: 11352/15000 (75.68%)\n",
            "\n",
            "new accuracy parameters saved 0.7568\n",
            "CNN_2  Epoch: 1 [0/45056 (0%)]\tLoss: 0.653857\n",
            "CNN_2  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.320809\n",
            "CNN_2  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.266487\n",
            "CNN_2  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.101623\n",
            "\n",
            "Test set: Average loss: 0.1550, Accuracy: 14374/15000 (95.83%)\n",
            "\n",
            "new accuracy parameters saved 0.9582666666666667\n",
            "CNN_2  Epoch: 2 [0/45056 (0%)]\tLoss: 0.108384\n",
            "CNN_2  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.075513\n",
            "CNN_2  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.127825\n",
            "CNN_2  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.133683\n",
            "\n",
            "Test set: Average loss: 0.1232, Accuracy: 14421/15000 (96.14%)\n",
            "\n",
            "new accuracy parameters saved 0.9614\n",
            "CNN_2  Epoch: 3 [0/45056 (0%)]\tLoss: 0.087938\n",
            "CNN_2  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.074948\n",
            "CNN_2  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.058190\n",
            "CNN_2  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.054926\n",
            "\n",
            "Test set: Average loss: 0.0690, Accuracy: 14682/15000 (97.88%)\n",
            "\n",
            "new accuracy parameters saved 0.9788\n",
            "CNN_2  Epoch: 4 [0/45056 (0%)]\tLoss: 0.027595\n",
            "CNN_2  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.075856\n",
            "CNN_2  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.039080\n",
            "CNN_2  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.017035\n",
            "\n",
            "Test set: Average loss: 0.0805, Accuracy: 14611/15000 (97.41%)\n",
            "\n",
            "CNN_2  Epoch: 5 [0/45056 (0%)]\tLoss: 0.038142\n",
            "CNN_2  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.111345\n",
            "CNN_2  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.043290\n",
            "CNN_2  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.086251\n",
            "\n",
            "Test set: Average loss: 0.0866, Accuracy: 14616/15000 (97.44%)\n",
            "\n",
            "CNN_2  Epoch: 6 [0/45056 (0%)]\tLoss: 0.083644\n",
            "CNN_2  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.076557\n",
            "CNN_2  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.071256\n",
            "CNN_2  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.053698\n",
            "\n",
            "Test set: Average loss: 0.0671, Accuracy: 14665/15000 (97.77%)\n",
            "\n",
            "CNN_2  Epoch: 7 [0/45056 (0%)]\tLoss: 0.069585\n",
            "CNN_2  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.013660\n",
            "CNN_2  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.081253\n",
            "CNN_2  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.022845\n",
            "\n",
            "Test set: Average loss: 0.0533, Accuracy: 14747/15000 (98.31%)\n",
            "\n",
            "new accuracy parameters saved 0.9831333333333333\n",
            "CNN_2  Epoch: 8 [0/45056 (0%)]\tLoss: 0.029523\n",
            "CNN_2  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.017251\n",
            "CNN_2  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.056830\n",
            "CNN_2  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.022361\n",
            "\n",
            "Test set: Average loss: 0.0470, Accuracy: 14778/15000 (98.52%)\n",
            "\n",
            "new accuracy parameters saved 0.9852\n",
            "CNN_2  Epoch: 9 [0/45056 (0%)]\tLoss: 0.034580\n",
            "CNN_2  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.025517\n",
            "CNN_2  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.091539\n",
            "CNN_2  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.021874\n",
            "\n",
            "Test set: Average loss: 0.0445, Accuracy: 14795/15000 (98.63%)\n",
            "\n",
            "new accuracy parameters saved 0.9863333333333333\n",
            "Training complete in 1m 24s\n",
            "\n",
            "   --  Run :  7\n",
            "CNN_2  Epoch: 0 [0/45056 (0%)]\tLoss: 2.308989\n",
            "CNN_2  Epoch: 0 [12800/45056 (28%)]\tLoss: 2.174320\n",
            "CNN_2  Epoch: 0 [25600/45056 (57%)]\tLoss: 1.269212\n",
            "CNN_2  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.869220\n",
            "\n",
            "Test set: Average loss: 0.4564, Accuracy: 13518/15000 (90.12%)\n",
            "\n",
            "new accuracy parameters saved 0.9012\n",
            "CNN_2  Epoch: 1 [0/45056 (0%)]\tLoss: 0.401212\n",
            "CNN_2  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.255736\n",
            "CNN_2  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.283591\n",
            "CNN_2  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.179136\n",
            "\n",
            "Test set: Average loss: 0.2036, Accuracy: 14062/15000 (93.75%)\n",
            "\n",
            "new accuracy parameters saved 0.9374666666666667\n",
            "CNN_2  Epoch: 2 [0/45056 (0%)]\tLoss: 0.109307\n",
            "CNN_2  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.121516\n",
            "CNN_2  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.154481\n",
            "CNN_2  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.097738\n",
            "\n",
            "Test set: Average loss: 0.1735, Accuracy: 14181/15000 (94.54%)\n",
            "\n",
            "new accuracy parameters saved 0.9454\n",
            "CNN_2  Epoch: 3 [0/45056 (0%)]\tLoss: 0.123241\n",
            "CNN_2  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.076785\n",
            "CNN_2  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.102614\n",
            "CNN_2  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.036169\n",
            "\n",
            "Test set: Average loss: 0.0747, Accuracy: 14653/15000 (97.69%)\n",
            "\n",
            "new accuracy parameters saved 0.9768666666666667\n",
            "CNN_2  Epoch: 4 [0/45056 (0%)]\tLoss: 0.028242\n",
            "CNN_2  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.036248\n",
            "CNN_2  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.120456\n",
            "CNN_2  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.075707\n",
            "\n",
            "Test set: Average loss: 0.0940, Accuracy: 14585/15000 (97.23%)\n",
            "\n",
            "CNN_2  Epoch: 5 [0/45056 (0%)]\tLoss: 0.070148\n",
            "CNN_2  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.083719\n",
            "CNN_2  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.073161\n",
            "CNN_2  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.070019\n",
            "\n",
            "Test set: Average loss: 0.0683, Accuracy: 14669/15000 (97.79%)\n",
            "\n",
            "new accuracy parameters saved 0.9779333333333333\n",
            "CNN_2  Epoch: 6 [0/45056 (0%)]\tLoss: 0.016409\n",
            "CNN_2  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.063382\n",
            "CNN_2  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.021866\n",
            "CNN_2  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.026599\n",
            "\n",
            "Test set: Average loss: 0.0611, Accuracy: 14707/15000 (98.05%)\n",
            "\n",
            "new accuracy parameters saved 0.9804666666666667\n",
            "CNN_2  Epoch: 7 [0/45056 (0%)]\tLoss: 0.056333\n",
            "CNN_2  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.041354\n",
            "CNN_2  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.060779\n",
            "CNN_2  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.075837\n",
            "\n",
            "Test set: Average loss: 0.0674, Accuracy: 14705/15000 (98.03%)\n",
            "\n",
            "CNN_2  Epoch: 8 [0/45056 (0%)]\tLoss: 0.016105\n",
            "CNN_2  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.018371\n",
            "CNN_2  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.034355\n",
            "CNN_2  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.020271\n",
            "\n",
            "Test set: Average loss: 0.0481, Accuracy: 14765/15000 (98.43%)\n",
            "\n",
            "new accuracy parameters saved 0.9843333333333333\n",
            "CNN_2  Epoch: 9 [0/45056 (0%)]\tLoss: 0.051641\n",
            "CNN_2  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.094079\n",
            "CNN_2  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.027847\n",
            "CNN_2  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.021206\n",
            "\n",
            "Test set: Average loss: 0.0757, Accuracy: 14633/15000 (97.55%)\n",
            "\n",
            "Training complete in 1m 23s\n",
            "\n",
            "   --  Run :  8\n",
            "CNN_2  Epoch: 0 [0/45056 (0%)]\tLoss: 2.306568\n",
            "CNN_2  Epoch: 0 [12800/45056 (28%)]\tLoss: 2.036438\n",
            "CNN_2  Epoch: 0 [25600/45056 (57%)]\tLoss: 1.463053\n",
            "CNN_2  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.487349\n",
            "\n",
            "Test set: Average loss: 0.3991, Accuracy: 13531/15000 (90.21%)\n",
            "\n",
            "new accuracy parameters saved 0.9020666666666667\n",
            "CNN_2  Epoch: 1 [0/45056 (0%)]\tLoss: 0.322580\n",
            "CNN_2  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.463916\n",
            "CNN_2  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.075098\n",
            "CNN_2  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.140767\n",
            "\n",
            "Test set: Average loss: 0.1357, Accuracy: 14406/15000 (96.04%)\n",
            "\n",
            "new accuracy parameters saved 0.9604\n",
            "CNN_2  Epoch: 2 [0/45056 (0%)]\tLoss: 0.175811\n",
            "CNN_2  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.193993\n",
            "CNN_2  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.054948\n",
            "CNN_2  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.103156\n",
            "\n",
            "Test set: Average loss: 0.1737, Accuracy: 14194/15000 (94.63%)\n",
            "\n",
            "CNN_2  Epoch: 3 [0/45056 (0%)]\tLoss: 0.095353\n",
            "CNN_2  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.092360\n",
            "CNN_2  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.067280\n",
            "CNN_2  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.033402\n",
            "\n",
            "Test set: Average loss: 0.0748, Accuracy: 14666/15000 (97.77%)\n",
            "\n",
            "new accuracy parameters saved 0.9777333333333333\n",
            "CNN_2  Epoch: 4 [0/45056 (0%)]\tLoss: 0.064558\n",
            "CNN_2  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.051354\n",
            "CNN_2  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.120432\n",
            "CNN_2  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.061979\n",
            "\n",
            "Test set: Average loss: 0.0826, Accuracy: 14620/15000 (97.47%)\n",
            "\n",
            "CNN_2  Epoch: 5 [0/45056 (0%)]\tLoss: 0.111810\n",
            "CNN_2  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.068105\n",
            "CNN_2  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.117586\n",
            "CNN_2  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.018312\n",
            "\n",
            "Test set: Average loss: 0.0521, Accuracy: 14756/15000 (98.37%)\n",
            "\n",
            "new accuracy parameters saved 0.9837333333333333\n",
            "CNN_2  Epoch: 6 [0/45056 (0%)]\tLoss: 0.025082\n",
            "CNN_2  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.009556\n",
            "CNN_2  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.026421\n",
            "CNN_2  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.047058\n",
            "\n",
            "Test set: Average loss: 0.0566, Accuracy: 14732/15000 (98.21%)\n",
            "\n",
            "CNN_2  Epoch: 7 [0/45056 (0%)]\tLoss: 0.022713\n",
            "CNN_2  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.044110\n",
            "CNN_2  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.754322\n",
            "CNN_2  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.032783\n",
            "\n",
            "Test set: Average loss: 0.0981, Accuracy: 14530/15000 (96.87%)\n",
            "\n",
            "CNN_2  Epoch: 8 [0/45056 (0%)]\tLoss: 0.085526\n",
            "CNN_2  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.034741\n",
            "CNN_2  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.028374\n",
            "CNN_2  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.081154\n",
            "\n",
            "Test set: Average loss: 0.0632, Accuracy: 14712/15000 (98.08%)\n",
            "\n",
            "CNN_2  Epoch: 9 [0/45056 (0%)]\tLoss: 0.021434\n",
            "CNN_2  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.019848\n",
            "CNN_2  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.057164\n",
            "CNN_2  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.016179\n",
            "\n",
            "Test set: Average loss: 0.0460, Accuracy: 14788/15000 (98.59%)\n",
            "\n",
            "new accuracy parameters saved 0.9858666666666667\n",
            "Training complete in 1m 23s\n",
            "\n",
            "   --  Run :  9\n",
            "CNN_2  Epoch: 0 [0/45056 (0%)]\tLoss: 2.303835\n",
            "CNN_2  Epoch: 0 [12800/45056 (28%)]\tLoss: 2.180648\n",
            "CNN_2  Epoch: 0 [25600/45056 (57%)]\tLoss: 1.409147\n",
            "CNN_2  Epoch: 0 [38400/45056 (85%)]\tLoss: 1.320739\n",
            "\n",
            "Test set: Average loss: 0.6744, Accuracy: 11744/15000 (78.29%)\n",
            "\n",
            "new accuracy parameters saved 0.7829333333333334\n",
            "CNN_2  Epoch: 1 [0/45056 (0%)]\tLoss: 0.576890\n",
            "CNN_2  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.370927\n",
            "CNN_2  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.194432\n",
            "CNN_2  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.536074\n",
            "\n",
            "Test set: Average loss: 0.2008, Accuracy: 14045/15000 (93.63%)\n",
            "\n",
            "new accuracy parameters saved 0.9363333333333334\n",
            "CNN_2  Epoch: 2 [0/45056 (0%)]\tLoss: 0.174232\n",
            "CNN_2  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.080948\n",
            "CNN_2  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.108487\n",
            "CNN_2  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.233343\n",
            "\n",
            "Test set: Average loss: 0.1907, Accuracy: 14110/15000 (94.07%)\n",
            "\n",
            "new accuracy parameters saved 0.9406666666666667\n",
            "CNN_2  Epoch: 3 [0/45056 (0%)]\tLoss: 0.320367\n",
            "CNN_2  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.107557\n",
            "CNN_2  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.088831\n",
            "CNN_2  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.077671\n",
            "\n",
            "Test set: Average loss: 0.1138, Accuracy: 14453/15000 (96.35%)\n",
            "\n",
            "new accuracy parameters saved 0.9635333333333334\n",
            "CNN_2  Epoch: 4 [0/45056 (0%)]\tLoss: 0.092773\n",
            "CNN_2  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.141878\n",
            "CNN_2  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.077532\n",
            "CNN_2  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.070098\n",
            "\n",
            "Test set: Average loss: 0.0816, Accuracy: 14611/15000 (97.41%)\n",
            "\n",
            "new accuracy parameters saved 0.9740666666666666\n",
            "CNN_2  Epoch: 5 [0/45056 (0%)]\tLoss: 0.070792\n",
            "CNN_2  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.107520\n",
            "CNN_2  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.030825\n",
            "CNN_2  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.067588\n",
            "\n",
            "Test set: Average loss: 0.0627, Accuracy: 14689/15000 (97.93%)\n",
            "\n",
            "new accuracy parameters saved 0.9792666666666666\n",
            "CNN_2  Epoch: 6 [0/45056 (0%)]\tLoss: 0.038534\n",
            "CNN_2  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.045870\n",
            "CNN_2  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.071659\n",
            "CNN_2  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.063885\n",
            "\n",
            "Test set: Average loss: 0.0696, Accuracy: 14668/15000 (97.79%)\n",
            "\n",
            "CNN_2  Epoch: 7 [0/45056 (0%)]\tLoss: 0.057705\n",
            "CNN_2  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.065227\n",
            "CNN_2  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.032053\n",
            "CNN_2  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.060479\n",
            "\n",
            "Test set: Average loss: 0.0724, Accuracy: 14669/15000 (97.79%)\n",
            "\n",
            "CNN_2  Epoch: 8 [0/45056 (0%)]\tLoss: 0.118888\n",
            "CNN_2  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.039672\n",
            "CNN_2  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.036772\n",
            "CNN_2  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.059320\n",
            "\n",
            "Test set: Average loss: 0.0562, Accuracy: 14741/15000 (98.27%)\n",
            "\n",
            "new accuracy parameters saved 0.9827333333333333\n",
            "CNN_2  Epoch: 9 [0/45056 (0%)]\tLoss: 0.049521\n",
            "CNN_2  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.022994\n",
            "CNN_2  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.891498\n",
            "CNN_2  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.092296\n",
            "\n",
            "Test set: Average loss: 0.0559, Accuracy: 14737/15000 (98.25%)\n",
            "\n",
            "Training complete in 1m 23s\n",
            "\n",
            "   --  Run :  10\n",
            "CNN_2  Epoch: 0 [0/45056 (0%)]\tLoss: 2.305807\n",
            "CNN_2  Epoch: 0 [12800/45056 (28%)]\tLoss: 2.089302\n",
            "CNN_2  Epoch: 0 [25600/45056 (57%)]\tLoss: 1.296019\n",
            "CNN_2  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.534499\n",
            "\n",
            "Test set: Average loss: 0.4500, Accuracy: 13273/15000 (88.49%)\n",
            "\n",
            "new accuracy parameters saved 0.8848666666666667\n",
            "CNN_2  Epoch: 1 [0/45056 (0%)]\tLoss: 0.516594\n",
            "CNN_2  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.367935\n",
            "CNN_2  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.193642\n",
            "CNN_2  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.739319\n",
            "\n",
            "Test set: Average loss: 0.1928, Accuracy: 14080/15000 (93.87%)\n",
            "\n",
            "new accuracy parameters saved 0.9386666666666666\n",
            "CNN_2  Epoch: 2 [0/45056 (0%)]\tLoss: 0.151242\n",
            "CNN_2  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.131856\n",
            "CNN_2  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.156919\n",
            "CNN_2  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.174467\n",
            "\n",
            "Test set: Average loss: 0.0948, Accuracy: 14566/15000 (97.11%)\n",
            "\n",
            "new accuracy parameters saved 0.9710666666666666\n",
            "CNN_2  Epoch: 3 [0/45056 (0%)]\tLoss: 0.076672\n",
            "CNN_2  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.125880\n",
            "CNN_2  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.075432\n",
            "CNN_2  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.047976\n",
            "\n",
            "Test set: Average loss: 0.0884, Accuracy: 14584/15000 (97.23%)\n",
            "\n",
            "new accuracy parameters saved 0.9722666666666666\n",
            "CNN_2  Epoch: 4 [0/45056 (0%)]\tLoss: 0.085493\n",
            "CNN_2  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.109036\n",
            "CNN_2  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.097581\n",
            "CNN_2  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.082370\n",
            "\n",
            "Test set: Average loss: 0.0716, Accuracy: 14649/15000 (97.66%)\n",
            "\n",
            "new accuracy parameters saved 0.9766\n",
            "CNN_2  Epoch: 5 [0/45056 (0%)]\tLoss: 0.065394\n",
            "CNN_2  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.034228\n",
            "CNN_2  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.059067\n",
            "CNN_2  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.078408\n",
            "\n",
            "Test set: Average loss: 0.0664, Accuracy: 14684/15000 (97.89%)\n",
            "\n",
            "new accuracy parameters saved 0.9789333333333333\n",
            "CNN_2  Epoch: 6 [0/45056 (0%)]\tLoss: 0.079134\n",
            "CNN_2  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.067290\n",
            "CNN_2  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.027379\n",
            "CNN_2  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.025397\n",
            "\n",
            "Test set: Average loss: 0.0544, Accuracy: 14752/15000 (98.35%)\n",
            "\n",
            "new accuracy parameters saved 0.9834666666666667\n",
            "CNN_2  Epoch: 7 [0/45056 (0%)]\tLoss: 0.026068\n",
            "CNN_2  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.038426\n",
            "CNN_2  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.029829\n",
            "CNN_2  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.053118\n",
            "\n",
            "Test set: Average loss: 0.0567, Accuracy: 14744/15000 (98.29%)\n",
            "\n",
            "CNN_2  Epoch: 8 [0/45056 (0%)]\tLoss: 0.034504\n",
            "CNN_2  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.027768\n",
            "CNN_2  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.006918\n",
            "CNN_2  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.025965\n",
            "\n",
            "Test set: Average loss: 0.0551, Accuracy: 14736/15000 (98.24%)\n",
            "\n",
            "CNN_2  Epoch: 9 [0/45056 (0%)]\tLoss: 0.025927\n",
            "CNN_2  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.019804\n",
            "CNN_2  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.064363\n",
            "CNN_2  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.156768\n",
            "\n",
            "Test set: Average loss: 0.0402, Accuracy: 14808/15000 (98.72%)\n",
            "\n",
            "new accuracy parameters saved 0.9872\n",
            "Training complete in 1m 23s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3NvIdf7DKM2F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Running CNN3 "
      ]
    },
    {
      "metadata": {
        "id": "dK7Dj6haKSbT",
        "colab_type": "code",
        "outputId": "fb2e732d-8b2b-4b11-d45d-48af730e0514",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 13680
        }
      },
      "cell_type": "code",
      "source": [
        "MultipleRun(runs,epochs,CNN_3())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   --  Run :  1\n",
            "CNN_3  Epoch: 0 [0/45056 (0%)]\tLoss: 2.299202\n",
            "CNN_3  Epoch: 0 [12800/45056 (28%)]\tLoss: 2.171045\n",
            "CNN_3  Epoch: 0 [25600/45056 (57%)]\tLoss: 1.596208\n",
            "CNN_3  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.517546\n",
            "\n",
            "Test set: Average loss: 0.4286, Accuracy: 12954/15000 (86.36%)\n",
            "\n",
            "new accuracy parameters saved 0.8636\n",
            "CNN_3  Epoch: 1 [0/45056 (0%)]\tLoss: 0.413768\n",
            "CNN_3  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.230569\n",
            "CNN_3  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.513815\n",
            "CNN_3  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.154269\n",
            "\n",
            "Test set: Average loss: 0.0925, Accuracy: 14572/15000 (97.15%)\n",
            "\n",
            "new accuracy parameters saved 0.9714666666666667\n",
            "CNN_3  Epoch: 2 [0/45056 (0%)]\tLoss: 0.081075\n",
            "CNN_3  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.142785\n",
            "CNN_3  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.042022\n",
            "CNN_3  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.046486\n",
            "\n",
            "Test set: Average loss: 0.0686, Accuracy: 14669/15000 (97.79%)\n",
            "\n",
            "new accuracy parameters saved 0.9779333333333333\n",
            "CNN_3  Epoch: 3 [0/45056 (0%)]\tLoss: 0.053437\n",
            "CNN_3  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.066147\n",
            "CNN_3  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.014925\n",
            "CNN_3  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.009612\n",
            "\n",
            "Test set: Average loss: 0.0624, Accuracy: 14712/15000 (98.08%)\n",
            "\n",
            "new accuracy parameters saved 0.9808\n",
            "CNN_3  Epoch: 4 [0/45056 (0%)]\tLoss: 0.028816\n",
            "CNN_3  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.027903\n",
            "CNN_3  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.046355\n",
            "CNN_3  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.088946\n",
            "\n",
            "Test set: Average loss: 0.0673, Accuracy: 14688/15000 (97.92%)\n",
            "\n",
            "CNN_3  Epoch: 5 [0/45056 (0%)]\tLoss: 0.049334\n",
            "CNN_3  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.012550\n",
            "CNN_3  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.032551\n",
            "CNN_3  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.031832\n",
            "\n",
            "Test set: Average loss: 0.0439, Accuracy: 14789/15000 (98.59%)\n",
            "\n",
            "new accuracy parameters saved 0.9859333333333333\n",
            "CNN_3  Epoch: 6 [0/45056 (0%)]\tLoss: 0.021849\n",
            "CNN_3  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.012429\n",
            "CNN_3  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.062772\n",
            "CNN_3  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.027332\n",
            "\n",
            "Test set: Average loss: 0.0544, Accuracy: 14754/15000 (98.36%)\n",
            "\n",
            "CNN_3  Epoch: 7 [0/45056 (0%)]\tLoss: 0.059403\n",
            "CNN_3  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.012313\n",
            "CNN_3  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.004917\n",
            "CNN_3  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.041544\n",
            "\n",
            "Test set: Average loss: 0.0390, Accuracy: 14815/15000 (98.77%)\n",
            "\n",
            "new accuracy parameters saved 0.9876666666666667\n",
            "CNN_3  Epoch: 8 [0/45056 (0%)]\tLoss: 0.032253\n",
            "CNN_3  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.010858\n",
            "CNN_3  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.015775\n",
            "CNN_3  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.011962\n",
            "\n",
            "Test set: Average loss: 0.0422, Accuracy: 14812/15000 (98.75%)\n",
            "\n",
            "CNN_3  Epoch: 9 [0/45056 (0%)]\tLoss: 0.011365\n",
            "CNN_3  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.001473\n",
            "CNN_3  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.005237\n",
            "CNN_3  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.009447\n",
            "\n",
            "Test set: Average loss: 0.0459, Accuracy: 14802/15000 (98.68%)\n",
            "\n",
            "Training complete in 1m 53s\n",
            "\n",
            "   --  Run :  2\n",
            "CNN_3  Epoch: 0 [0/45056 (0%)]\tLoss: 2.302181\n",
            "CNN_3  Epoch: 0 [12800/45056 (28%)]\tLoss: 2.152527\n",
            "CNN_3  Epoch: 0 [25600/45056 (57%)]\tLoss: 2.541841\n",
            "CNN_3  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.344055\n",
            "\n",
            "Test set: Average loss: 0.2145, Accuracy: 14122/15000 (94.15%)\n",
            "\n",
            "new accuracy parameters saved 0.9414666666666667\n",
            "CNN_3  Epoch: 1 [0/45056 (0%)]\tLoss: 0.307789\n",
            "CNN_3  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.094084\n",
            "CNN_3  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.162231\n",
            "CNN_3  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.034587\n",
            "\n",
            "Test set: Average loss: 0.1097, Accuracy: 14505/15000 (96.70%)\n",
            "\n",
            "new accuracy parameters saved 0.967\n",
            "CNN_3  Epoch: 2 [0/45056 (0%)]\tLoss: 0.148807\n",
            "CNN_3  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.078854\n",
            "CNN_3  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.068073\n",
            "CNN_3  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.047779\n",
            "\n",
            "Test set: Average loss: 0.0990, Accuracy: 14513/15000 (96.75%)\n",
            "\n",
            "new accuracy parameters saved 0.9675333333333334\n",
            "CNN_3  Epoch: 3 [0/45056 (0%)]\tLoss: 0.072041\n",
            "CNN_3  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.106991\n",
            "CNN_3  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.059147\n",
            "CNN_3  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.007841\n",
            "\n",
            "Test set: Average loss: 0.0668, Accuracy: 14707/15000 (98.05%)\n",
            "\n",
            "new accuracy parameters saved 0.9804666666666667\n",
            "CNN_3  Epoch: 4 [0/45056 (0%)]\tLoss: 0.045534\n",
            "CNN_3  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.038613\n",
            "CNN_3  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.091649\n",
            "CNN_3  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.068976\n",
            "\n",
            "Test set: Average loss: 0.0450, Accuracy: 14773/15000 (98.49%)\n",
            "\n",
            "new accuracy parameters saved 0.9848666666666667\n",
            "CNN_3  Epoch: 5 [0/45056 (0%)]\tLoss: 0.014712\n",
            "CNN_3  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.025599\n",
            "CNN_3  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.077813\n",
            "CNN_3  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.360976\n",
            "\n",
            "Test set: Average loss: 0.0916, Accuracy: 14563/15000 (97.09%)\n",
            "\n",
            "CNN_3  Epoch: 6 [0/45056 (0%)]\tLoss: 0.055864\n",
            "CNN_3  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.029411\n",
            "CNN_3  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.054944\n",
            "CNN_3  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.093267\n",
            "\n",
            "Test set: Average loss: 0.0666, Accuracy: 14681/15000 (97.87%)\n",
            "\n",
            "CNN_3  Epoch: 7 [0/45056 (0%)]\tLoss: 0.068216\n",
            "CNN_3  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.030253\n",
            "CNN_3  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.064595\n",
            "CNN_3  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.075908\n",
            "\n",
            "Test set: Average loss: 0.0695, Accuracy: 14675/15000 (97.83%)\n",
            "\n",
            "CNN_3  Epoch: 8 [0/45056 (0%)]\tLoss: 0.034587\n",
            "CNN_3  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.013804\n",
            "CNN_3  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.009181\n",
            "CNN_3  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.016614\n",
            "\n",
            "Test set: Average loss: 0.0415, Accuracy: 14803/15000 (98.69%)\n",
            "\n",
            "new accuracy parameters saved 0.9868666666666667\n",
            "CNN_3  Epoch: 9 [0/45056 (0%)]\tLoss: 0.041372\n",
            "CNN_3  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.030218\n",
            "CNN_3  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.005160\n",
            "CNN_3  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.043040\n",
            "\n",
            "Test set: Average loss: 0.0370, Accuracy: 14836/15000 (98.91%)\n",
            "\n",
            "new accuracy parameters saved 0.9890666666666666\n",
            "Training complete in 1m 52s\n",
            "\n",
            "   --  Run :  3\n",
            "CNN_3  Epoch: 0 [0/45056 (0%)]\tLoss: 2.304762\n",
            "CNN_3  Epoch: 0 [12800/45056 (28%)]\tLoss: 2.181115\n",
            "CNN_3  Epoch: 0 [25600/45056 (57%)]\tLoss: 0.861630\n",
            "CNN_3  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.430178\n",
            "\n",
            "Test set: Average loss: 0.3369, Accuracy: 13396/15000 (89.31%)\n",
            "\n",
            "new accuracy parameters saved 0.8930666666666667\n",
            "CNN_3  Epoch: 1 [0/45056 (0%)]\tLoss: 0.283224\n",
            "CNN_3  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.191068\n",
            "CNN_3  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.104252\n",
            "CNN_3  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.118479\n",
            "\n",
            "Test set: Average loss: 0.1076, Accuracy: 14486/15000 (96.57%)\n",
            "\n",
            "new accuracy parameters saved 0.9657333333333333\n",
            "CNN_3  Epoch: 2 [0/45056 (0%)]\tLoss: 0.093755\n",
            "CNN_3  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.097302\n",
            "CNN_3  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.111522\n",
            "CNN_3  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.108921\n",
            "\n",
            "Test set: Average loss: 0.1211, Accuracy: 14457/15000 (96.38%)\n",
            "\n",
            "CNN_3  Epoch: 3 [0/45056 (0%)]\tLoss: 0.139406\n",
            "CNN_3  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.057797\n",
            "CNN_3  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.080959\n",
            "CNN_3  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.045400\n",
            "\n",
            "Test set: Average loss: 0.0532, Accuracy: 14767/15000 (98.45%)\n",
            "\n",
            "new accuracy parameters saved 0.9844666666666667\n",
            "CNN_3  Epoch: 4 [0/45056 (0%)]\tLoss: 0.053477\n",
            "CNN_3  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.065944\n",
            "CNN_3  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.049324\n",
            "CNN_3  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.074205\n",
            "\n",
            "Test set: Average loss: 0.0486, Accuracy: 14761/15000 (98.41%)\n",
            "\n",
            "CNN_3  Epoch: 5 [0/45056 (0%)]\tLoss: 0.032071\n",
            "CNN_3  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.025584\n",
            "CNN_3  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.024323\n",
            "CNN_3  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.033983\n",
            "\n",
            "Test set: Average loss: 0.0465, Accuracy: 14768/15000 (98.45%)\n",
            "\n",
            "new accuracy parameters saved 0.9845333333333334\n",
            "CNN_3  Epoch: 6 [0/45056 (0%)]\tLoss: 0.032694\n",
            "CNN_3  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.011683\n",
            "CNN_3  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.023449\n",
            "CNN_3  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.013061\n",
            "\n",
            "Test set: Average loss: 0.0429, Accuracy: 14804/15000 (98.69%)\n",
            "\n",
            "new accuracy parameters saved 0.9869333333333333\n",
            "CNN_3  Epoch: 7 [0/45056 (0%)]\tLoss: 0.018522\n",
            "CNN_3  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.051866\n",
            "CNN_3  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.027717\n",
            "CNN_3  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.029891\n",
            "\n",
            "Test set: Average loss: 0.0431, Accuracy: 14800/15000 (98.67%)\n",
            "\n",
            "CNN_3  Epoch: 8 [0/45056 (0%)]\tLoss: 0.031949\n",
            "CNN_3  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.012195\n",
            "CNN_3  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.021412\n",
            "CNN_3  Epoch: 8 [38400/45056 (85%)]\tLoss: 2.496947\n",
            "\n",
            "Test set: Average loss: 0.0848, Accuracy: 14623/15000 (97.49%)\n",
            "\n",
            "CNN_3  Epoch: 9 [0/45056 (0%)]\tLoss: 0.097112\n",
            "CNN_3  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.059570\n",
            "CNN_3  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.018196\n",
            "CNN_3  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.038582\n",
            "\n",
            "Test set: Average loss: 0.0571, Accuracy: 14728/15000 (98.19%)\n",
            "\n",
            "Training complete in 1m 52s\n",
            "\n",
            "   --  Run :  4\n",
            "CNN_3  Epoch: 0 [0/45056 (0%)]\tLoss: 2.307709\n",
            "CNN_3  Epoch: 0 [12800/45056 (28%)]\tLoss: 2.249716\n",
            "CNN_3  Epoch: 0 [25600/45056 (57%)]\tLoss: 1.530184\n",
            "CNN_3  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.255340\n",
            "\n",
            "Test set: Average loss: 0.2492, Accuracy: 13848/15000 (92.32%)\n",
            "\n",
            "new accuracy parameters saved 0.9232\n",
            "CNN_3  Epoch: 1 [0/45056 (0%)]\tLoss: 0.288132\n",
            "CNN_3  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.165894\n",
            "CNN_3  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.099603\n",
            "CNN_3  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.123267\n",
            "\n",
            "Test set: Average loss: 0.2400, Accuracy: 13865/15000 (92.43%)\n",
            "\n",
            "new accuracy parameters saved 0.9243333333333333\n",
            "CNN_3  Epoch: 2 [0/45056 (0%)]\tLoss: 0.139173\n",
            "CNN_3  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.176587\n",
            "CNN_3  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.065699\n",
            "CNN_3  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.035155\n",
            "\n",
            "Test set: Average loss: 0.0865, Accuracy: 14585/15000 (97.23%)\n",
            "\n",
            "new accuracy parameters saved 0.9723333333333334\n",
            "CNN_3  Epoch: 3 [0/45056 (0%)]\tLoss: 0.074058\n",
            "CNN_3  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.052481\n",
            "CNN_3  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.072276\n",
            "CNN_3  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.066405\n",
            "\n",
            "Test set: Average loss: 0.0584, Accuracy: 14734/15000 (98.23%)\n",
            "\n",
            "new accuracy parameters saved 0.9822666666666666\n",
            "CNN_3  Epoch: 4 [0/45056 (0%)]\tLoss: 0.067795\n",
            "CNN_3  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.039406\n",
            "CNN_3  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.062993\n",
            "CNN_3  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.016406\n",
            "\n",
            "Test set: Average loss: 0.0685, Accuracy: 14689/15000 (97.93%)\n",
            "\n",
            "CNN_3  Epoch: 5 [0/45056 (0%)]\tLoss: 0.015744\n",
            "CNN_3  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.011570\n",
            "CNN_3  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.021480\n",
            "CNN_3  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.061781\n",
            "\n",
            "Test set: Average loss: 0.0555, Accuracy: 14768/15000 (98.45%)\n",
            "\n",
            "new accuracy parameters saved 0.9845333333333334\n",
            "CNN_3  Epoch: 6 [0/45056 (0%)]\tLoss: 0.022846\n",
            "CNN_3  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.003418\n",
            "CNN_3  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.002932\n",
            "CNN_3  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.092763\n",
            "\n",
            "Test set: Average loss: 0.0403, Accuracy: 14814/15000 (98.76%)\n",
            "\n",
            "new accuracy parameters saved 0.9876\n",
            "CNN_3  Epoch: 7 [0/45056 (0%)]\tLoss: 0.009658\n",
            "CNN_3  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.011149\n",
            "CNN_3  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.032434\n",
            "CNN_3  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.006931\n",
            "\n",
            "Test set: Average loss: 0.0468, Accuracy: 14782/15000 (98.55%)\n",
            "\n",
            "CNN_3  Epoch: 8 [0/45056 (0%)]\tLoss: 0.004417\n",
            "CNN_3  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.017731\n",
            "CNN_3  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.006117\n",
            "CNN_3  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.013604\n",
            "\n",
            "Test set: Average loss: 0.0576, Accuracy: 14733/15000 (98.22%)\n",
            "\n",
            "CNN_3  Epoch: 9 [0/45056 (0%)]\tLoss: 0.010270\n",
            "CNN_3  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.007589\n",
            "CNN_3  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.046364\n",
            "CNN_3  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.010793\n",
            "\n",
            "Test set: Average loss: 0.0369, Accuracy: 14819/15000 (98.79%)\n",
            "\n",
            "new accuracy parameters saved 0.9879333333333333\n",
            "Training complete in 1m 52s\n",
            "\n",
            "   --  Run :  5\n",
            "CNN_3  Epoch: 0 [0/45056 (0%)]\tLoss: 2.298085\n",
            "CNN_3  Epoch: 0 [12800/45056 (28%)]\tLoss: 2.186016\n",
            "CNN_3  Epoch: 0 [25600/45056 (57%)]\tLoss: 1.222403\n",
            "CNN_3  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.235916\n",
            "\n",
            "Test set: Average loss: 0.2323, Accuracy: 13888/15000 (92.59%)\n",
            "\n",
            "new accuracy parameters saved 0.9258666666666666\n",
            "CNN_3  Epoch: 1 [0/45056 (0%)]\tLoss: 0.263338\n",
            "CNN_3  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.203945\n",
            "CNN_3  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.047355\n",
            "CNN_3  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.074911\n",
            "\n",
            "Test set: Average loss: 0.1180, Accuracy: 14431/15000 (96.21%)\n",
            "\n",
            "new accuracy parameters saved 0.9620666666666666\n",
            "CNN_3  Epoch: 2 [0/45056 (0%)]\tLoss: 0.141810\n",
            "CNN_3  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.092535\n",
            "CNN_3  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.051874\n",
            "CNN_3  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.071733\n",
            "\n",
            "Test set: Average loss: 0.5835, Accuracy: 12646/15000 (84.31%)\n",
            "\n",
            "CNN_3  Epoch: 3 [0/45056 (0%)]\tLoss: 0.539933\n",
            "CNN_3  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.055120\n",
            "CNN_3  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.048598\n",
            "CNN_3  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.154666\n",
            "\n",
            "Test set: Average loss: 0.0543, Accuracy: 14739/15000 (98.26%)\n",
            "\n",
            "new accuracy parameters saved 0.9826\n",
            "CNN_3  Epoch: 4 [0/45056 (0%)]\tLoss: 0.043460\n",
            "CNN_3  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.044867\n",
            "CNN_3  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.050903\n",
            "CNN_3  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.040105\n",
            "\n",
            "Test set: Average loss: 0.0770, Accuracy: 14625/15000 (97.50%)\n",
            "\n",
            "CNN_3  Epoch: 5 [0/45056 (0%)]\tLoss: 0.152596\n",
            "CNN_3  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.033355\n",
            "CNN_3  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.022089\n",
            "CNN_3  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.062274\n",
            "\n",
            "Test set: Average loss: 0.0503, Accuracy: 14762/15000 (98.41%)\n",
            "\n",
            "new accuracy parameters saved 0.9841333333333333\n",
            "CNN_3  Epoch: 6 [0/45056 (0%)]\tLoss: 0.020245\n",
            "CNN_3  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.149684\n",
            "CNN_3  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.007953\n",
            "CNN_3  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.032991\n",
            "\n",
            "Test set: Average loss: 0.0448, Accuracy: 14798/15000 (98.65%)\n",
            "\n",
            "new accuracy parameters saved 0.9865333333333334\n",
            "CNN_3  Epoch: 7 [0/45056 (0%)]\tLoss: 0.004916\n",
            "CNN_3  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.015947\n",
            "CNN_3  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.047551\n",
            "CNN_3  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.013175\n",
            "\n",
            "Test set: Average loss: 0.0385, Accuracy: 14807/15000 (98.71%)\n",
            "\n",
            "new accuracy parameters saved 0.9871333333333333\n",
            "CNN_3  Epoch: 8 [0/45056 (0%)]\tLoss: 0.005241\n",
            "CNN_3  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.077658\n",
            "CNN_3  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.005572\n",
            "CNN_3  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.008439\n",
            "\n",
            "Test set: Average loss: 0.0371, Accuracy: 14835/15000 (98.90%)\n",
            "\n",
            "new accuracy parameters saved 0.989\n",
            "CNN_3  Epoch: 9 [0/45056 (0%)]\tLoss: 0.137942\n",
            "CNN_3  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.010559\n",
            "CNN_3  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.011553\n",
            "CNN_3  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.006338\n",
            "\n",
            "Test set: Average loss: 0.0390, Accuracy: 14824/15000 (98.83%)\n",
            "\n",
            "Training complete in 1m 52s\n",
            "\n",
            "   --  Run :  6\n",
            "CNN_3  Epoch: 0 [0/45056 (0%)]\tLoss: 2.301404\n",
            "CNN_3  Epoch: 0 [12800/45056 (28%)]\tLoss: 2.072329\n",
            "CNN_3  Epoch: 0 [25600/45056 (57%)]\tLoss: 1.216116\n",
            "CNN_3  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.429361\n",
            "\n",
            "Test set: Average loss: 0.2810, Accuracy: 13670/15000 (91.13%)\n",
            "\n",
            "new accuracy parameters saved 0.9113333333333333\n",
            "CNN_3  Epoch: 1 [0/45056 (0%)]\tLoss: 0.381986\n",
            "CNN_3  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.108101\n",
            "CNN_3  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.081342\n",
            "CNN_3  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.133790\n",
            "\n",
            "Test set: Average loss: 0.1491, Accuracy: 14290/15000 (95.27%)\n",
            "\n",
            "new accuracy parameters saved 0.9526666666666667\n",
            "CNN_3  Epoch: 2 [0/45056 (0%)]\tLoss: 0.098194\n",
            "CNN_3  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.115441\n",
            "CNN_3  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.090361\n",
            "CNN_3  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.115606\n",
            "\n",
            "Test set: Average loss: 0.0721, Accuracy: 14666/15000 (97.77%)\n",
            "\n",
            "new accuracy parameters saved 0.9777333333333333\n",
            "CNN_3  Epoch: 3 [0/45056 (0%)]\tLoss: 0.043391\n",
            "CNN_3  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.037571\n",
            "CNN_3  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.034136\n",
            "CNN_3  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.150622\n",
            "\n",
            "Test set: Average loss: 0.0545, Accuracy: 14744/15000 (98.29%)\n",
            "\n",
            "new accuracy parameters saved 0.9829333333333333\n",
            "CNN_3  Epoch: 4 [0/45056 (0%)]\tLoss: 0.038946\n",
            "CNN_3  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.064657\n",
            "CNN_3  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.035905\n",
            "CNN_3  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.072892\n",
            "\n",
            "Test set: Average loss: 0.0512, Accuracy: 14770/15000 (98.47%)\n",
            "\n",
            "new accuracy parameters saved 0.9846666666666667\n",
            "CNN_3  Epoch: 5 [0/45056 (0%)]\tLoss: 0.058002\n",
            "CNN_3  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.051752\n",
            "CNN_3  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.069413\n",
            "CNN_3  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.073925\n",
            "\n",
            "Test set: Average loss: 0.0654, Accuracy: 14697/15000 (97.98%)\n",
            "\n",
            "CNN_3  Epoch: 6 [0/45056 (0%)]\tLoss: 0.062110\n",
            "CNN_3  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.006106\n",
            "CNN_3  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.049953\n",
            "CNN_3  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.019403\n",
            "\n",
            "Test set: Average loss: 0.0448, Accuracy: 14795/15000 (98.63%)\n",
            "\n",
            "new accuracy parameters saved 0.9863333333333333\n",
            "CNN_3  Epoch: 7 [0/45056 (0%)]\tLoss: 0.035117\n",
            "CNN_3  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.042182\n",
            "CNN_3  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.041741\n",
            "CNN_3  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.029053\n",
            "\n",
            "Test set: Average loss: 0.0440, Accuracy: 14790/15000 (98.60%)\n",
            "\n",
            "CNN_3  Epoch: 8 [0/45056 (0%)]\tLoss: 0.022741\n",
            "CNN_3  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.047255\n",
            "CNN_3  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.003092\n",
            "CNN_3  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.015697\n",
            "\n",
            "Test set: Average loss: 0.0373, Accuracy: 14834/15000 (98.89%)\n",
            "\n",
            "new accuracy parameters saved 0.9889333333333333\n",
            "CNN_3  Epoch: 9 [0/45056 (0%)]\tLoss: 0.015319\n",
            "CNN_3  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.038111\n",
            "CNN_3  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.007006\n",
            "CNN_3  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.028571\n",
            "\n",
            "Test set: Average loss: 0.0389, Accuracy: 14819/15000 (98.79%)\n",
            "\n",
            "Training complete in 1m 52s\n",
            "\n",
            "   --  Run :  7\n",
            "CNN_3  Epoch: 0 [0/45056 (0%)]\tLoss: 2.298952\n",
            "CNN_3  Epoch: 0 [12800/45056 (28%)]\tLoss: 2.192933\n",
            "CNN_3  Epoch: 0 [25600/45056 (57%)]\tLoss: 1.843028\n",
            "CNN_3  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.340906\n",
            "\n",
            "Test set: Average loss: 0.2876, Accuracy: 13643/15000 (90.95%)\n",
            "\n",
            "new accuracy parameters saved 0.9095333333333333\n",
            "CNN_3  Epoch: 1 [0/45056 (0%)]\tLoss: 0.236795\n",
            "CNN_3  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.121647\n",
            "CNN_3  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.085769\n",
            "CNN_3  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.092508\n",
            "\n",
            "Test set: Average loss: 0.0906, Accuracy: 14562/15000 (97.08%)\n",
            "\n",
            "new accuracy parameters saved 0.9708\n",
            "CNN_3  Epoch: 2 [0/45056 (0%)]\tLoss: 0.157301\n",
            "CNN_3  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.095794\n",
            "CNN_3  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.042179\n",
            "CNN_3  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.034289\n",
            "\n",
            "Test set: Average loss: 0.1183, Accuracy: 14422/15000 (96.15%)\n",
            "\n",
            "CNN_3  Epoch: 3 [0/45056 (0%)]\tLoss: 0.102599\n",
            "CNN_3  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.045040\n",
            "CNN_3  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.141552\n",
            "CNN_3  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.042631\n",
            "\n",
            "Test set: Average loss: 0.0681, Accuracy: 14672/15000 (97.81%)\n",
            "\n",
            "new accuracy parameters saved 0.9781333333333333\n",
            "CNN_3  Epoch: 4 [0/45056 (0%)]\tLoss: 0.016179\n",
            "CNN_3  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.074655\n",
            "CNN_3  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.048596\n",
            "CNN_3  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.069127\n",
            "\n",
            "Test set: Average loss: 0.0533, Accuracy: 14761/15000 (98.41%)\n",
            "\n",
            "new accuracy parameters saved 0.9840666666666666\n",
            "CNN_3  Epoch: 5 [0/45056 (0%)]\tLoss: 0.036121\n",
            "CNN_3  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.095638\n",
            "CNN_3  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.039037\n",
            "CNN_3  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.021638\n",
            "\n",
            "Test set: Average loss: 0.0506, Accuracy: 14769/15000 (98.46%)\n",
            "\n",
            "new accuracy parameters saved 0.9846\n",
            "CNN_3  Epoch: 6 [0/45056 (0%)]\tLoss: 0.117924\n",
            "CNN_3  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.016803\n",
            "CNN_3  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.023664\n",
            "CNN_3  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.027966\n",
            "\n",
            "Test set: Average loss: 0.0490, Accuracy: 14770/15000 (98.47%)\n",
            "\n",
            "new accuracy parameters saved 0.9846666666666667\n",
            "CNN_3  Epoch: 7 [0/45056 (0%)]\tLoss: 0.028950\n",
            "CNN_3  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.026264\n",
            "CNN_3  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.010017\n",
            "CNN_3  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.009160\n",
            "\n",
            "Test set: Average loss: 0.0358, Accuracy: 14834/15000 (98.89%)\n",
            "\n",
            "new accuracy parameters saved 0.9889333333333333\n",
            "CNN_3  Epoch: 8 [0/45056 (0%)]\tLoss: 0.020116\n",
            "CNN_3  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.017485\n",
            "CNN_3  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.081034\n",
            "CNN_3  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.074493\n",
            "\n",
            "Test set: Average loss: 0.0499, Accuracy: 14774/15000 (98.49%)\n",
            "\n",
            "CNN_3  Epoch: 9 [0/45056 (0%)]\tLoss: 0.050486\n",
            "CNN_3  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.031082\n",
            "CNN_3  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.039398\n",
            "CNN_3  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.013670\n",
            "\n",
            "Test set: Average loss: 0.0389, Accuracy: 14821/15000 (98.81%)\n",
            "\n",
            "Training complete in 1m 52s\n",
            "\n",
            "   --  Run :  8\n",
            "CNN_3  Epoch: 0 [0/45056 (0%)]\tLoss: 2.303519\n",
            "CNN_3  Epoch: 0 [12800/45056 (28%)]\tLoss: 2.187892\n",
            "CNN_3  Epoch: 0 [25600/45056 (57%)]\tLoss: 2.094219\n",
            "CNN_3  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.343180\n",
            "\n",
            "Test set: Average loss: 0.1963, Accuracy: 14178/15000 (94.52%)\n",
            "\n",
            "new accuracy parameters saved 0.9452\n",
            "CNN_3  Epoch: 1 [0/45056 (0%)]\tLoss: 0.184361\n",
            "CNN_3  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.106323\n",
            "CNN_3  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.110844\n",
            "CNN_3  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.081916\n",
            "\n",
            "Test set: Average loss: 0.1341, Accuracy: 14365/15000 (95.77%)\n",
            "\n",
            "new accuracy parameters saved 0.9576666666666667\n",
            "CNN_3  Epoch: 2 [0/45056 (0%)]\tLoss: 0.065795\n",
            "CNN_3  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.109572\n",
            "CNN_3  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.112046\n",
            "CNN_3  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.081328\n",
            "\n",
            "Test set: Average loss: 0.0787, Accuracy: 14629/15000 (97.53%)\n",
            "\n",
            "new accuracy parameters saved 0.9752666666666666\n",
            "CNN_3  Epoch: 3 [0/45056 (0%)]\tLoss: 0.033337\n",
            "CNN_3  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.114232\n",
            "CNN_3  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.027787\n",
            "CNN_3  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.056535\n",
            "\n",
            "Test set: Average loss: 0.0522, Accuracy: 14743/15000 (98.29%)\n",
            "\n",
            "new accuracy parameters saved 0.9828666666666667\n",
            "CNN_3  Epoch: 4 [0/45056 (0%)]\tLoss: 0.013357\n",
            "CNN_3  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.020162\n",
            "CNN_3  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.049642\n",
            "CNN_3  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.061193\n",
            "\n",
            "Test set: Average loss: 0.0448, Accuracy: 14785/15000 (98.57%)\n",
            "\n",
            "new accuracy parameters saved 0.9856666666666667\n",
            "CNN_3  Epoch: 5 [0/45056 (0%)]\tLoss: 0.023853\n",
            "CNN_3  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.015265\n",
            "CNN_3  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.043859\n",
            "CNN_3  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.028162\n",
            "\n",
            "Test set: Average loss: 0.0477, Accuracy: 14768/15000 (98.45%)\n",
            "\n",
            "CNN_3  Epoch: 6 [0/45056 (0%)]\tLoss: 0.063634\n",
            "CNN_3  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.060357\n",
            "CNN_3  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.020350\n",
            "CNN_3  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.050252\n",
            "\n",
            "Test set: Average loss: 0.0406, Accuracy: 14823/15000 (98.82%)\n",
            "\n",
            "new accuracy parameters saved 0.9882\n",
            "CNN_3  Epoch: 7 [0/45056 (0%)]\tLoss: 0.016739\n",
            "CNN_3  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.023062\n",
            "CNN_3  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.001422\n",
            "CNN_3  Epoch: 7 [38400/45056 (85%)]\tLoss: 2.860531\n",
            "\n",
            "Test set: Average loss: 0.0513, Accuracy: 14785/15000 (98.57%)\n",
            "\n",
            "CNN_3  Epoch: 8 [0/45056 (0%)]\tLoss: 0.027996\n",
            "CNN_3  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.032469\n",
            "CNN_3  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.016538\n",
            "CNN_3  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.009020\n",
            "\n",
            "Test set: Average loss: 0.0508, Accuracy: 14755/15000 (98.37%)\n",
            "\n",
            "CNN_3  Epoch: 9 [0/45056 (0%)]\tLoss: 0.025725\n",
            "CNN_3  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.030624\n",
            "CNN_3  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.002382\n",
            "CNN_3  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.036368\n",
            "\n",
            "Test set: Average loss: 0.0457, Accuracy: 14796/15000 (98.64%)\n",
            "\n",
            "Training complete in 1m 53s\n",
            "\n",
            "   --  Run :  9\n",
            "CNN_3  Epoch: 0 [0/45056 (0%)]\tLoss: 2.303444\n",
            "CNN_3  Epoch: 0 [12800/45056 (28%)]\tLoss: 2.143012\n",
            "CNN_3  Epoch: 0 [25600/45056 (57%)]\tLoss: 1.358224\n",
            "CNN_3  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.220562\n",
            "\n",
            "Test set: Average loss: 0.1966, Accuracy: 14089/15000 (93.93%)\n",
            "\n",
            "new accuracy parameters saved 0.9392666666666667\n",
            "CNN_3  Epoch: 1 [0/45056 (0%)]\tLoss: 0.178705\n",
            "CNN_3  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.180981\n",
            "CNN_3  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.120149\n",
            "CNN_3  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.104820\n",
            "\n",
            "Test set: Average loss: 0.1373, Accuracy: 14358/15000 (95.72%)\n",
            "\n",
            "new accuracy parameters saved 0.9572\n",
            "CNN_3  Epoch: 2 [0/45056 (0%)]\tLoss: 0.098417\n",
            "CNN_3  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.030765\n",
            "CNN_3  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.044434\n",
            "CNN_3  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.201818\n",
            "\n",
            "Test set: Average loss: 0.0729, Accuracy: 14668/15000 (97.79%)\n",
            "\n",
            "new accuracy parameters saved 0.9778666666666667\n",
            "CNN_3  Epoch: 3 [0/45056 (0%)]\tLoss: 0.041933\n",
            "CNN_3  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.075972\n",
            "CNN_3  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.032494\n",
            "CNN_3  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.033398\n",
            "\n",
            "Test set: Average loss: 0.0603, Accuracy: 14726/15000 (98.17%)\n",
            "\n",
            "new accuracy parameters saved 0.9817333333333333\n",
            "CNN_3  Epoch: 4 [0/45056 (0%)]\tLoss: 0.051272\n",
            "CNN_3  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.070437\n",
            "CNN_3  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.063173\n",
            "CNN_3  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.035867\n",
            "\n",
            "Test set: Average loss: 0.0517, Accuracy: 14743/15000 (98.29%)\n",
            "\n",
            "new accuracy parameters saved 0.9828666666666667\n",
            "CNN_3  Epoch: 5 [0/45056 (0%)]\tLoss: 0.088053\n",
            "CNN_3  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.045022\n",
            "CNN_3  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.080217\n",
            "CNN_3  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.014197\n",
            "\n",
            "Test set: Average loss: 0.0425, Accuracy: 14798/15000 (98.65%)\n",
            "\n",
            "new accuracy parameters saved 0.9865333333333334\n",
            "CNN_3  Epoch: 6 [0/45056 (0%)]\tLoss: 0.020856\n",
            "CNN_3  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.021440\n",
            "CNN_3  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.084733\n",
            "CNN_3  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.074797\n",
            "\n",
            "Test set: Average loss: 0.0565, Accuracy: 14748/15000 (98.32%)\n",
            "\n",
            "CNN_3  Epoch: 7 [0/45056 (0%)]\tLoss: 0.117956\n",
            "CNN_3  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.001561\n",
            "CNN_3  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.013016\n",
            "CNN_3  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.006587\n",
            "\n",
            "Test set: Average loss: 0.0412, Accuracy: 14807/15000 (98.71%)\n",
            "\n",
            "new accuracy parameters saved 0.9871333333333333\n",
            "CNN_3  Epoch: 8 [0/45056 (0%)]\tLoss: 0.032597\n",
            "CNN_3  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.015006\n",
            "CNN_3  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.008514\n",
            "CNN_3  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.004297\n",
            "\n",
            "Test set: Average loss: 0.0367, Accuracy: 14822/15000 (98.81%)\n",
            "\n",
            "new accuracy parameters saved 0.9881333333333333\n",
            "CNN_3  Epoch: 9 [0/45056 (0%)]\tLoss: 0.018546\n",
            "CNN_3  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.017900\n",
            "CNN_3  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.008887\n",
            "CNN_3  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.050155\n",
            "\n",
            "Test set: Average loss: 0.0424, Accuracy: 14803/15000 (98.69%)\n",
            "\n",
            "Training complete in 1m 53s\n",
            "\n",
            "   --  Run :  10\n",
            "CNN_3  Epoch: 0 [0/45056 (0%)]\tLoss: 2.299166\n",
            "CNN_3  Epoch: 0 [12800/45056 (28%)]\tLoss: 2.134989\n",
            "CNN_3  Epoch: 0 [25600/45056 (57%)]\tLoss: 1.436861\n",
            "CNN_3  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.758543\n",
            "\n",
            "Test set: Average loss: 0.3500, Accuracy: 13229/15000 (88.19%)\n",
            "\n",
            "new accuracy parameters saved 0.8819333333333333\n",
            "CNN_3  Epoch: 1 [0/45056 (0%)]\tLoss: 0.248247\n",
            "CNN_3  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.181740\n",
            "CNN_3  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.136025\n",
            "CNN_3  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.118137\n",
            "\n",
            "Test set: Average loss: 0.1357, Accuracy: 14380/15000 (95.87%)\n",
            "\n",
            "new accuracy parameters saved 0.9586666666666667\n",
            "CNN_3  Epoch: 2 [0/45056 (0%)]\tLoss: 0.105063\n",
            "CNN_3  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.065645\n",
            "CNN_3  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.117611\n",
            "CNN_3  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.064300\n",
            "\n",
            "Test set: Average loss: 0.0724, Accuracy: 14667/15000 (97.78%)\n",
            "\n",
            "new accuracy parameters saved 0.9778\n",
            "CNN_3  Epoch: 3 [0/45056 (0%)]\tLoss: 0.028886\n",
            "CNN_3  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.262927\n",
            "CNN_3  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.160686\n",
            "CNN_3  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.108142\n",
            "\n",
            "Test set: Average loss: 0.0613, Accuracy: 14705/15000 (98.03%)\n",
            "\n",
            "new accuracy parameters saved 0.9803333333333333\n",
            "CNN_3  Epoch: 4 [0/45056 (0%)]\tLoss: 0.018512\n",
            "CNN_3  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.023862\n",
            "CNN_3  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.022837\n",
            "CNN_3  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.040968\n",
            "\n",
            "Test set: Average loss: 0.0801, Accuracy: 14637/15000 (97.58%)\n",
            "\n",
            "CNN_3  Epoch: 5 [0/45056 (0%)]\tLoss: 0.077540\n",
            "CNN_3  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.106761\n",
            "CNN_3  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.054616\n",
            "CNN_3  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.031304\n",
            "\n",
            "Test set: Average loss: 0.0625, Accuracy: 14707/15000 (98.05%)\n",
            "\n",
            "new accuracy parameters saved 0.9804666666666667\n",
            "CNN_3  Epoch: 6 [0/45056 (0%)]\tLoss: 0.031494\n",
            "CNN_3  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.012140\n",
            "CNN_3  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.023389\n",
            "CNN_3  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.034080\n",
            "\n",
            "Test set: Average loss: 0.0428, Accuracy: 14797/15000 (98.65%)\n",
            "\n",
            "new accuracy parameters saved 0.9864666666666667\n",
            "CNN_3  Epoch: 7 [0/45056 (0%)]\tLoss: 0.051054\n",
            "CNN_3  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.028448\n",
            "CNN_3  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.030173\n",
            "CNN_3  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.010194\n",
            "\n",
            "Test set: Average loss: 0.0424, Accuracy: 14813/15000 (98.75%)\n",
            "\n",
            "new accuracy parameters saved 0.9875333333333334\n",
            "CNN_3  Epoch: 8 [0/45056 (0%)]\tLoss: 0.021382\n",
            "CNN_3  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.018300\n",
            "CNN_3  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.018910\n",
            "CNN_3  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.104414\n",
            "\n",
            "Test set: Average loss: 0.0989, Accuracy: 14566/15000 (97.11%)\n",
            "\n",
            "CNN_3  Epoch: 9 [0/45056 (0%)]\tLoss: 0.073173\n",
            "CNN_3  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.010208\n",
            "CNN_3  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.023073\n",
            "CNN_3  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.031022\n",
            "\n",
            "Test set: Average loss: 0.0432, Accuracy: 14798/15000 (98.65%)\n",
            "\n",
            "Training complete in 1m 53s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OBRH6Vd8aTAG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### MLP"
      ]
    },
    {
      "metadata": {
        "id": "ZOKaKG_XCzzQ",
        "colab_type": "code",
        "outputId": "08c87c67-c137-4e08-d69b-9bdf66d7cee1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 14211
        }
      },
      "cell_type": "code",
      "source": [
        "MultipleRun(runs,epochs,MLP_2L())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   --  Run :  1\n",
            "MLP_2L  Epoch: 0 [0/45056 (0%)]\tLoss: 2.299602\n",
            "MLP_2L  Epoch: 0 [12800/45056 (28%)]\tLoss: 0.564381\n",
            "MLP_2L  Epoch: 0 [25600/45056 (57%)]\tLoss: 0.372678\n",
            "MLP_2L  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.328719\n",
            "\n",
            "Test set: Average loss: 0.3264, Accuracy: 13579/15000 (90.53%)\n",
            "\n",
            "new accuracy parameters saved 0.9052666666666667\n",
            "MLP_2L  Epoch: 1 [0/45056 (0%)]\tLoss: 0.319893\n",
            "MLP_2L  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.466304\n",
            "MLP_2L  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.342601\n",
            "MLP_2L  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.213206\n",
            "\n",
            "Test set: Average loss: 0.2961, Accuracy: 13694/15000 (91.29%)\n",
            "\n",
            "new accuracy parameters saved 0.9129333333333334\n",
            "MLP_2L  Epoch: 2 [0/45056 (0%)]\tLoss: 0.171679\n",
            "MLP_2L  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.318711\n",
            "MLP_2L  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.155034\n",
            "MLP_2L  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.201595\n",
            "\n",
            "Test set: Average loss: 0.2037, Accuracy: 14116/15000 (94.11%)\n",
            "\n",
            "new accuracy parameters saved 0.9410666666666667\n",
            "MLP_2L  Epoch: 3 [0/45056 (0%)]\tLoss: 0.209388\n",
            "MLP_2L  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.232062\n",
            "MLP_2L  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.131379\n",
            "MLP_2L  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.153489\n",
            "\n",
            "Test set: Average loss: 0.1751, Accuracy: 14243/15000 (94.95%)\n",
            "\n",
            "new accuracy parameters saved 0.9495333333333333\n",
            "MLP_2L  Epoch: 4 [0/45056 (0%)]\tLoss: 0.239418\n",
            "MLP_2L  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.114193\n",
            "MLP_2L  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.109186\n",
            "MLP_2L  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.183184\n",
            "\n",
            "Test set: Average loss: 0.1421, Accuracy: 14355/15000 (95.70%)\n",
            "\n",
            "new accuracy parameters saved 0.957\n",
            "MLP_2L  Epoch: 5 [0/45056 (0%)]\tLoss: 0.080342\n",
            "MLP_2L  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.130003\n",
            "MLP_2L  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.150985\n",
            "MLP_2L  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.044219\n",
            "\n",
            "Test set: Average loss: 0.1311, Accuracy: 14423/15000 (96.15%)\n",
            "\n",
            "new accuracy parameters saved 0.9615333333333334\n",
            "MLP_2L  Epoch: 6 [0/45056 (0%)]\tLoss: 0.034238\n",
            "MLP_2L  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.081056\n",
            "MLP_2L  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.055225\n",
            "MLP_2L  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.152700\n",
            "\n",
            "Test set: Average loss: 0.1175, Accuracy: 14474/15000 (96.49%)\n",
            "\n",
            "new accuracy parameters saved 0.9649333333333333\n",
            "MLP_2L  Epoch: 7 [0/45056 (0%)]\tLoss: 0.121104\n",
            "MLP_2L  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.090301\n",
            "MLP_2L  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.071197\n",
            "MLP_2L  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.067337\n",
            "\n",
            "Test set: Average loss: 0.1080, Accuracy: 14534/15000 (96.89%)\n",
            "\n",
            "new accuracy parameters saved 0.9689333333333333\n",
            "MLP_2L  Epoch: 8 [0/45056 (0%)]\tLoss: 0.072582\n",
            "MLP_2L  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.031947\n",
            "MLP_2L  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.115825\n",
            "MLP_2L  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.044336\n",
            "\n",
            "Test set: Average loss: 0.0996, Accuracy: 14575/15000 (97.17%)\n",
            "\n",
            "new accuracy parameters saved 0.9716666666666667\n",
            "MLP_2L  Epoch: 9 [0/45056 (0%)]\tLoss: 0.074843\n",
            "MLP_2L  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.081366\n",
            "MLP_2L  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.098036\n",
            "MLP_2L  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.031722\n",
            "\n",
            "Test set: Average loss: 0.0924, Accuracy: 14590/15000 (97.27%)\n",
            "\n",
            "new accuracy parameters saved 0.9726666666666667\n",
            "Training complete in 1m 6s\n",
            "\n",
            "   --  Run :  2\n",
            "MLP_2L  Epoch: 0 [0/45056 (0%)]\tLoss: 2.298192\n",
            "MLP_2L  Epoch: 0 [12800/45056 (28%)]\tLoss: 0.626613\n",
            "MLP_2L  Epoch: 0 [25600/45056 (57%)]\tLoss: 0.403535\n",
            "MLP_2L  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.459809\n",
            "\n",
            "Test set: Average loss: 0.3532, Accuracy: 13426/15000 (89.51%)\n",
            "\n",
            "new accuracy parameters saved 0.8950666666666667\n",
            "MLP_2L  Epoch: 1 [0/45056 (0%)]\tLoss: 0.288402\n",
            "MLP_2L  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.389743\n",
            "MLP_2L  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.344634\n",
            "MLP_2L  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.240249\n",
            "\n",
            "Test set: Average loss: 0.2579, Accuracy: 13850/15000 (92.33%)\n",
            "\n",
            "new accuracy parameters saved 0.9233333333333333\n",
            "MLP_2L  Epoch: 2 [0/45056 (0%)]\tLoss: 0.328550\n",
            "MLP_2L  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.282391\n",
            "MLP_2L  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.305469\n",
            "MLP_2L  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.160544\n",
            "\n",
            "Test set: Average loss: 0.2190, Accuracy: 14048/15000 (93.65%)\n",
            "\n",
            "new accuracy parameters saved 0.9365333333333333\n",
            "MLP_2L  Epoch: 3 [0/45056 (0%)]\tLoss: 0.237096\n",
            "MLP_2L  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.123236\n",
            "MLP_2L  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.207425\n",
            "MLP_2L  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.145824\n",
            "\n",
            "Test set: Average loss: 0.1639, Accuracy: 14302/15000 (95.35%)\n",
            "\n",
            "new accuracy parameters saved 0.9534666666666667\n",
            "MLP_2L  Epoch: 4 [0/45056 (0%)]\tLoss: 0.127646\n",
            "MLP_2L  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.178504\n",
            "MLP_2L  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.123091\n",
            "MLP_2L  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.112282\n",
            "\n",
            "Test set: Average loss: 0.1519, Accuracy: 14338/15000 (95.59%)\n",
            "\n",
            "new accuracy parameters saved 0.9558666666666666\n",
            "MLP_2L  Epoch: 5 [0/45056 (0%)]\tLoss: 0.086990\n",
            "MLP_2L  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.156195\n",
            "MLP_2L  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.192383\n",
            "MLP_2L  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.077125\n",
            "\n",
            "Test set: Average loss: 0.1310, Accuracy: 14423/15000 (96.15%)\n",
            "\n",
            "new accuracy parameters saved 0.9615333333333334\n",
            "MLP_2L  Epoch: 6 [0/45056 (0%)]\tLoss: 0.072170\n",
            "MLP_2L  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.059709\n",
            "MLP_2L  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.108662\n",
            "MLP_2L  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.077495\n",
            "\n",
            "Test set: Average loss: 0.1150, Accuracy: 14477/15000 (96.51%)\n",
            "\n",
            "new accuracy parameters saved 0.9651333333333333\n",
            "MLP_2L  Epoch: 7 [0/45056 (0%)]\tLoss: 0.096023\n",
            "MLP_2L  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.086766\n",
            "MLP_2L  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.079494\n",
            "MLP_2L  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.059572\n",
            "\n",
            "Test set: Average loss: 0.1137, Accuracy: 14490/15000 (96.60%)\n",
            "\n",
            "new accuracy parameters saved 0.966\n",
            "MLP_2L  Epoch: 8 [0/45056 (0%)]\tLoss: 0.114130\n",
            "MLP_2L  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.041500\n",
            "MLP_2L  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.042431\n",
            "MLP_2L  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.020763\n",
            "\n",
            "Test set: Average loss: 0.1018, Accuracy: 14548/15000 (96.99%)\n",
            "\n",
            "new accuracy parameters saved 0.9698666666666667\n",
            "MLP_2L  Epoch: 9 [0/45056 (0%)]\tLoss: 0.075475\n",
            "MLP_2L  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.077383\n",
            "MLP_2L  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.150725\n",
            "MLP_2L  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.038213\n",
            "\n",
            "Test set: Average loss: 0.0924, Accuracy: 14610/15000 (97.40%)\n",
            "\n",
            "new accuracy parameters saved 0.974\n",
            "Training complete in 1m 6s\n",
            "\n",
            "   --  Run :  3\n",
            "MLP_2L  Epoch: 0 [0/45056 (0%)]\tLoss: 2.295503\n",
            "MLP_2L  Epoch: 0 [12800/45056 (28%)]\tLoss: 0.657250\n",
            "MLP_2L  Epoch: 0 [25600/45056 (57%)]\tLoss: 0.395898\n",
            "MLP_2L  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.353222\n",
            "\n",
            "Test set: Average loss: 0.3437, Accuracy: 13462/15000 (89.75%)\n",
            "\n",
            "new accuracy parameters saved 0.8974666666666666\n",
            "MLP_2L  Epoch: 1 [0/45056 (0%)]\tLoss: 0.303337\n",
            "MLP_2L  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.316461\n",
            "MLP_2L  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.191645\n",
            "MLP_2L  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.190075\n",
            "\n",
            "Test set: Average loss: 0.2560, Accuracy: 13908/15000 (92.72%)\n",
            "\n",
            "new accuracy parameters saved 0.9272\n",
            "MLP_2L  Epoch: 2 [0/45056 (0%)]\tLoss: 0.150479\n",
            "MLP_2L  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.258503\n",
            "MLP_2L  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.201265\n",
            "MLP_2L  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.205214\n",
            "\n",
            "Test set: Average loss: 0.2025, Accuracy: 14140/15000 (94.27%)\n",
            "\n",
            "new accuracy parameters saved 0.9426666666666667\n",
            "MLP_2L  Epoch: 3 [0/45056 (0%)]\tLoss: 0.146905\n",
            "MLP_2L  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.127916\n",
            "MLP_2L  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.114640\n",
            "MLP_2L  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.133692\n",
            "\n",
            "Test set: Average loss: 0.1813, Accuracy: 14215/15000 (94.77%)\n",
            "\n",
            "new accuracy parameters saved 0.9476666666666667\n",
            "MLP_2L  Epoch: 4 [0/45056 (0%)]\tLoss: 0.217201\n",
            "MLP_2L  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.103564\n",
            "MLP_2L  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.239437\n",
            "MLP_2L  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.125938\n",
            "\n",
            "Test set: Average loss: 0.1482, Accuracy: 14361/15000 (95.74%)\n",
            "\n",
            "new accuracy parameters saved 0.9574\n",
            "MLP_2L  Epoch: 5 [0/45056 (0%)]\tLoss: 0.169834\n",
            "MLP_2L  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.161762\n",
            "MLP_2L  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.108460\n",
            "MLP_2L  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.112120\n",
            "\n",
            "Test set: Average loss: 0.1357, Accuracy: 14404/15000 (96.03%)\n",
            "\n",
            "new accuracy parameters saved 0.9602666666666667\n",
            "MLP_2L  Epoch: 6 [0/45056 (0%)]\tLoss: 0.139804\n",
            "MLP_2L  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.130120\n",
            "MLP_2L  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.114370\n",
            "MLP_2L  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.077005\n",
            "\n",
            "Test set: Average loss: 0.1242, Accuracy: 14446/15000 (96.31%)\n",
            "\n",
            "new accuracy parameters saved 0.9630666666666666\n",
            "MLP_2L  Epoch: 7 [0/45056 (0%)]\tLoss: 0.082961\n",
            "MLP_2L  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.085292\n",
            "MLP_2L  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.058683\n",
            "MLP_2L  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.066581\n",
            "\n",
            "Test set: Average loss: 0.1074, Accuracy: 14528/15000 (96.85%)\n",
            "\n",
            "new accuracy parameters saved 0.9685333333333334\n",
            "MLP_2L  Epoch: 8 [0/45056 (0%)]\tLoss: 0.069885\n",
            "MLP_2L  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.104951\n",
            "MLP_2L  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.070024\n",
            "MLP_2L  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.081144\n",
            "\n",
            "Test set: Average loss: 0.1012, Accuracy: 14551/15000 (97.01%)\n",
            "\n",
            "new accuracy parameters saved 0.9700666666666666\n",
            "MLP_2L  Epoch: 9 [0/45056 (0%)]\tLoss: 0.095722\n",
            "MLP_2L  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.156194\n",
            "MLP_2L  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.054056\n",
            "MLP_2L  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.061730\n",
            "\n",
            "Test set: Average loss: 0.0940, Accuracy: 14579/15000 (97.19%)\n",
            "\n",
            "new accuracy parameters saved 0.9719333333333333\n",
            "Training complete in 1m 6s\n",
            "\n",
            "   --  Run :  4\n",
            "MLP_2L  Epoch: 0 [0/45056 (0%)]\tLoss: 2.307764\n",
            "MLP_2L  Epoch: 0 [12800/45056 (28%)]\tLoss: 0.649776\n",
            "MLP_2L  Epoch: 0 [25600/45056 (57%)]\tLoss: 0.509883\n",
            "MLP_2L  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.349267\n",
            "\n",
            "Test set: Average loss: 0.3496, Accuracy: 13410/15000 (89.40%)\n",
            "\n",
            "new accuracy parameters saved 0.894\n",
            "MLP_2L  Epoch: 1 [0/45056 (0%)]\tLoss: 0.361665\n",
            "MLP_2L  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.238724\n",
            "MLP_2L  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.312922\n",
            "MLP_2L  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.415961\n",
            "\n",
            "Test set: Average loss: 0.2480, Accuracy: 13893/15000 (92.62%)\n",
            "\n",
            "new accuracy parameters saved 0.9262\n",
            "MLP_2L  Epoch: 2 [0/45056 (0%)]\tLoss: 0.260452\n",
            "MLP_2L  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.233464\n",
            "MLP_2L  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.190497\n",
            "MLP_2L  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.192051\n",
            "\n",
            "Test set: Average loss: 0.2095, Accuracy: 14108/15000 (94.05%)\n",
            "\n",
            "new accuracy parameters saved 0.9405333333333333\n",
            "MLP_2L  Epoch: 3 [0/45056 (0%)]\tLoss: 0.292299\n",
            "MLP_2L  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.155108\n",
            "MLP_2L  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.173242\n",
            "MLP_2L  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.082684\n",
            "\n",
            "Test set: Average loss: 0.1788, Accuracy: 14233/15000 (94.89%)\n",
            "\n",
            "new accuracy parameters saved 0.9488666666666666\n",
            "MLP_2L  Epoch: 4 [0/45056 (0%)]\tLoss: 0.348390\n",
            "MLP_2L  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.119568\n",
            "MLP_2L  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.122638\n",
            "MLP_2L  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.189107\n",
            "\n",
            "Test set: Average loss: 0.1512, Accuracy: 14347/15000 (95.65%)\n",
            "\n",
            "new accuracy parameters saved 0.9564666666666667\n",
            "MLP_2L  Epoch: 5 [0/45056 (0%)]\tLoss: 0.073522\n",
            "MLP_2L  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.081962\n",
            "MLP_2L  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.103105\n",
            "MLP_2L  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.094573\n",
            "\n",
            "Test set: Average loss: 0.1291, Accuracy: 14425/15000 (96.17%)\n",
            "\n",
            "new accuracy parameters saved 0.9616666666666667\n",
            "MLP_2L  Epoch: 6 [0/45056 (0%)]\tLoss: 0.096313\n",
            "MLP_2L  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.190660\n",
            "MLP_2L  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.103441\n",
            "MLP_2L  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.093292\n",
            "\n",
            "Test set: Average loss: 0.1207, Accuracy: 14471/15000 (96.47%)\n",
            "\n",
            "new accuracy parameters saved 0.9647333333333333\n",
            "MLP_2L  Epoch: 7 [0/45056 (0%)]\tLoss: 0.188425\n",
            "MLP_2L  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.069028\n",
            "MLP_2L  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.084651\n",
            "MLP_2L  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.137113\n",
            "\n",
            "Test set: Average loss: 0.1153, Accuracy: 14493/15000 (96.62%)\n",
            "\n",
            "new accuracy parameters saved 0.9662\n",
            "MLP_2L  Epoch: 8 [0/45056 (0%)]\tLoss: 0.055355\n",
            "MLP_2L  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.164832\n",
            "MLP_2L  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.153769\n",
            "MLP_2L  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.129036\n",
            "\n",
            "Test set: Average loss: 0.1056, Accuracy: 14527/15000 (96.85%)\n",
            "\n",
            "new accuracy parameters saved 0.9684666666666667\n",
            "MLP_2L  Epoch: 9 [0/45056 (0%)]\tLoss: 0.122947\n",
            "MLP_2L  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.046456\n",
            "MLP_2L  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.090401\n",
            "MLP_2L  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.057729\n",
            "\n",
            "Test set: Average loss: 0.0978, Accuracy: 14572/15000 (97.15%)\n",
            "\n",
            "new accuracy parameters saved 0.9714666666666667\n",
            "Training complete in 1m 4s\n",
            "\n",
            "   --  Run :  5\n",
            "MLP_2L  Epoch: 0 [0/45056 (0%)]\tLoss: 2.299332\n",
            "MLP_2L  Epoch: 0 [12800/45056 (28%)]\tLoss: 0.646397\n",
            "MLP_2L  Epoch: 0 [25600/45056 (57%)]\tLoss: 0.483580\n",
            "MLP_2L  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.367307\n",
            "\n",
            "Test set: Average loss: 0.3224, Accuracy: 13587/15000 (90.58%)\n",
            "\n",
            "new accuracy parameters saved 0.9058\n",
            "MLP_2L  Epoch: 1 [0/45056 (0%)]\tLoss: 0.246011\n",
            "MLP_2L  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.355704\n",
            "MLP_2L  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.392811\n",
            "MLP_2L  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.175220\n",
            "\n",
            "Test set: Average loss: 0.2523, Accuracy: 13915/15000 (92.77%)\n",
            "\n",
            "new accuracy parameters saved 0.9276666666666666\n",
            "MLP_2L  Epoch: 2 [0/45056 (0%)]\tLoss: 0.299029\n",
            "MLP_2L  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.133904\n",
            "MLP_2L  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.233873\n",
            "MLP_2L  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.136930\n",
            "\n",
            "Test set: Average loss: 0.2278, Accuracy: 14033/15000 (93.55%)\n",
            "\n",
            "new accuracy parameters saved 0.9355333333333333\n",
            "MLP_2L  Epoch: 3 [0/45056 (0%)]\tLoss: 0.135530\n",
            "MLP_2L  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.184225\n",
            "MLP_2L  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.164223\n",
            "MLP_2L  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.092842\n",
            "\n",
            "Test set: Average loss: 0.1902, Accuracy: 14151/15000 (94.34%)\n",
            "\n",
            "new accuracy parameters saved 0.9434\n",
            "MLP_2L  Epoch: 4 [0/45056 (0%)]\tLoss: 0.141888\n",
            "MLP_2L  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.090855\n",
            "MLP_2L  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.106752\n",
            "MLP_2L  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.197213\n",
            "\n",
            "Test set: Average loss: 0.1458, Accuracy: 14354/15000 (95.69%)\n",
            "\n",
            "new accuracy parameters saved 0.9569333333333333\n",
            "MLP_2L  Epoch: 5 [0/45056 (0%)]\tLoss: 0.174785\n",
            "MLP_2L  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.187279\n",
            "MLP_2L  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.161411\n",
            "MLP_2L  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.118180\n",
            "\n",
            "Test set: Average loss: 0.1344, Accuracy: 14383/15000 (95.89%)\n",
            "\n",
            "new accuracy parameters saved 0.9588666666666666\n",
            "MLP_2L  Epoch: 6 [0/45056 (0%)]\tLoss: 0.140213\n",
            "MLP_2L  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.127081\n",
            "MLP_2L  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.060179\n",
            "MLP_2L  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.078287\n",
            "\n",
            "Test set: Average loss: 0.1163, Accuracy: 14502/15000 (96.68%)\n",
            "\n",
            "new accuracy parameters saved 0.9668\n",
            "MLP_2L  Epoch: 7 [0/45056 (0%)]\tLoss: 0.072607\n",
            "MLP_2L  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.103950\n",
            "MLP_2L  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.129240\n",
            "MLP_2L  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.092747\n",
            "\n",
            "Test set: Average loss: 0.1089, Accuracy: 14499/15000 (96.66%)\n",
            "\n",
            "MLP_2L  Epoch: 8 [0/45056 (0%)]\tLoss: 0.110706\n",
            "MLP_2L  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.041432\n",
            "MLP_2L  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.102904\n",
            "MLP_2L  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.059386\n",
            "\n",
            "Test set: Average loss: 0.1022, Accuracy: 14535/15000 (96.90%)\n",
            "\n",
            "new accuracy parameters saved 0.969\n",
            "MLP_2L  Epoch: 9 [0/45056 (0%)]\tLoss: 0.028522\n",
            "MLP_2L  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.100255\n",
            "MLP_2L  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.062968\n",
            "MLP_2L  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.089026\n",
            "\n",
            "Test set: Average loss: 0.0949, Accuracy: 14562/15000 (97.08%)\n",
            "\n",
            "new accuracy parameters saved 0.9708\n",
            "Training complete in 1m 4s\n",
            "\n",
            "   --  Run :  6\n",
            "MLP_2L  Epoch: 0 [0/45056 (0%)]\tLoss: 2.297394\n",
            "MLP_2L  Epoch: 0 [12800/45056 (28%)]\tLoss: 0.730745\n",
            "MLP_2L  Epoch: 0 [25600/45056 (57%)]\tLoss: 0.332179\n",
            "MLP_2L  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.411515\n",
            "\n",
            "Test set: Average loss: 0.3327, Accuracy: 13538/15000 (90.25%)\n",
            "\n",
            "new accuracy parameters saved 0.9025333333333333\n",
            "MLP_2L  Epoch: 1 [0/45056 (0%)]\tLoss: 0.195369\n",
            "MLP_2L  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.299787\n",
            "MLP_2L  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.201120\n",
            "MLP_2L  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.239760\n",
            "\n",
            "Test set: Average loss: 0.2427, Accuracy: 13945/15000 (92.97%)\n",
            "\n",
            "new accuracy parameters saved 0.9296666666666666\n",
            "MLP_2L  Epoch: 2 [0/45056 (0%)]\tLoss: 0.221624\n",
            "MLP_2L  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.248333\n",
            "MLP_2L  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.218489\n",
            "MLP_2L  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.147042\n",
            "\n",
            "Test set: Average loss: 0.2048, Accuracy: 14106/15000 (94.04%)\n",
            "\n",
            "new accuracy parameters saved 0.9404\n",
            "MLP_2L  Epoch: 3 [0/45056 (0%)]\tLoss: 0.184677\n",
            "MLP_2L  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.158765\n",
            "MLP_2L  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.132667\n",
            "MLP_2L  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.172761\n",
            "\n",
            "Test set: Average loss: 0.1692, Accuracy: 14281/15000 (95.21%)\n",
            "\n",
            "new accuracy parameters saved 0.9520666666666666\n",
            "MLP_2L  Epoch: 4 [0/45056 (0%)]\tLoss: 0.222372\n",
            "MLP_2L  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.130830\n",
            "MLP_2L  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.071974\n",
            "MLP_2L  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.077739\n",
            "\n",
            "Test set: Average loss: 0.1425, Accuracy: 14378/15000 (95.85%)\n",
            "\n",
            "new accuracy parameters saved 0.9585333333333333\n",
            "MLP_2L  Epoch: 5 [0/45056 (0%)]\tLoss: 0.146165\n",
            "MLP_2L  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.144749\n",
            "MLP_2L  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.175271\n",
            "MLP_2L  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.078318\n",
            "\n",
            "Test set: Average loss: 0.1301, Accuracy: 14431/15000 (96.21%)\n",
            "\n",
            "new accuracy parameters saved 0.9620666666666666\n",
            "MLP_2L  Epoch: 6 [0/45056 (0%)]\tLoss: 0.133289\n",
            "MLP_2L  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.080421\n",
            "MLP_2L  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.085127\n",
            "MLP_2L  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.123411\n",
            "\n",
            "Test set: Average loss: 0.1172, Accuracy: 14477/15000 (96.51%)\n",
            "\n",
            "new accuracy parameters saved 0.9651333333333333\n",
            "MLP_2L  Epoch: 7 [0/45056 (0%)]\tLoss: 0.087895\n",
            "MLP_2L  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.070612\n",
            "MLP_2L  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.213827\n",
            "MLP_2L  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.041075\n",
            "\n",
            "Test set: Average loss: 0.1136, Accuracy: 14496/15000 (96.64%)\n",
            "\n",
            "new accuracy parameters saved 0.9664\n",
            "MLP_2L  Epoch: 8 [0/45056 (0%)]\tLoss: 0.143774\n",
            "MLP_2L  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.065510\n",
            "MLP_2L  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.107857\n",
            "MLP_2L  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.094091\n",
            "\n",
            "Test set: Average loss: 0.1048, Accuracy: 14546/15000 (96.97%)\n",
            "\n",
            "new accuracy parameters saved 0.9697333333333333\n",
            "MLP_2L  Epoch: 9 [0/45056 (0%)]\tLoss: 0.099745\n",
            "MLP_2L  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.073013\n",
            "MLP_2L  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.069571\n",
            "MLP_2L  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.069592\n",
            "\n",
            "Test set: Average loss: 0.0957, Accuracy: 14564/15000 (97.09%)\n",
            "\n",
            "new accuracy parameters saved 0.9709333333333333\n",
            "Training complete in 1m 5s\n",
            "\n",
            "   --  Run :  7\n",
            "MLP_2L  Epoch: 0 [0/45056 (0%)]\tLoss: 2.307468\n",
            "MLP_2L  Epoch: 0 [12800/45056 (28%)]\tLoss: 0.693579\n",
            "MLP_2L  Epoch: 0 [25600/45056 (57%)]\tLoss: 0.379573\n",
            "MLP_2L  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.344691\n",
            "\n",
            "Test set: Average loss: 0.3198, Accuracy: 13636/15000 (90.91%)\n",
            "\n",
            "new accuracy parameters saved 0.9090666666666667\n",
            "MLP_2L  Epoch: 1 [0/45056 (0%)]\tLoss: 0.368848\n",
            "MLP_2L  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.308551\n",
            "MLP_2L  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.274365\n",
            "MLP_2L  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.246378\n",
            "\n",
            "Test set: Average loss: 0.2521, Accuracy: 13892/15000 (92.61%)\n",
            "\n",
            "new accuracy parameters saved 0.9261333333333334\n",
            "MLP_2L  Epoch: 2 [0/45056 (0%)]\tLoss: 0.165055\n",
            "MLP_2L  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.261882\n",
            "MLP_2L  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.300002\n",
            "MLP_2L  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.241143\n",
            "\n",
            "Test set: Average loss: 0.1981, Accuracy: 14161/15000 (94.41%)\n",
            "\n",
            "new accuracy parameters saved 0.9440666666666667\n",
            "MLP_2L  Epoch: 3 [0/45056 (0%)]\tLoss: 0.145152\n",
            "MLP_2L  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.123547\n",
            "MLP_2L  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.174490\n",
            "MLP_2L  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.136610\n",
            "\n",
            "Test set: Average loss: 0.1675, Accuracy: 14244/15000 (94.96%)\n",
            "\n",
            "new accuracy parameters saved 0.9496\n",
            "MLP_2L  Epoch: 4 [0/45056 (0%)]\tLoss: 0.133948\n",
            "MLP_2L  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.075745\n",
            "MLP_2L  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.150568\n",
            "MLP_2L  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.161435\n",
            "\n",
            "Test set: Average loss: 0.1404, Accuracy: 14385/15000 (95.90%)\n",
            "\n",
            "new accuracy parameters saved 0.959\n",
            "MLP_2L  Epoch: 5 [0/45056 (0%)]\tLoss: 0.093387\n",
            "MLP_2L  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.171357\n",
            "MLP_2L  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.094195\n",
            "MLP_2L  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.090506\n",
            "\n",
            "Test set: Average loss: 0.1301, Accuracy: 14421/15000 (96.14%)\n",
            "\n",
            "new accuracy parameters saved 0.9614\n",
            "MLP_2L  Epoch: 6 [0/45056 (0%)]\tLoss: 0.110073\n",
            "MLP_2L  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.130197\n",
            "MLP_2L  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.133348\n",
            "MLP_2L  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.100217\n",
            "\n",
            "Test set: Average loss: 0.1141, Accuracy: 14491/15000 (96.61%)\n",
            "\n",
            "new accuracy parameters saved 0.9660666666666666\n",
            "MLP_2L  Epoch: 7 [0/45056 (0%)]\tLoss: 0.165824\n",
            "MLP_2L  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.048734\n",
            "MLP_2L  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.077354\n",
            "MLP_2L  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.037278\n",
            "\n",
            "Test set: Average loss: 0.1121, Accuracy: 14508/15000 (96.72%)\n",
            "\n",
            "new accuracy parameters saved 0.9672\n",
            "MLP_2L  Epoch: 8 [0/45056 (0%)]\tLoss: 0.031117\n",
            "MLP_2L  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.046519\n",
            "MLP_2L  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.055979\n",
            "MLP_2L  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.144618\n",
            "\n",
            "Test set: Average loss: 0.0985, Accuracy: 14558/15000 (97.05%)\n",
            "\n",
            "new accuracy parameters saved 0.9705333333333334\n",
            "MLP_2L  Epoch: 9 [0/45056 (0%)]\tLoss: 0.046488\n",
            "MLP_2L  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.061621\n",
            "MLP_2L  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.034217\n",
            "MLP_2L  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.051121\n",
            "\n",
            "Test set: Average loss: 0.0903, Accuracy: 14592/15000 (97.28%)\n",
            "\n",
            "new accuracy parameters saved 0.9728\n",
            "Training complete in 1m 4s\n",
            "\n",
            "   --  Run :  8\n",
            "MLP_2L  Epoch: 0 [0/45056 (0%)]\tLoss: 2.302510\n",
            "MLP_2L  Epoch: 0 [12800/45056 (28%)]\tLoss: 0.716880\n",
            "MLP_2L  Epoch: 0 [25600/45056 (57%)]\tLoss: 0.394838\n",
            "MLP_2L  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.263237\n",
            "\n",
            "Test set: Average loss: 0.3813, Accuracy: 13221/15000 (88.14%)\n",
            "\n",
            "new accuracy parameters saved 0.8814\n",
            "MLP_2L  Epoch: 1 [0/45056 (0%)]\tLoss: 0.476290\n",
            "MLP_2L  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.205686\n",
            "MLP_2L  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.332599\n",
            "MLP_2L  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.268130\n",
            "\n",
            "Test set: Average loss: 0.2603, Accuracy: 13833/15000 (92.22%)\n",
            "\n",
            "new accuracy parameters saved 0.9222\n",
            "MLP_2L  Epoch: 2 [0/45056 (0%)]\tLoss: 0.252972\n",
            "MLP_2L  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.154049\n",
            "MLP_2L  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.211918\n",
            "MLP_2L  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.241435\n",
            "\n",
            "Test set: Average loss: 0.1935, Accuracy: 14156/15000 (94.37%)\n",
            "\n",
            "new accuracy parameters saved 0.9437333333333333\n",
            "MLP_2L  Epoch: 3 [0/45056 (0%)]\tLoss: 0.183604\n",
            "MLP_2L  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.217322\n",
            "MLP_2L  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.145289\n",
            "MLP_2L  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.149591\n",
            "\n",
            "Test set: Average loss: 0.1721, Accuracy: 14259/15000 (95.06%)\n",
            "\n",
            "new accuracy parameters saved 0.9506\n",
            "MLP_2L  Epoch: 4 [0/45056 (0%)]\tLoss: 0.134335\n",
            "MLP_2L  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.080384\n",
            "MLP_2L  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.142127\n",
            "MLP_2L  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.091338\n",
            "\n",
            "Test set: Average loss: 0.1529, Accuracy: 14325/15000 (95.50%)\n",
            "\n",
            "new accuracy parameters saved 0.955\n",
            "MLP_2L  Epoch: 5 [0/45056 (0%)]\tLoss: 0.121918\n",
            "MLP_2L  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.094239\n",
            "MLP_2L  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.133036\n",
            "MLP_2L  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.132984\n",
            "\n",
            "Test set: Average loss: 0.1475, Accuracy: 14327/15000 (95.51%)\n",
            "\n",
            "new accuracy parameters saved 0.9551333333333333\n",
            "MLP_2L  Epoch: 6 [0/45056 (0%)]\tLoss: 0.246550\n",
            "MLP_2L  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.173793\n",
            "MLP_2L  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.056764\n",
            "MLP_2L  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.081236\n",
            "\n",
            "Test set: Average loss: 0.1176, Accuracy: 14466/15000 (96.44%)\n",
            "\n",
            "new accuracy parameters saved 0.9644\n",
            "MLP_2L  Epoch: 7 [0/45056 (0%)]\tLoss: 0.115445\n",
            "MLP_2L  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.069065\n",
            "MLP_2L  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.068927\n",
            "MLP_2L  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.101931\n",
            "\n",
            "Test set: Average loss: 0.1034, Accuracy: 14533/15000 (96.89%)\n",
            "\n",
            "new accuracy parameters saved 0.9688666666666667\n",
            "MLP_2L  Epoch: 8 [0/45056 (0%)]\tLoss: 0.056564\n",
            "MLP_2L  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.122203\n",
            "MLP_2L  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.052265\n",
            "MLP_2L  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.120941\n",
            "\n",
            "Test set: Average loss: 0.0994, Accuracy: 14550/15000 (97.00%)\n",
            "\n",
            "new accuracy parameters saved 0.97\n",
            "MLP_2L  Epoch: 9 [0/45056 (0%)]\tLoss: 0.055268\n",
            "MLP_2L  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.029672\n",
            "MLP_2L  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.065548\n",
            "MLP_2L  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.048909\n",
            "\n",
            "Test set: Average loss: 0.0907, Accuracy: 14607/15000 (97.38%)\n",
            "\n",
            "new accuracy parameters saved 0.9738\n",
            "Training complete in 1m 5s\n",
            "\n",
            "   --  Run :  9\n",
            "MLP_2L  Epoch: 0 [0/45056 (0%)]\tLoss: 2.307560\n",
            "MLP_2L  Epoch: 0 [12800/45056 (28%)]\tLoss: 0.647688\n",
            "MLP_2L  Epoch: 0 [25600/45056 (57%)]\tLoss: 0.391032\n",
            "MLP_2L  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.365162\n",
            "\n",
            "Test set: Average loss: 0.3644, Accuracy: 13302/15000 (88.68%)\n",
            "\n",
            "new accuracy parameters saved 0.8868\n",
            "MLP_2L  Epoch: 1 [0/45056 (0%)]\tLoss: 0.405243\n",
            "MLP_2L  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.242173\n",
            "MLP_2L  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.375156\n",
            "MLP_2L  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.266154\n",
            "\n",
            "Test set: Average loss: 0.2455, Accuracy: 13916/15000 (92.77%)\n",
            "\n",
            "new accuracy parameters saved 0.9277333333333333\n",
            "MLP_2L  Epoch: 2 [0/45056 (0%)]\tLoss: 0.208697\n",
            "MLP_2L  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.204437\n",
            "MLP_2L  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.287386\n",
            "MLP_2L  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.185614\n",
            "\n",
            "Test set: Average loss: 0.1963, Accuracy: 14154/15000 (94.36%)\n",
            "\n",
            "new accuracy parameters saved 0.9436\n",
            "MLP_2L  Epoch: 3 [0/45056 (0%)]\tLoss: 0.257301\n",
            "MLP_2L  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.310335\n",
            "MLP_2L  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.116660\n",
            "MLP_2L  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.166239\n",
            "\n",
            "Test set: Average loss: 0.1626, Accuracy: 14299/15000 (95.33%)\n",
            "\n",
            "new accuracy parameters saved 0.9532666666666667\n",
            "MLP_2L  Epoch: 4 [0/45056 (0%)]\tLoss: 0.149444\n",
            "MLP_2L  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.150878\n",
            "MLP_2L  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.226167\n",
            "MLP_2L  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.161920\n",
            "\n",
            "Test set: Average loss: 0.1525, Accuracy: 14329/15000 (95.53%)\n",
            "\n",
            "new accuracy parameters saved 0.9552666666666667\n",
            "MLP_2L  Epoch: 5 [0/45056 (0%)]\tLoss: 0.073935\n",
            "MLP_2L  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.097079\n",
            "MLP_2L  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.177495\n",
            "MLP_2L  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.113636\n",
            "\n",
            "Test set: Average loss: 0.1307, Accuracy: 14408/15000 (96.05%)\n",
            "\n",
            "new accuracy parameters saved 0.9605333333333334\n",
            "MLP_2L  Epoch: 6 [0/45056 (0%)]\tLoss: 0.104477\n",
            "MLP_2L  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.071612\n",
            "MLP_2L  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.061845\n",
            "MLP_2L  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.101643\n",
            "\n",
            "Test set: Average loss: 0.1197, Accuracy: 14472/15000 (96.48%)\n",
            "\n",
            "new accuracy parameters saved 0.9648\n",
            "MLP_2L  Epoch: 7 [0/45056 (0%)]\tLoss: 0.052190\n",
            "MLP_2L  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.065163\n",
            "MLP_2L  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.083235\n",
            "MLP_2L  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.088293\n",
            "\n",
            "Test set: Average loss: 0.1141, Accuracy: 14491/15000 (96.61%)\n",
            "\n",
            "new accuracy parameters saved 0.9660666666666666\n",
            "MLP_2L  Epoch: 8 [0/45056 (0%)]\tLoss: 0.098329\n",
            "MLP_2L  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.068597\n",
            "MLP_2L  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.092533\n",
            "MLP_2L  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.092099\n",
            "\n",
            "Test set: Average loss: 0.0980, Accuracy: 14565/15000 (97.10%)\n",
            "\n",
            "new accuracy parameters saved 0.971\n",
            "MLP_2L  Epoch: 9 [0/45056 (0%)]\tLoss: 0.050123\n",
            "MLP_2L  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.083929\n",
            "MLP_2L  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.066824\n",
            "MLP_2L  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.031950\n",
            "\n",
            "Test set: Average loss: 0.0987, Accuracy: 14547/15000 (96.98%)\n",
            "\n",
            "Training complete in 1m 4s\n",
            "\n",
            "   --  Run :  10\n",
            "MLP_2L  Epoch: 0 [0/45056 (0%)]\tLoss: 2.303923\n",
            "MLP_2L  Epoch: 0 [12800/45056 (28%)]\tLoss: 0.577331\n",
            "MLP_2L  Epoch: 0 [25600/45056 (57%)]\tLoss: 0.363801\n",
            "MLP_2L  Epoch: 0 [38400/45056 (85%)]\tLoss: 0.312498\n",
            "\n",
            "Test set: Average loss: 0.4539, Accuracy: 12794/15000 (85.29%)\n",
            "\n",
            "new accuracy parameters saved 0.8529333333333333\n",
            "MLP_2L  Epoch: 1 [0/45056 (0%)]\tLoss: 0.330928\n",
            "MLP_2L  Epoch: 1 [12800/45056 (28%)]\tLoss: 0.261185\n",
            "MLP_2L  Epoch: 1 [25600/45056 (57%)]\tLoss: 0.209768\n",
            "MLP_2L  Epoch: 1 [38400/45056 (85%)]\tLoss: 0.322187\n",
            "\n",
            "Test set: Average loss: 0.2362, Accuracy: 13986/15000 (93.24%)\n",
            "\n",
            "new accuracy parameters saved 0.9324\n",
            "MLP_2L  Epoch: 2 [0/45056 (0%)]\tLoss: 0.195537\n",
            "MLP_2L  Epoch: 2 [12800/45056 (28%)]\tLoss: 0.159196\n",
            "MLP_2L  Epoch: 2 [25600/45056 (57%)]\tLoss: 0.187827\n",
            "MLP_2L  Epoch: 2 [38400/45056 (85%)]\tLoss: 0.159245\n",
            "\n",
            "Test set: Average loss: 0.1991, Accuracy: 14138/15000 (94.25%)\n",
            "\n",
            "new accuracy parameters saved 0.9425333333333333\n",
            "MLP_2L  Epoch: 3 [0/45056 (0%)]\tLoss: 0.291999\n",
            "MLP_2L  Epoch: 3 [12800/45056 (28%)]\tLoss: 0.104976\n",
            "MLP_2L  Epoch: 3 [25600/45056 (57%)]\tLoss: 0.130256\n",
            "MLP_2L  Epoch: 3 [38400/45056 (85%)]\tLoss: 0.192502\n",
            "\n",
            "Test set: Average loss: 0.1672, Accuracy: 14280/15000 (95.20%)\n",
            "\n",
            "new accuracy parameters saved 0.952\n",
            "MLP_2L  Epoch: 4 [0/45056 (0%)]\tLoss: 0.163673\n",
            "MLP_2L  Epoch: 4 [12800/45056 (28%)]\tLoss: 0.146292\n",
            "MLP_2L  Epoch: 4 [25600/45056 (57%)]\tLoss: 0.117101\n",
            "MLP_2L  Epoch: 4 [38400/45056 (85%)]\tLoss: 0.147215\n",
            "\n",
            "Test set: Average loss: 0.1442, Accuracy: 14367/15000 (95.78%)\n",
            "\n",
            "new accuracy parameters saved 0.9578\n",
            "MLP_2L  Epoch: 5 [0/45056 (0%)]\tLoss: 0.159140\n",
            "MLP_2L  Epoch: 5 [12800/45056 (28%)]\tLoss: 0.066440\n",
            "MLP_2L  Epoch: 5 [25600/45056 (57%)]\tLoss: 0.085616\n",
            "MLP_2L  Epoch: 5 [38400/45056 (85%)]\tLoss: 0.133458\n",
            "\n",
            "Test set: Average loss: 0.1350, Accuracy: 14412/15000 (96.08%)\n",
            "\n",
            "new accuracy parameters saved 0.9608\n",
            "MLP_2L  Epoch: 6 [0/45056 (0%)]\tLoss: 0.063220\n",
            "MLP_2L  Epoch: 6 [12800/45056 (28%)]\tLoss: 0.070558\n",
            "MLP_2L  Epoch: 6 [25600/45056 (57%)]\tLoss: 0.161093\n",
            "MLP_2L  Epoch: 6 [38400/45056 (85%)]\tLoss: 0.051725\n",
            "\n",
            "Test set: Average loss: 0.1184, Accuracy: 14481/15000 (96.54%)\n",
            "\n",
            "new accuracy parameters saved 0.9654\n",
            "MLP_2L  Epoch: 7 [0/45056 (0%)]\tLoss: 0.034623\n",
            "MLP_2L  Epoch: 7 [12800/45056 (28%)]\tLoss: 0.062585\n",
            "MLP_2L  Epoch: 7 [25600/45056 (57%)]\tLoss: 0.118135\n",
            "MLP_2L  Epoch: 7 [38400/45056 (85%)]\tLoss: 0.043058\n",
            "\n",
            "Test set: Average loss: 0.1061, Accuracy: 14529/15000 (96.86%)\n",
            "\n",
            "new accuracy parameters saved 0.9686\n",
            "MLP_2L  Epoch: 8 [0/45056 (0%)]\tLoss: 0.097339\n",
            "MLP_2L  Epoch: 8 [12800/45056 (28%)]\tLoss: 0.063166\n",
            "MLP_2L  Epoch: 8 [25600/45056 (57%)]\tLoss: 0.059511\n",
            "MLP_2L  Epoch: 8 [38400/45056 (85%)]\tLoss: 0.063902\n",
            "\n",
            "Test set: Average loss: 0.1000, Accuracy: 14564/15000 (97.09%)\n",
            "\n",
            "new accuracy parameters saved 0.9709333333333333\n",
            "MLP_2L  Epoch: 9 [0/45056 (0%)]\tLoss: 0.043592\n",
            "MLP_2L  Epoch: 9 [12800/45056 (28%)]\tLoss: 0.048457\n",
            "MLP_2L  Epoch: 9 [25600/45056 (57%)]\tLoss: 0.024093\n",
            "MLP_2L  Epoch: 9 [38400/45056 (85%)]\tLoss: 0.124970\n",
            "\n",
            "Test set: Average loss: 0.0929, Accuracy: 14585/15000 (97.23%)\n",
            "\n",
            "new accuracy parameters saved 0.9723333333333334\n",
            "Training complete in 1m 4s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QG7PYeXHFnvb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Compairing Results"
      ]
    },
    {
      "metadata": {
        "id": "xgCPpl4zB7NK",
        "colab_type": "code",
        "outputId": "ba2c2454-ed3c-4b5b-d05a-4536ed7da190",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))\n",
        "\n",
        "for filename in os.listdir(savedir):\n",
        "    if filename.endswith('.pkl'):\n",
        "        with open(os.path.join(savedir, filename),'rb') as fin:\n",
        "            results = pickle.load(fin)\n",
        "            ax1.plot(results['loss'])\n",
        "            ax1.set_ylabel('cross entropy')\n",
        "            ax1.set_xlabel('epochs')\n",
        "            \n",
        "            ax2.plot(results['accuracy'], label = filename[:-4])\n",
        "            ax2.set_ylabel('accuracy')\n",
        "            ax2.set_xlabel('epochs')\n",
        "            \n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb95e99e4a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABI0AAAF0CAYAAACubclCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1440x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "msLlyF8JOEPX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Final comments"
      ]
    },
    {
      "metadata": {
        "id": "XhL3tsvzUwKU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "When looking at the aboves charts, we have to remember that it is plotted from the average of 10 runs of 10 epochs.\n",
        "So what we see is really the evolution of the models during training.  For exemple: CNN2 always starts with poor results after the first epoch.\n",
        "\n",
        "##### CNN1\n",
        "As this model slightly outperforms the MLP at every epoch, we can imagine it also doing better in the long run.  However, since they end up with the same value on the last epoch, we would need a longer run to confirm that.  With half as many parameters as the MLP, it would be tempting to declare this model simply superior to the MLP, but we should not jump to hasty conclusions as we should remember from part 1 that some models with lower number of parameters also performed well, so there might be a redundent amount of neurons that contribute much.\n",
        "\n",
        "Still it is really interesting that we can simply replace 400k parameters with 320 parameters and gain in performance.  It clearly shows that a single layers gets more out of the 32 feature maps then a fully connected layer.\n",
        "\n",
        "##### CNN2 \n",
        "With a second round of convolution the effect is even more pronounced and the model now clearly outperforms the MLP.  However about ~150k paramaters are needed to perform that convolution of 16 filters on the 32 feature maps.  \n",
        "\n",
        "##### CNN3\n",
        "Finnaly this model adds yet more interesting observations.  Not only does CNN3 and this MLP have similar number of parameters (715k vs 800k), but they are also distributed and balanced in a similar way.  Also, compared to CNN2 where we had 16 filters. we now have 2 and 8 for CNN3.  So mostly only the architecture changed, being deeper instead of wider with an added 64 intermediary feature maps and a corresponding 120k increase in the number of parameters. \n",
        "\n",
        "It is unknown with this experiment how much convolution depth we can add and still gain perfomance, either plateau or loose performance.  Just looking at CNN2 and CNN3 at epoch 10 we can imagine that they could converge on the same value.  \n",
        "\n",
        "What this experiment has shown though with all three models is how much convolution is better then a fully connected layer for image based classification.  MNIST being a relatively simple dataset, we suspect that with a more complex task the difference would be even more pronounced. \n"
      ]
    }
  ]
}