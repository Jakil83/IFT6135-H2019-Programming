{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 of TP 1\n",
    "\n",
    "Project by - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as mp\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "\n",
    "The dataset mnist.pkl.npy was downloaded using download.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets (train, valid, test) - (50000, 784) (10000, 784) (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_data = np.load('mnist.pkl.npy')\n",
    "print(\"Datasets (train, valid, test) -\",np.shape(train_data[0]), np.shape(valid_data[0]), np.shape(test_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model \n",
    "Consider an MLP with two hidden layers with $h^1$ and $h^2$ hidden units.For the MNIST dataset,  the number of features of the input datah0is 784.  The output of theneural network is parameterized by a softmax of $h^3= 10$ classes.\n",
    "\n",
    "1.  Build an MLP and choose the values of $h^1$ and $h^2$ such that the total number of parameters(including biases) falls within the range of [0.5M, 1.0M].\n",
    "\n",
    "2.  Implement the forward and backward propagation of the MLP in numpy without using any ofthe deep learning frameworks that provides automatic differentiation.  Use the class structureprovided here.\n",
    "\n",
    "3.  Train the MLP using the probability loss (cross entropy) as training criterion.  We minimizethis criterion to optimize the model parameters usingstochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### class for MLP\n",
    "class NN(object):\n",
    "    def __init__(self, hidden_dims=(512,768), weight_type = \"Glorot\", learning_rate = 0.1, silent_search = 0):\n",
    "        ## Initialising the hidden layer sizes\n",
    "        self.h0 = 784\n",
    "        self.h1 = int(hidden_dims[0])\n",
    "        self.h2 = int(hidden_dims[1])\n",
    "        self.h3 = 10\n",
    "        \n",
    "        ## learning rate\n",
    "        self.eta = learning_rate\n",
    "        \n",
    "        ## initializing the bias\n",
    "        self.b0 = np.zeros(self.h1).reshape(-1,1) \n",
    "        self.b1 = np.zeros(self.h2).reshape(-1,1) \n",
    "        self.b2 = np.zeros(self.h3).reshape(-1,1) \n",
    "        \n",
    "        ## Weight variables\n",
    "        self.W0 = 0 \n",
    "        self.W1 = 0 \n",
    "        self.W2 = 0 \n",
    "        \n",
    "        ## initializing the weights depending on the weight_type\n",
    "        self.initialize_weights(weight_type = weight_type)\n",
    "        \n",
    "        ## To obtain the total count of parameters int he model and restric it to between 0.5-1 Million.\n",
    "        print(\"parameter count =\", sum([len(self.b0),len(self.b1),len(self.b2),len(self.W0.reshape(-1,1)),len(self.W1.reshape(-1,1)),len(self.W2.reshape(-1,1))]))\n",
    "        \n",
    "        self.silent = silent_search        \n",
    "            \n",
    "    def initialize_weights(self, weight_type = \"Glorot\"):\n",
    "        if weight_type == \"Zeros\":\n",
    "            self.W0 = np.zeros(self.h1*self.h0).reshape(self.h1,self.h0)\n",
    "            self.W1 = np.zeros(self.h2*self.h1).reshape(self.h2,self.h1)\n",
    "            self.W2 = np.zeros(self.h3*self.h2).reshape(self.h3,self.h2)\n",
    "        elif weight_type == \"Normal\":\n",
    "            self.W0 = np.random.normal(0.0,1.0e2,size = self.h1*self.h0).reshape(self.h1,self.h0)\n",
    "            self.W1 = np.random.normal(0.0,1.0e2,size = self.h2*self.h1).reshape(self.h2,self.h1)\n",
    "            self.W2 = np.random.normal(0.0,1.0e2,size = self.h3*self.h2).reshape(self.h3,self.h2)\n",
    "        elif weight_type == \"Glorot\":\n",
    "            dl0 = np.sqrt(6.0/(self.h1+self.h0))\n",
    "            self.W0 = np.random.uniform(-dl0,dl0,size= self.h1*self.h0).reshape(self.h1,self.h0)\n",
    "            dl1 = np.sqrt(6.0/(self.h2+self.h1))\n",
    "            self.W1 = np.random.uniform(-dl1,dl1,size= self.h2*self.h1).reshape(self.h2,self.h1)\n",
    "            dl2 = np.sqrt(6.0/(self.h3+self.h2))\n",
    "            self.W2 = np.random.uniform(-dl2,dl2,size= self.h3*self.h2).reshape(self.h3,self.h2)\n",
    "        \n",
    "    def forward(self, input_data):       \n",
    "        input_data = np.array(input_data)                                   # [d,n]\n",
    "        cache = {}           \n",
    "        cache['input'] = input_data\n",
    "         \n",
    "        cache['ha_1'] = np.matmul(self.W0,cache['input']) + self.b0\n",
    "        cache['hs_1'] = self.activation(cache['ha_1'])\n",
    "        \n",
    "        cache['ha_2'] = np.matmul(self.W1,cache['hs_1']) + self.b1\n",
    "        cache['hs_2'] = self.activation(cache['ha_2'])\n",
    "                           \n",
    "        cache['ha_3'] = np.matmul(self.W2,cache['hs_2']) + self.b2\n",
    "        cache['output'] = self.softmax(cache['ha_3']) \n",
    "            \n",
    "        return cache\n",
    "\n",
    "\n",
    "    def activation(self, input_data):\n",
    "        ## relu activation\n",
    "        x = np.array(input_data)\n",
    "        x[x<0] = 0\n",
    "        return x\n",
    "               \n",
    "    def softmax(self, input_data):\n",
    "        ## thanks to https://deepnotes.io/softmax-crossentropy\n",
    "        exps = np.exp(input_data - np.max(input_data,axis=0))\n",
    "        return np.true_divide(exps, np.sum(exps,axis = 0))\n",
    "    \n",
    "    \n",
    "    def loss(self, prediction_list, labels):\n",
    "        ## probability loss (cross entropy)\n",
    "        loss_values = [-np.log(prediction[label]) for label,prediction in zip(labels,prediction_list.T)]\n",
    "        return np.array(loss_values)\n",
    "\n",
    "    def backward(self, cache, labels):\n",
    "        n = float(len(labels))\n",
    "        grads = {}      \n",
    "        grads['ha_3'] = cache['output'] - np.transpose(np.eye(self.h3)[labels])     \n",
    "        \n",
    "        grads['W2'] = np.matmul(grads['ha_3'], cache['hs_2'].T)/n   \n",
    "        grads['b2'] = np.array(np.sum(grads['ha_3'],axis = 1)/n).reshape(-1,1)  \n",
    "        grads['hs_2'] = np.matmul(self.W2.T, grads['ha_3']) \n",
    "        grads['ha_2'] = np.multiply(grads['hs_2'], np.where(cache['ha_2'] > 0,1.0,0.0))\n",
    "                           \n",
    "        grads['W1'] = np.matmul(grads['ha_2'], cache['hs_1'].T)/n\n",
    "        grads['b1'] = np.array(np.sum(grads['ha_2'],axis = 1)/n).reshape(-1,1)\n",
    "        grads['hs_1'] = np.matmul(self.W1.T, grads['ha_2']) \n",
    "        grads['ha_1'] = np.multiply(grads['hs_1'], np.where(cache['ha_1'] > 0,1.0,0.0))\n",
    "                           \n",
    "        grads['W0'] = np.matmul(grads['ha_1'], cache['input'].T)/n\n",
    "        grads['b0'] = np.array(np.sum(grads['ha_1'],axis = 1)/n).reshape(-1,1)\n",
    "                           \n",
    "        return grads\n",
    "    \n",
    "    def update(self, grads):  \n",
    "        # Stocastic gradient descent\n",
    "        self.W2 -= self.eta*grads['W2']\n",
    "        self.W1 -= self.eta*grads['W1']\n",
    "        self.W0 -= self.eta*grads['W0']\n",
    "        self.b2 -= self.eta*grads['b2']\n",
    "        self.b1 -= self.eta*grads['b1']\n",
    "        self.b0 -= self.eta*grads['b0']\n",
    "        \n",
    "    def classification_accuracy(self, predicted_labels, true_labels):\n",
    "        # Classification accuracy in percent\n",
    "        return np.sum(np.array(predicted_labels)==np.array(true_labels))*100.0/np.array(predicted_labels).shape[0]\n",
    "                           \n",
    "    def train(self, input_data, labels, epochs = 10, mini_batch = 10): \n",
    "        n = len(labels)\n",
    "        input_data = input_data.T\n",
    "        loss = []\n",
    "        \n",
    "        for e in range(epochs):  \n",
    "            i = 0\n",
    "            while(i < n):\n",
    "                if i + mini_batch <= n:\n",
    "                    minibatch_data = input_data[:,i:i + mini_batch]\n",
    "                    minibatch_labels = labels[i:i + mini_batch]\n",
    "                else:\n",
    "                    minibatch_data = input_data[:,i:]\n",
    "                    minibatch_labels = labels[i:]\n",
    "                    \n",
    "                cache = self.forward(minibatch_data)\n",
    "                grads = self.backward(cache,minibatch_labels)\n",
    "                        \n",
    "                self.update(grads)\n",
    "                i = i + mini_batch\n",
    "                \n",
    "            predictions, meanloss = self.test(input_data,labels, run_for = \"training\")\n",
    "            loss.append(meanloss)\n",
    "            if (self.silent == 0):\n",
    "                print(\"Classification accuracy [%] on training = \",self.classification_accuracy(predictions,labels))  \n",
    "        return loss         \n",
    "        \n",
    "    def test(self, input_data, labels, run_for = \"testing\"):\n",
    "        if (self.silent == 0):\n",
    "            print(\"\\n\\n\"+run_for+\"..\")\n",
    "            \n",
    "        cache = self.forward(input_data)\n",
    "        \n",
    "        mean_loss = np.mean(self.loss(cache['output'],labels))\n",
    "        if (self.silent == 0):\n",
    "            print(\"Mean Loss for \"+run_for+\" = \",mean_loss) \n",
    "            \n",
    "        return np.argmax(cache['output'],axis = 0), mean_loss\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the following sub-questions, please specify the model architecture (number of hidden units per layer, and the total number of parameters), the nonlinearity chosen as neuron activation, learning rate, mini-batch size.\n",
    "\n",
    "\n",
    "Model Architecture - 2 layer MLP with input size 784, first hiddent layer of size 512, second hidden layer of size 768 and output layer of size 10.\n",
    "\n",
    "Total number of parameters - 804352.\n",
    "\n",
    "Neuron activation non-linearity - Rectified linear Unit (ReLu).\n",
    "\n",
    "Output non-linearity - Softmax.\n",
    "\n",
    "Learning rate (eta) - 0.1.\n",
    "\n",
    "Mini-Batch size - 128.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "n = len(train_data[0])\n",
    "print(n)\n",
    "train_data_features = train_data[0][:n]\n",
    "train_data_labels = train_data[1][:n]\n",
    "\n",
    "valid_data_features = valid_data[0]\n",
    "valid_data_labels = valid_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to run a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(epoch = 10, mini_batch = 128, weight_type = \"Glorot\"):\n",
    "    since = time.time()\n",
    "    \n",
    "    NN_0 = NN(weight_type = weight_type)\n",
    "    print(\"Loading model : \",NN_0.h1,\"x\",NN_0.h2,\"lr : \", NN_0.eta)\n",
    "    \n",
    "    loss_val = NN_0.train(train_data_features,train_data_labels,epochs = epoch, mini_batch = mini_batch)\n",
    "    \n",
    "    valid_predictions, _ = NN_0.test(valid_data_features.T, valid_data_labels)\n",
    "    acc = NN_0.classification_accuracy(valid_predictions, valid_data_labels)\n",
    "    print(\"Classification accuracy [%] on validation = \",acc)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "          time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "    return loss_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization: In this sub-question, we consider different initial values for the weight param-eters.  Set the biases to be zeros, and consider the following settings for the weight parameters:\n",
    "\n",
    "### 1.  Train the model for 10 epochs using the initialization methods above and record the average loss measured on the training data at the end of each epoch (10 values for each setup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter count = 803594\n",
      "Loading model :  512 x 768 lr :  0.1\n",
      "\n",
      "\n",
      "training..\n",
      "Mean Loss for training =  2.30106604987257\n",
      "Classification accuracy [%] on training =  11.356\n",
      "\n",
      "\n",
      "training..\n",
      "Mean Loss for training =  2.3010626883570473\n",
      "Classification accuracy [%] on training =  11.356\n",
      "\n",
      "\n",
      "training..\n",
      "Mean Loss for training =  2.301062639269737\n",
      "Classification accuracy [%] on training =  11.356\n",
      "\n",
      "\n",
      "training..\n",
      "Mean Loss for training =  2.30106263826223\n",
      "Classification accuracy [%] on training =  11.356\n",
      "\n",
      "\n",
      "training..\n",
      "Mean Loss for training =  2.301062638237786\n",
      "Classification accuracy [%] on training =  11.356\n",
      "\n",
      "\n",
      "training..\n",
      "Mean Loss for training =  2.3010626382371333\n",
      "Classification accuracy [%] on training =  11.356\n",
      "\n",
      "\n",
      "training..\n",
      "Mean Loss for training =  2.301062638237115\n",
      "Classification accuracy [%] on training =  11.356\n",
      "\n",
      "\n",
      "training..\n",
      "Mean Loss for training =  2.301062638237115\n",
      "Classification accuracy [%] on training =  11.356\n",
      "\n",
      "\n",
      "training..\n",
      "Mean Loss for training =  2.301062638237115\n",
      "Classification accuracy [%] on training =  11.356\n",
      "\n",
      "\n",
      "training..\n",
      "Mean Loss for training =  2.301062638237115\n",
      "Classification accuracy [%] on training =  11.356\n",
      "\n",
      "\n",
      "testing..\n",
      "Mean Loss for testing =  2.301854740465508\n",
      "Classification accuracy [%] on validation =  10.64\n",
      "Training complete in 1m 13s\n"
     ]
    }
   ],
   "source": [
    "zeros_losses = run(weight_type = \"Zeros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter count = 803594\n",
      "Loading model :  512 x 768 lr :  0.1\n",
      "\n",
      "\n",
      "training..\n",
      "Mean Loss for training =  2.301148133809196\n",
      "Classification accuracy [%] on training =  11.356\n",
      "\n",
      "\n",
      "training..\n",
      "Mean Loss for training =  2.3010627737389466\n",
      "Classification accuracy [%] on training =  11.356\n",
      "\n",
      "\n",
      "training..\n",
      "Mean Loss for training =  2.3010626412812285\n",
      "Classification accuracy [%] on training =  11.356\n",
      "\n",
      "\n",
      "training..\n",
      "Mean Loss for training =  2.3010626383428696\n",
      "Classification accuracy [%] on training =  11.356\n",
      "\n",
      "\n",
      "training..\n",
      "Mean Loss for training =  2.3010626382406802\n",
      "Classification accuracy [%] on training =  11.356\n",
      "\n",
      "\n",
      "training..\n",
      "Mean Loss for training =  2.301062638237229\n",
      "Classification accuracy [%] on training =  11.356\n",
      "\n",
      "\n",
      "training..\n",
      "Mean Loss for training =  2.3010626382371187\n",
      "Classification accuracy [%] on training =  11.356\n",
      "\n",
      "\n",
      "training..\n",
      "Mean Loss for training =  2.301062638237115\n",
      "Classification accuracy [%] on training =  11.356\n",
      "\n",
      "\n",
      "training..\n",
      "Mean Loss for training =  2.301062638237115\n",
      "Classification accuracy [%] on training =  11.356\n",
      "\n",
      "\n",
      "training..\n",
      "Mean Loss for training =  2.301062638237115\n",
      "Classification accuracy [%] on training =  11.356\n",
      "\n",
      "\n",
      "testing..\n",
      "Mean Loss for testing =  inf\n",
      "Classification accuracy [%] on validation =  10.64\n",
      "Training complete in 1m 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/helios/lib/python3.7/site-packages/ipykernel_launcher.py:79: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "normal_losses = run(weight_type = \"Normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter count = 803594\n",
      "Loading model :  512 x 768 lr :  0.1\n",
      "\n",
      "\n",
      "training..\n",
      "Mean Loss for training =  0.2724472669813704\n",
      "Classification accuracy [%] on training =  91.848\n",
      "\n",
      "\n",
      "training..\n",
      "Mean Loss for training =  0.18523987693175825\n",
      "Classification accuracy [%] on training =  94.536\n",
      "\n",
      "\n",
      "training..\n",
      "Mean Loss for training =  0.1400291295956501\n",
      "Classification accuracy [%] on training =  95.918\n",
      "\n",
      "\n",
      "training..\n",
      "Mean Loss for training =  0.11225144840363874\n",
      "Classification accuracy [%] on training =  96.736\n",
      "\n",
      "\n",
      "training..\n",
      "Mean Loss for training =  0.09331997012000898\n",
      "Classification accuracy [%] on training =  97.302\n",
      "\n",
      "\n",
      "training..\n",
      "Mean Loss for training =  0.07926168895761627\n",
      "Classification accuracy [%] on training =  97.692\n",
      "\n",
      "\n",
      "training..\n",
      "Mean Loss for training =  0.06788345141223112\n",
      "Classification accuracy [%] on training =  98.048\n",
      "\n",
      "\n",
      "training..\n",
      "Mean Loss for training =  0.05868652831809437\n",
      "Classification accuracy [%] on training =  98.34\n",
      "\n",
      "\n",
      "training..\n",
      "Mean Loss for training =  0.0512073964103037\n",
      "Classification accuracy [%] on training =  98.576\n",
      "\n",
      "\n",
      "training..\n",
      "Mean Loss for training =  0.04491940048679391\n",
      "Classification accuracy [%] on training =  98.756\n",
      "\n",
      "\n",
      "testing..\n",
      "Mean Loss for testing =  0.08393884677883673\n",
      "Classification accuracy [%] on validation =  97.53\n",
      "Training complete in 1m 4s\n"
     ]
    }
   ],
   "source": [
    "glorot_losses = run(weight_type = \"Glorot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Compare the three setups by plotting the losses against the training time (epoch) and commenton the result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(glorot_losses, label = \"glorot\", color = 'blue')\n",
    "plt.plot(zeros_losses, label = \"zero\", color = 'magenta')\n",
    "plt.plot(normal_losses, label = \"normal\", color = 'cyan')\n",
    "\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that for both Zeros and Normal initializations, the loss stays high and the accuracy is 10 as the model is training at a very slow pace. This is because, in the case of Zero initialization, the gradients and the weights are zero. The changes in loss are due to training of bias to the majority class.\n",
    "\n",
    "Same phenomenon happens for normal initialization. Due to large values of weights (due to the non zero probability of normal distribution for large values), the gradients of the weights explode (move towards infinity and negative infinity) due to the unbounded nature of ReLu activation function. This can be solved with a bounded activation function like sigmoid. This trend will eventually leads to zero weight gradients and the training slows to similar to the zero initialization case.\n",
    "\n",
    "With glorot initialization, the network trains easily reaching high accuracy and low mean loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Search -  From now on, use the Glorot initialization method.\n",
    "\n",
    "### 1.  Find out a combination of hyper-parameters (model architecture, learning rate, nonlinearity,etc.)  such that the average accuracy rate on the validation set (r_valid)) is at least 97%.\n",
    "### 2.  Report the hyper-parameters you tried and the corresponding r_(valid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid of model parameters - [hidden_layer_1,hidden_layer_2,inv_learning_rate]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([392, 512,   1]),\n",
       " array([392, 768,   1]),\n",
       " array([ 392, 1024,    1]),\n",
       " array([512, 256,   1]),\n",
       " array([512, 512,   1]),\n",
       " array([512, 768,   1]),\n",
       " array([ 512, 1024,    1]),\n",
       " array([768, 256,   1]),\n",
       " array([392, 512,  10]),\n",
       " array([392, 768,  10]),\n",
       " array([ 392, 1024,   10]),\n",
       " array([512, 256,  10]),\n",
       " array([512, 512,  10]),\n",
       " array([512, 768,  10]),\n",
       " array([ 512, 1024,   10]),\n",
       " array([768, 256,  10]),\n",
       " array([392, 512, 100]),\n",
       " array([392, 768, 100]),\n",
       " array([ 392, 1024,  100]),\n",
       " array([512, 256, 100]),\n",
       " array([512, 512, 100]),\n",
       " array([512, 768, 100]),\n",
       " array([ 512, 1024,  100]),\n",
       " array([768, 256, 100])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid of parameters - grid should be an array of integer, as the numpy will return only a list of the same type\n",
    "parameter_space = {\n",
    "    # grid of {392,512,768,1024}x{256,512,768,1024}\n",
    "    'hidden_1': [392, 512, 768, 1024],\n",
    "    'hidden_2': [256, 512, 768, 1024],\n",
    "    'inv_learning_rate' : [1,10,100]   \n",
    "}\n",
    "\n",
    "def invRate(dot):\n",
    "    ## function to convert from inv_learning_rate to learning_rate\n",
    "    if (dot[2] == 0):\n",
    "        raise ValueError('Inverse learning should not be 0')\n",
    "    return 1/dot[2]\n",
    "\n",
    "\n",
    "def parameter_count(n0 = 784, n1 = 0, n2 = 0, n3 = 10):\n",
    "    return ((n0+1)*n1)+((n1+1)*n2)+(n2+1)*n3\n",
    "    \n",
    "def validParam(dot):\n",
    "    ## invalid values (number of paramters > 1M or < 500K) are eliminated\n",
    "    n1 = dot[0]\n",
    "    n2 = dot[1]\n",
    "    size = parameter_count(n0 = 784, n1 = n1, n2 = n2, n3 = 10)\n",
    "    return (size > 500000 and size < 1000000)\n",
    "\n",
    "# Creating the grid of model parameters\n",
    "## Cartesian product : for each grid point, (possible combination in the parameter space) run n epochs and store the validation score\n",
    "grid = np.array(np.meshgrid(parameter_space['hidden_1'],parameter_space['hidden_2'], parameter_space['inv_learning_rate'])).T.reshape(-1,len(parameter_space))\n",
    "grid = [x for x in grid if validParam(x)]\n",
    "\n",
    "\n",
    "print(\"Grid of model parameters - [hidden_layer_1,hidden_layer_2,inv_learning_rate]\")\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "parameter count = 514066\n",
      "Loading model :  392 x 512 lr :  1.0\n",
      "Classification Accuracy on validation =  96.56 %\n",
      "Training complete in 0m 39s\n",
      "\n",
      "parameter count = 617234\n",
      "Loading model :  392 x 768 lr :  1.0\n",
      "Classification Accuracy on validation =  97.58 %\n",
      "Training complete in 1m 31s\n",
      "\n",
      "parameter count = 720402\n",
      "Loading model :  392 x 1024 lr :  1.0\n",
      "Classification Accuracy on validation =  97.39 %\n",
      "Training complete in 2m 34s\n",
      "\n",
      "parameter count = 535818\n",
      "Loading model :  512 x 256 lr :  1.0\n",
      "Classification Accuracy on validation =  97.57 %\n",
      "Training complete in 3m 11s\n",
      "\n",
      "parameter count = 669706\n",
      "Loading model :  512 x 512 lr :  1.0\n",
      "Classification Accuracy on validation =  97.74 %\n",
      "Training complete in 4m 1s\n",
      "\n",
      "parameter count = 803594\n",
      "Loading model :  512 x 768 lr :  1.0\n",
      "Classification Accuracy on validation =  97.65 %\n",
      "Training complete in 5m 6s\n",
      "\n",
      "parameter count = 937482\n",
      "Loading model :  512 x 1024 lr :  1.0\n",
      "Classification Accuracy on validation =  97.5 %\n",
      "Training complete in 6m 21s\n",
      "\n",
      "parameter count = 802314\n",
      "Loading model :  768 x 256 lr :  1.0\n",
      "Classification Accuracy on validation =  97.74 %\n",
      "Training complete in 7m 12s\n",
      "\n",
      "parameter count = 514066\n",
      "Loading model :  392 x 512 lr :  0.1\n",
      "Classification Accuracy on validation =  97.5 %\n",
      "Training complete in 7m 55s\n",
      "\n",
      "parameter count = 617234\n",
      "Loading model :  392 x 768 lr :  0.1\n",
      "Classification Accuracy on validation =  97.61 %\n",
      "Training complete in 8m 48s\n",
      "\n",
      "parameter count = 720402\n",
      "Loading model :  392 x 1024 lr :  0.1\n",
      "Classification Accuracy on validation =  97.54 %\n",
      "Training complete in 9m 51s\n",
      "\n",
      "parameter count = 535818\n",
      "Loading model :  512 x 256 lr :  0.1\n",
      "Classification Accuracy on validation =  97.67 %\n",
      "Training complete in 10m 31s\n",
      "\n",
      "parameter count = 669706\n",
      "Loading model :  512 x 512 lr :  0.1\n",
      "Classification Accuracy on validation =  97.69 %\n",
      "Training complete in 11m 24s\n",
      "\n",
      "parameter count = 803594\n",
      "Loading model :  512 x 768 lr :  0.1\n",
      "Classification Accuracy on validation =  97.74 %\n",
      "Training complete in 12m 31s\n",
      "\n",
      "parameter count = 937482\n",
      "Loading model :  512 x 1024 lr :  0.1\n",
      "Classification Accuracy on validation =  97.67 %\n",
      "Training complete in 13m 47s\n",
      "\n",
      "parameter count = 802314\n",
      "Loading model :  768 x 256 lr :  0.1\n",
      "Classification Accuracy on validation =  97.54 %\n",
      "Training complete in 14m 42s\n",
      "\n",
      "parameter count = 514066\n",
      "Loading model :  392 x 512 lr :  0.01\n",
      "Classification Accuracy on validation =  93.54 %\n",
      "Training complete in 15m 23s\n",
      "\n",
      "parameter count = 617234\n",
      "Loading model :  392 x 768 lr :  0.01\n",
      "Classification Accuracy on validation =  93.67 %\n",
      "Training complete in 16m 14s\n",
      "\n",
      "parameter count = 720402\n",
      "Loading model :  392 x 1024 lr :  0.01\n",
      "Classification Accuracy on validation =  93.28 %\n",
      "Training complete in 17m 15s\n",
      "\n",
      "parameter count = 535818\n",
      "Loading model :  512 x 256 lr :  0.01\n",
      "Classification Accuracy on validation =  93.87 %\n",
      "Training complete in 17m 53s\n",
      "\n",
      "parameter count = 669706\n",
      "Loading model :  512 x 512 lr :  0.01\n",
      "Classification Accuracy on validation =  93.79 %\n",
      "Training complete in 18m 43s\n",
      "\n",
      "parameter count = 803594\n",
      "Loading model :  512 x 768 lr :  0.01\n",
      "Classification Accuracy on validation =  93.46 %\n",
      "Training complete in 19m 45s\n",
      "\n",
      "parameter count = 937482\n",
      "Loading model :  512 x 1024 lr :  0.01\n",
      "Classification Accuracy on validation =  93.77 %\n",
      "Training complete in 20m 56s\n",
      "\n",
      "parameter count = 802314\n",
      "Loading model :  768 x 256 lr :  0.01\n",
      "Classification Accuracy on validation =  93.99 %\n",
      "Training complete in 21m 49s\n"
     ]
    }
   ],
   "source": [
    "## for every dot on the grid, run 10 epochs and save validation accuracy\n",
    "\n",
    "dot_predictions = {}\n",
    "\n",
    "#this grid search takes about an hour for 24 search points \n",
    "since = time.time()\n",
    "\n",
    "for index, dot in enumerate(grid):\n",
    "    print(\"\")\n",
    "    NN_0 = NN(hidden_dims=(dot[0],dot[1]), weight_type = \"Glorot\", learning_rate = invRate(dot), silent_search = 1)\n",
    "    print(\"Loading model : \",NN_0.h1,\"x\",NN_0.h2,\"lr : \", NN_0.eta)\n",
    "    \n",
    "    NN_0.train(train_data_features, train_data_labels, epochs = 10, mini_batch = 128)\n",
    "    \n",
    "    valid_predictions, _ = NN_0.test(valid_data_features.T,valid_data_labels)\n",
    "    acc = NN_0.classification_accuracy(valid_predictions,valid_data_labels)\n",
    "    print(\"Classification Accuracy on validation = \",acc,\"%\")\n",
    "    \n",
    "    dot_predictions[\"model_\"+str(index)] = {}\n",
    "    dot_predictions[\"model_\"+str(index)][\"hidden_1\"] = dot[0]\n",
    "    dot_predictions[\"model_\"+str(index)][\"hidden_2\"] = dot[1]\n",
    "    dot_predictions[\"model_\"+str(index)][\"parameters\"] = parameter_count(n0 = 784, n1 = dot[0], n2 = dot[1], n3 = 10)\n",
    "    dot_predictions[\"model_\"+str(index)][\"lr\"] = invRate(dot)\n",
    "    dot_predictions[\"model_\"+str(index)][\"r_valid [%]\"] = acc\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "      time_elapsed // 60, time_elapsed % 60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('model_0', {'hidden_1': 392, 'hidden_2': 512, 'parameters': 514066, 'lr': 1.0, 'r_valid [%]': 96.56})\n",
      "('model_1', {'hidden_1': 392, 'hidden_2': 768, 'parameters': 617234, 'lr': 1.0, 'r_valid [%]': 97.58})\n",
      "('model_2', {'hidden_1': 392, 'hidden_2': 1024, 'parameters': 720402, 'lr': 1.0, 'r_valid [%]': 97.39})\n",
      "('model_3', {'hidden_1': 512, 'hidden_2': 256, 'parameters': 535818, 'lr': 1.0, 'r_valid [%]': 97.57})\n",
      "('model_4', {'hidden_1': 512, 'hidden_2': 512, 'parameters': 669706, 'lr': 1.0, 'r_valid [%]': 97.74})\n",
      "('model_5', {'hidden_1': 512, 'hidden_2': 768, 'parameters': 803594, 'lr': 1.0, 'r_valid [%]': 97.65})\n",
      "('model_6', {'hidden_1': 512, 'hidden_2': 1024, 'parameters': 937482, 'lr': 1.0, 'r_valid [%]': 97.5})\n",
      "('model_7', {'hidden_1': 768, 'hidden_2': 256, 'parameters': 802314, 'lr': 1.0, 'r_valid [%]': 97.74})\n",
      "('model_8', {'hidden_1': 392, 'hidden_2': 512, 'parameters': 514066, 'lr': 0.1, 'r_valid [%]': 97.5})\n",
      "('model_9', {'hidden_1': 392, 'hidden_2': 768, 'parameters': 617234, 'lr': 0.1, 'r_valid [%]': 97.61})\n",
      "('model_10', {'hidden_1': 392, 'hidden_2': 1024, 'parameters': 720402, 'lr': 0.1, 'r_valid [%]': 97.54})\n",
      "('model_11', {'hidden_1': 512, 'hidden_2': 256, 'parameters': 535818, 'lr': 0.1, 'r_valid [%]': 97.67})\n",
      "('model_12', {'hidden_1': 512, 'hidden_2': 512, 'parameters': 669706, 'lr': 0.1, 'r_valid [%]': 97.69})\n",
      "('model_13', {'hidden_1': 512, 'hidden_2': 768, 'parameters': 803594, 'lr': 0.1, 'r_valid [%]': 97.74})\n",
      "('model_14', {'hidden_1': 512, 'hidden_2': 1024, 'parameters': 937482, 'lr': 0.1, 'r_valid [%]': 97.67})\n",
      "('model_15', {'hidden_1': 768, 'hidden_2': 256, 'parameters': 802314, 'lr': 0.1, 'r_valid [%]': 97.54})\n",
      "('model_16', {'hidden_1': 392, 'hidden_2': 512, 'parameters': 514066, 'lr': 0.01, 'r_valid [%]': 93.54})\n",
      "('model_17', {'hidden_1': 392, 'hidden_2': 768, 'parameters': 617234, 'lr': 0.01, 'r_valid [%]': 93.67})\n",
      "('model_18', {'hidden_1': 392, 'hidden_2': 1024, 'parameters': 720402, 'lr': 0.01, 'r_valid [%]': 93.28})\n",
      "('model_19', {'hidden_1': 512, 'hidden_2': 256, 'parameters': 535818, 'lr': 0.01, 'r_valid [%]': 93.87})\n",
      "('model_20', {'hidden_1': 512, 'hidden_2': 512, 'parameters': 669706, 'lr': 0.01, 'r_valid [%]': 93.79})\n",
      "('model_21', {'hidden_1': 512, 'hidden_2': 768, 'parameters': 803594, 'lr': 0.01, 'r_valid [%]': 93.46})\n",
      "('model_22', {'hidden_1': 512, 'hidden_2': 1024, 'parameters': 937482, 'lr': 0.01, 'r_valid [%]': 93.77})\n",
      "('model_23', {'hidden_1': 768, 'hidden_2': 256, 'parameters': 802314, 'lr': 0.01, 'r_valid [%]': 93.99})\n"
     ]
    }
   ],
   "source": [
    "for row in dot_predictions.items():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the different parameters, we find that for learning rate smaller than 0.1, the accuracy is lower after 10 epoch, i.e. training process is slower. For learning rate of 1, a few models - 512 x 512 (model_4) , 768 x 256 (model_7) reaches an accuracy of 97.74. For the learning rate of 0.1, the highest accuracy is reached by 512 x 786 (model_13)  with an accuracy of 97.74 percent. \n",
    "\n",
    "Lower learning rate helps in training better and thus we continue with 512 x 786 (model_13)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Validate Gradients using Finite Difference\n",
    "\n",
    "### 1. Compute finite difference gradients using $\\epsilon = \\frac{1}{N}$ for different values of N. Use at least 5 values of N from theset $\\{k  10^i: \\ i \\in \\{0, \\dots, 5\\}, \\ k \\in \\{1, 5\\}\\} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.1, 0.01, 0.001, 0.0001, 1e-05]\n"
     ]
    }
   ],
   "source": [
    "N_list = [10.0**i for i in range(6)]#[1,1.0e1,1.0e2,1.0e3,1.0e4,1.0e5,1.0e6,1.0e7]\n",
    "epsilon_list = [1.0/i for i in N_list]\n",
    "print(epsilon_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter count = 803594\n",
      "Loading model :  512 x 768 lr :  0.1\n"
     ]
    }
   ],
   "source": [
    "# model \n",
    "NN1 = NN(hidden_dims=(512,768), weight_type = \"Glorot\", learning_rate = 0.1)\n",
    "print(\"Loading model : \",NN1.h1,\"x\",NN1.h2,\"lr : \", NN1.eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([748., 718., 733., 816., 785., 809., 753., 782., 748., 788.]),\n",
       " array([-8.77566535e-02, -7.02024187e-02, -5.26481839e-02, -3.50939491e-02,\n",
       "        -1.75397143e-02,  1.45204861e-05,  1.75687553e-02,  3.51229901e-02,\n",
       "         5.26772249e-02,  7.02314597e-02,  8.77856945e-02]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(NN1.W2.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking one input point \n",
    "ind = 0\n",
    "x_i = train_data_features[ind].reshape(-1,1)\n",
    "y_i = [train_data_labels[ind]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epsilon =  1.0\n",
      "W2[0,0] finite difference = [0.] \t finite difference/grads[W2][0,0] =  [nan]\n",
      "W2[0,1] finite difference = [0.05484835] \t finite difference/grads[W2][0,1] =  [1.0350778]\n",
      "W2[0,2] finite difference = [0.] \t finite difference/grads[W2][0,2] =  [nan]\n",
      "W2[0,3] finite difference = [0.0067867] \t finite difference/grads[W2][0,3] =  [1.00057534]\n",
      "W2[0,4] finite difference = [0.01999971] \t finite difference/grads[W2][0,4] =  [1.00495239]\n",
      "W2[0,5] finite difference = [0.01013697] \t finite difference/grads[W2][0,5] =  [1.00128175]\n",
      "W2[0,6] finite difference = [0.] \t finite difference/grads[W2][0,6] =  [nan]\n",
      "W2[0,7] finite difference = [0.] \t finite difference/grads[W2][0,7] =  [nan]\n",
      "W2[0,8] finite difference = [0.00431851] \t finite difference/grads[W2][0,8] =  [1.00023312]\n",
      "W2[0,9] finite difference = [0.00253696] \t finite difference/grads[W2][0,9] =  [1.00008048]\n",
      "\n",
      "Epsilon =  0.1\n",
      "W2[0,0] finite difference = [0.] \t finite difference/grads[W2][0,0] =  [nan]\n",
      "W2[0,1] finite difference = [0.0530082] \t finite difference/grads[W2][0,1] =  [1.00035115]\n",
      "W2[0,2] finite difference = [0.] \t finite difference/grads[W2][0,2] =  [nan]\n",
      "W2[0,3] finite difference = [0.00678284] \t finite difference/grads[W2][0,3] =  [1.00000575]\n",
      "W2[0,4] finite difference = [0.01990214] \t finite difference/grads[W2][0,4] =  [1.00004953]\n",
      "W2[0,5] finite difference = [0.01012413] \t finite difference/grads[W2][0,5] =  [1.00001282]\n",
      "W2[0,6] finite difference = [0.] \t finite difference/grads[W2][0,6] =  [nan]\n",
      "W2[0,7] finite difference = [0.] \t finite difference/grads[W2][0,7] =  [nan]\n",
      "W2[0,8] finite difference = [0.00431751] \t finite difference/grads[W2][0,8] =  [1.00000233]\n",
      "W2[0,9] finite difference = [0.00253676] \t finite difference/grads[W2][0,9] =  [1.0000008]\n",
      "\n",
      "Epsilon =  0.01\n",
      "W2[0,0] finite difference = [0.] \t finite difference/grads[W2][0,0] =  [nan]\n",
      "W2[0,1] finite difference = [0.05298978] \t finite difference/grads[W2][0,1] =  [1.00000351]\n",
      "W2[0,2] finite difference = [0.] \t finite difference/grads[W2][0,2] =  [nan]\n",
      "W2[0,3] finite difference = [0.0067828] \t finite difference/grads[W2][0,3] =  [1.00000006]\n",
      "W2[0,4] finite difference = [0.01990117] \t finite difference/grads[W2][0,4] =  [1.0000005]\n",
      "W2[0,5] finite difference = [0.010124] \t finite difference/grads[W2][0,5] =  [1.00000013]\n",
      "W2[0,6] finite difference = [0.] \t finite difference/grads[W2][0,6] =  [nan]\n",
      "W2[0,7] finite difference = [0.] \t finite difference/grads[W2][0,7] =  [nan]\n",
      "W2[0,8] finite difference = [0.0043175] \t finite difference/grads[W2][0,8] =  [1.00000002]\n",
      "W2[0,9] finite difference = [0.00253675] \t finite difference/grads[W2][0,9] =  [1.00000001]\n",
      "\n",
      "Epsilon =  0.001\n",
      "W2[0,0] finite difference = [0.] \t finite difference/grads[W2][0,0] =  [nan]\n",
      "W2[0,1] finite difference = [0.05298959] \t finite difference/grads[W2][0,1] =  [1.00000004]\n",
      "W2[0,2] finite difference = [0.] \t finite difference/grads[W2][0,2] =  [nan]\n",
      "W2[0,3] finite difference = [0.0067828] \t finite difference/grads[W2][0,3] =  [1.]\n",
      "W2[0,4] finite difference = [0.01990116] \t finite difference/grads[W2][0,4] =  [1.]\n",
      "W2[0,5] finite difference = [0.010124] \t finite difference/grads[W2][0,5] =  [1.]\n",
      "W2[0,6] finite difference = [0.] \t finite difference/grads[W2][0,6] =  [nan]\n",
      "W2[0,7] finite difference = [0.] \t finite difference/grads[W2][0,7] =  [nan]\n",
      "W2[0,8] finite difference = [0.0043175] \t finite difference/grads[W2][0,8] =  [1.]\n",
      "W2[0,9] finite difference = [0.00253675] \t finite difference/grads[W2][0,9] =  [1.]\n",
      "\n",
      "Epsilon =  0.0001\n",
      "W2[0,0] finite difference = [0.] \t finite difference/grads[W2][0,0] =  [nan]\n",
      "W2[0,1] finite difference = [0.05298959] \t finite difference/grads[W2][0,1] =  [1.]\n",
      "W2[0,2] finite difference = [0.] \t finite difference/grads[W2][0,2] =  [nan]\n",
      "W2[0,3] finite difference = [0.0067828] \t finite difference/grads[W2][0,3] =  [1.]\n",
      "W2[0,4] finite difference = [0.01990116] \t finite difference/grads[W2][0,4] =  [1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/helios/lib/python3.7/site-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2[0,5] finite difference = [0.010124] \t finite difference/grads[W2][0,5] =  [1.]\n",
      "W2[0,6] finite difference = [0.] \t finite difference/grads[W2][0,6] =  [nan]\n",
      "W2[0,7] finite difference = [0.] \t finite difference/grads[W2][0,7] =  [nan]\n",
      "W2[0,8] finite difference = [0.0043175] \t finite difference/grads[W2][0,8] =  [1.]\n",
      "W2[0,9] finite difference = [0.00253675] \t finite difference/grads[W2][0,9] =  [1.]\n",
      "\n",
      "Epsilon =  1e-05\n",
      "W2[0,0] finite difference = [0.] \t finite difference/grads[W2][0,0] =  [nan]\n",
      "W2[0,1] finite difference = [0.05298959] \t finite difference/grads[W2][0,1] =  [1.]\n",
      "W2[0,2] finite difference = [0.] \t finite difference/grads[W2][0,2] =  [nan]\n",
      "W2[0,3] finite difference = [0.0067828] \t finite difference/grads[W2][0,3] =  [1.]\n",
      "W2[0,4] finite difference = [0.01990116] \t finite difference/grads[W2][0,4] =  [1.]\n",
      "W2[0,5] finite difference = [0.010124] \t finite difference/grads[W2][0,5] =  [1.]\n",
      "W2[0,6] finite difference = [0.] \t finite difference/grads[W2][0,6] =  [nan]\n",
      "W2[0,7] finite difference = [0.] \t finite difference/grads[W2][0,7] =  [nan]\n",
      "W2[0,8] finite difference = [0.0043175] \t finite difference/grads[W2][0,8] =  [1.]\n",
      "W2[0,9] finite difference = [0.00253675] \t finite difference/grads[W2][0,9] =  [1.]\n"
     ]
    }
   ],
   "source": [
    "max_absolute_diff = []\n",
    "for epsilon in epsilon_list:\n",
    "    print(\"\\nEpsilon = \",epsilon)\n",
    "    absolute_diff = []\n",
    "    for i in [0]:\n",
    "        for j in range(10):\n",
    "            \n",
    "            NN1.W2[i][j] -= epsilon\n",
    "            cache = NN1.forward(x_i)\n",
    "            before = NN1.loss(cache['output'],y_i)\n",
    "            NN1.W2[i][j] += epsilon\n",
    "            \n",
    "            NN1.W2[i][j] += epsilon\n",
    "            cache = NN1.forward(x_i)\n",
    "            after = NN1.loss(cache['output'],y_i)\n",
    "            NN1.W2[i][j] -= epsilon\n",
    "            \n",
    "            finite_difference_gradient = (after-before)/(2*epsilon)\n",
    "            \n",
    "            cache = NN1.forward(x_i)\n",
    "            grads = NN1.backward(cache,y_i)\n",
    "            \n",
    "            \n",
    "            print('W2['+str(i)+','+str(j)+'] finite difference =', finite_difference_gradient\n",
    "                  ,'\\t finite difference/grads[W2]['+str(i)+','+str(j)+'] = ',\n",
    "                  finite_difference_gradient/grads['W2'][i][j])\n",
    "            \n",
    "            ## computing the absolute difference between finite difference and backprop computed gradient.\n",
    "            absolute_diff.append(np.abs(finite_difference_gradient - grads['W2'][i][j]))\n",
    "    \n",
    "    \n",
    "    ## Maximum of the absolute differnce seen for this epsilon value.\n",
    "    max_absolute_diff.append(np.max(absolute_diff))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Plot the maximum difference between the true gradient and the finite difference gradient $(\\max_{1 \\leq i \\leq p} |\\nabla^N_i - \\frac{\\partial L}{\\partial \\theta_i}| )$ as a function of $N$. Comment on the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.loglog(N_list,max_absolute_diff,'ro-')\n",
    "plt.ylabel('Max difference')\n",
    "plt.xlabel('N')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that for larger $N$ (i.e. smaller $\\epsilon$), the finite difference gradient is closer to the gradient obtained by back propagation. Therefore, we can confirm that our backpropagation function is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
